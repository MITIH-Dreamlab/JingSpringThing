name: logseq-xr

services:
  webxr-graph:
    build: .
    image: logseq-xr-image:latest
    read_only: false
    deploy:
      resources:
        limits:
          cpus: '16.0'
          memory: 64G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "4000:80"    # Map container's 80 to host's 4000
      - "8080:8080"  # Backend API
    environment:
      - RUST_LOG=debug
      - RUST_BACKTRACE=1
      - BIND_ADDRESS=0.0.0.0
      - PORT=8080
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./settings.toml:/app/settings.toml:ro
      - type: tmpfs
        target: /tmp
        tmpfs:
          size: 4G
          mode: 1777
    networks:
      - logseq-net
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "1g"
        max-file: "5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: logseqxr-tunnel
    read_only: true
    command: tunnel --no-autoupdate run --token ${TUNNEL_TOKEN} --url http://webxr-graph:80
    environment:
      - TUNNEL_TOKEN=${TUNNEL_TOKEN}
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
    networks:
      - logseq-net
    restart: unless-stopped
    depends_on:
      webxr-graph:
        condition: service_healthy
    logging:
      driver: "json-file"
      options:
        max-size: "1g"
        max-file: "5"

networks:
  logseq-net:
    driver: bridge
    internal: false
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
    driver_opts:
      encrypted: "true"
