diff --git a/Dockerfile b/Dockerfile
index c890b935..10d5deab 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -3,9 +3,9 @@ FROM node:latest AS frontend-builder
 
 WORKDIR /app
 
-# Copy package files, vite config, and the entire data directory
+# Copy package files, vite config, and the public directory
 COPY package.json pnpm-lock.yaml vite.config.js ./
-COPY data ./data
+COPY data/public ./data/public
 
 # Install pnpm globally
 RUN npm install -g pnpm
@@ -82,9 +82,6 @@ RUN mkdir -p /app/data/public/dist /app/data/markdown /app/src /app/data/piper
 # Create an empty metadata.json file
 RUN mkdir -p /app/data/markdown && touch /app/data/markdown/metadata.json && echo "{}" > /app/data/markdown/metadata.json
 
-# Copy topics.csv file into the container
-COPY data/topics.csv /app/data/topics.csv
-
 # Copy the local piper voice model
 COPY data/piper/en_GB-northern_english_male-medium.onnx /app/data/piper/en_GB-northern_english_male-medium.onnx
 
diff --git a/data/public/js/components/visualization/nodes.js b/data/public/js/components/visualization/nodes.js
index 512b3fc4..f6609228 100644
--- a/data/public/js/components/visualization/nodes.js
+++ b/data/public/js/components/visualization/nodes.js
@@ -26,9 +26,9 @@ export class NodeManager {
         this.edgeMeshes = new Map();
         
         // Node settings
-        this.minNodeSize = 0.1;  // Increased from 1
-        this.maxNodeSize = 5; // Increased from 5
-        this.nodeSizeScalingFactor = 1;  // Increased from 1
+        this.minNodeSize = 0.1;  // Minimum node size in visualization
+        this.maxNodeSize = 5;    // Maximum node size in visualization
+        this.nodeSizeScalingFactor = 1;  // Global scaling factor
         this.labelFontSize = 18;
         this.nodeColor = new THREE.Color(0x4444ff);  // Initialize as THREE.Color
 
@@ -37,13 +37,24 @@ export class NodeManager {
         this.edgeOpacity = 0.6;
     }
 
-    calculateNodeSize(fileSize) {
+    getNodeSize(metadata) {
+        // Use the node_size from metadata if available
+        if (metadata.node_size) {
+            // Convert from server's range (5.0-50.0) to visualization range
+            const serverMin = 5.0;
+            const serverMax = 50.0;
+            const normalizedSize = (parseFloat(metadata.node_size) - serverMin) / (serverMax - serverMin);
+            return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize * this.nodeSizeScalingFactor;
+        }
+        
+        // Fallback to file size calculation if node_size is not available
+        const fileSize = parseInt(metadata.file_size) || 1;
         const logMin = Math.log(1);
         const logMax = Math.log(1e9); // 1GB as max reference
         const logSize = Math.log(fileSize + 1);
         
         const normalizedSize = (logSize - logMin) / (logMax - logMin);
-        return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize;
+        return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize * this.nodeSizeScalingFactor;
     }
 
     calculateNodeColor(lastModified) {
@@ -156,7 +167,7 @@ export class NodeManager {
             const lastModified = metadata.last_modified || new Date().toISOString();
             const hyperlinkCount = parseInt(metadata.hyperlink_count) || 0;
 
-            const size = this.calculateNodeSize(fileSize) * this.nodeSizeScalingFactor;
+            const size = this.getNodeSize(metadata);
             const color = this.calculateNodeColor(lastModified);
 
             let mesh = this.nodeMeshes.get(node.id);
diff --git a/data/topics.csv b/data/topics.csv
deleted file mode 100644
index 1b994196..00000000
--- a/data/topics.csv
+++ /dev/null
@@ -1,178 +0,0 @@
-Apple
-Distributed Identity
-Norbert Wiener
-Proprietary AI Video
-Facebook Meta
-Training and fine tuning
-Agents
-Comparison of SDXL and Midjourney v6
-Speech and voice
-Bitcoin Technical Overview
-Diagrams as Code
-Education and AI
-p(doom)
-Stable Diffusion
-Hardware and Edge
-pages_Web3
-Text to 3D and 4D
-Human tracking and SLAM capture
-Courses and Training
-cypherpunk
-Landscape
-pages_Decentralised Web from DigiSoc
-RGB and Client Side Validation
-Digital Asset Risks
-ollama
-Bitcoin
-Knowledge Graphing and RAG
-Base models
-GANs
-LoRA DoRA etc
-Comparison of GPT4 and Gemini Ultra
-Product and Risk Management
-Music and audio
-Global Inequality
-Diffusion Models
-Tokenization
-Upscaling
-Semantic Web
-artificial superintelligence
-Metaverse and Spatial Risks
-Cyber security and Cryptography
-multimodal
-Multi Agent RAG scrapbook
-Deepfakes and fraudulent content
-Proprietary Large Language Models
-flossverse
-Metaverse Ontology
-Proprietary Video VP & 3D
-Deep Learning
-Time Series Forecasting
-Blockchain
-Ai in Games
-ecash
-pages_Bitcoin Mining and Energy
-Prompt Engineering
-Evaluation and leaderboards
-Ethereum
-Geopolitical hot takes
-Digital Society Surveillance
-Layoff tracker and threatened roles
-AI Video
-contents
-SLAM
-Research Tools
-Bitcoin As Money
-AI privacy at the 2024 Olympics
-Soon-Next-Later (AI futurology)
-Tuesday 11th of June FRAME reporting
-Definitions and frameworks for Metaverse
-nostr
-OpenAI
-Microsoft Work Trends Impact 2024
-Vesuvian Scrolls
-Bitcoin ETF
-Octave Multi Model Laboratory
-ChatGPT
-Microsoft CoPilot
-pages_qlora
-Vision Pro
-Safety and alignment
-Blender
-pages_Salford-Uni-GenAI-Lectures
-Death of the Internet
-latent space
-Robotics
-pages_Voice
-Digital Objects
-Agentic Mycelia
-AnimateDiff
-Energy and Power
-AI Scrapers
-ComfyUI
-Artificial Intelligence
-Anthropic Claude
-Tim Reutermann
-Coding support
-Virtual Production
-Machine Learning
-Proprietary Image Generation
-Mixed reality
-California AI bill
-Conspiracies
-Competition in AI
-Knowhere
-pages_trust
-Depth Estimation
-Deepmind
-Leopold Aschenbrenner
-Social contract and jobs
-Transformers
-pages_RAG
-Recent Projects
-State Space and Other Approaches
-Flux
-Gold
-Bitcoin Value Proposition
-Robin Hanson
-Client side DCO
-Agentic Metaverse for Global Creatives
-AI Adoption
-Money
-Jailbreaking
-Sam Hammond
-BTC Layer 3
-Gemini
-Product Design
-Politics, Law, Privacy
-Llama
-Singularity
-Algorithmic Bias and Variance
-pages_Text-to-3D & 4D
-Overview of Machine Learning Techniques
-AI Companies
-Cyber Security and Military
-infrastructure
-Cashu
-Humans, Avatars , Character
-Accessibility
-collaborative
-AI Risks
-Convergence
-Large language models
-NVIDIA Omniverse
-relighting
-Human vs AI
-Segmentation and Identification
-Runes and Glyphs
-Parametric
-State of the art in AI
-Calculating Empires
-Open Generative AI tools
-Spatial Computing
-Visionflow
-pages_Identity
-Inpainting
-EU AI Act
-pages_Telepresence
-IPAdapter
-Rust
-Lead Poisoning Hypothesis
-Stable Coins
-pages_SDXL
-National Industrial Centre for Virtual Environments
-Model Optimisation and Performance
-Scene Capture and Reconstruction
-Decentralised Web
-Metaverse as Markets
-Comfy UI for Fashion and Brands
-Medical AI
-Automated Podcast Project
-Gaussian splatting and Similar
-Revision List
-Suggested Reading Order
-Hyper personalisation
-Privacy, Trust and Safety
-license
-Controlnet and similar
-Introduction to me
diff --git a/diff.patch b/diff.patch
new file mode 100644
index 00000000..99ceb679
--- /dev/null
+++ b/diff.patch
@@ -0,0 +1,843 @@
+diff --git a/data/public/js/components/visualization/nodes.js b/data/public/js/components/visualization/nodes.js
+index 512b3fc4..f6609228 100644
+--- a/data/public/js/components/visualization/nodes.js
++++ b/data/public/js/components/visualization/nodes.js
+@@ -26,9 +26,9 @@ export class NodeManager {
+         this.edgeMeshes = new Map();
+         
+         // Node settings
+-        this.minNodeSize = 0.1;  // Increased from 1
+-        this.maxNodeSize = 5; // Increased from 5
+-        this.nodeSizeScalingFactor = 1;  // Increased from 1
++        this.minNodeSize = 0.1;  // Minimum node size in visualization
++        this.maxNodeSize = 5;    // Maximum node size in visualization
++        this.nodeSizeScalingFactor = 1;  // Global scaling factor
+         this.labelFontSize = 18;
+         this.nodeColor = new THREE.Color(0x4444ff);  // Initialize as THREE.Color
+ 
+@@ -37,13 +37,24 @@ export class NodeManager {
+         this.edgeOpacity = 0.6;
+     }
+ 
+-    calculateNodeSize(fileSize) {
++    getNodeSize(metadata) {
++        // Use the node_size from metadata if available
++        if (metadata.node_size) {
++            // Convert from server's range (5.0-50.0) to visualization range
++            const serverMin = 5.0;
++            const serverMax = 50.0;
++            const normalizedSize = (parseFloat(metadata.node_size) - serverMin) / (serverMax - serverMin);
++            return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize * this.nodeSizeScalingFactor;
++        }
++        
++        // Fallback to file size calculation if node_size is not available
++        const fileSize = parseInt(metadata.file_size) || 1;
+         const logMin = Math.log(1);
+         const logMax = Math.log(1e9); // 1GB as max reference
+         const logSize = Math.log(fileSize + 1);
+         
+         const normalizedSize = (logSize - logMin) / (logMax - logMin);
+-        return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize;
++        return this.minNodeSize + (this.maxNodeSize - this.minNodeSize) * normalizedSize * this.nodeSizeScalingFactor;
+     }
+ 
+     calculateNodeColor(lastModified) {
+@@ -156,7 +167,7 @@ export class NodeManager {
+             const lastModified = metadata.last_modified || new Date().toISOString();
+             const hyperlinkCount = parseInt(metadata.hyperlink_count) || 0;
+ 
+-            const size = this.calculateNodeSize(fileSize) * this.nodeSizeScalingFactor;
++            const size = this.getNodeSize(metadata);
+             const color = this.calculateNodeColor(lastModified);
+ 
+             let mesh = this.nodeMeshes.get(node.id);
+diff --git a/src/models/metadata.rs b/src/models/metadata.rs
+index 6d40b03a..8cfd67f3 100644
+--- a/src/models/metadata.rs
++++ b/src/models/metadata.rs
+@@ -7,6 +7,7 @@ use std::collections::HashMap;
+ pub struct Metadata {
+     pub file_name: String,
+     pub file_size: usize,
++    pub node_size: f64,  // Added field for scaled node size
+     pub hyperlink_count: usize,
+     pub sha1: String,
+     pub perplexity_link: String,
+diff --git a/src/services/file_service.rs b/src/services/file_service.rs
+index 5a2b62bf..b83e023a 100644
+--- a/src/services/file_service.rs
++++ b/src/services/file_service.rs
+@@ -1,7 +1,8 @@
+ use crate::models::metadata::Metadata;
+ use crate::config::Settings;
+ use serde::{Deserialize, Serialize};
+-use reqwest::{Client, header::{HeaderMap, HeaderValue, IF_NONE_MATCH, ETAG}};
++use reqwest::Client;
++use reqwest::header::{HeaderMap, HeaderValue, IF_NONE_MATCH, ETAG};
+ use async_trait::async_trait;
+ use log::{info, debug, error};
+ use regex::Regex;
+@@ -13,16 +14,16 @@ use chrono::{Utc, DateTime};
+ use std::sync::Arc;
+ use tokio::sync::RwLock;
+ use std::error::Error as StdError;
+-use futures::stream::{self, StreamExt};
++use futures::stream::StreamExt;
+ use std::time::Duration;
+ use tokio::time::sleep;
+ 
+-// Rest of the file remains unchanged
++// Constants
+ const METADATA_PATH: &str = "data/markdown/metadata.json";
+ const MARKDOWN_DIR: &str = "data/markdown";
+-const CACHE_DURATION: Duration = Duration::from_secs(300); // 5 minutes
+-const MAX_CONCURRENT_DOWNLOADS: usize = 5;
+ const GITHUB_API_DELAY: Duration = Duration::from_millis(100); // Rate limiting delay
++const MIN_NODE_SIZE: f64 = 5.0;
++const MAX_NODE_SIZE: f64 = 50.0;
+ 
+ #[derive(Serialize, Deserialize, Clone)]
+ pub struct GithubFile {
+@@ -40,6 +41,8 @@ pub struct GithubFileMetadata {
+     pub etag: Option<String>,
+     #[serde(with = "chrono::serde::ts_seconds_option")]
+     pub last_checked: Option<DateTime<Utc>>,
++    #[serde(with = "chrono::serde::ts_seconds_option")]
++    pub last_modified: Option<DateTime<Utc>>,
+ }
+ 
+ #[derive(Serialize, Deserialize, Clone)]
+@@ -50,21 +53,11 @@ pub struct ProcessedFile {
+     pub metadata: Metadata,
+ }
+ 
+-#[derive(Debug, Serialize, Deserialize)]
+-struct TreeResponse {
+-    sha: String,
+-    tree: Vec<TreeItem>,
+-    truncated: bool,
+-}
+-
+-#[derive(Debug, Serialize, Deserialize)]
+-struct TreeItem {
+-    path: String,
+-    mode: String,
+-    #[serde(rename = "type")]
+-    item_type: String,
+-    sha: String,
+-    url: Option<String>,
++// Structure to hold reference information
++#[derive(Default)]
++struct ReferenceInfo {
++    direct_mentions: usize,
++    hyperlinks: usize,
+ }
+ 
+ #[async_trait]
+@@ -72,6 +65,7 @@ pub trait GitHubService: Send + Sync {
+     async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>>;
+     async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>>;
+     async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>>;
++    async fn get_file_last_modified(&self, file_path: &str) -> Result<DateTime<Utc>, Box<dyn StdError + Send + Sync>>;
+ }
+ 
+ pub struct RealGitHubService {
+@@ -108,27 +102,11 @@ impl RealGitHubService {
+             metadata_cache: Arc::new(RwLock::new(HashMap::new())),
+         })
+     }
++}
+ 
+-    async fn fetch_directory_contents(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
+-        // First, check the cache
+-        {
+-            let cache = self.metadata_cache.read().await;
+-            let now = Utc::now();
+-            
+-            // If cache is fresh and not empty, use it
+-            if !cache.is_empty() {
+-                if let Some(first_item) = cache.values().next() {
+-                    if let Some(last_checked) = first_item.last_checked {
+-                        if (now - last_checked) < chrono::Duration::from_std(CACHE_DURATION).unwrap() {
+-                            debug!("Using cached metadata for files");
+-                            return Ok(cache.values().cloned().collect());
+-                        }
+-                    }
+-                }
+-            }
+-        }
+-
+-        // Cache is stale or empty, fetch from GitHub
++#[async_trait]
++impl GitHubService for RealGitHubService {
++    async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
+         let url = format!(
+             "https://api.github.com/repos/{}/{}/contents/{}",
+             self.owner, self.repo, self.base_path
+@@ -141,90 +119,29 @@ impl RealGitHubService {
+ 
+         let contents: Vec<serde_json::Value> = response.json().await?;
+         
+-        let markdown_files: Vec<GithubFileMetadata> = contents.into_iter()
+-            .filter(|item| {
+-                let is_file = item["type"].as_str().unwrap_or("") == "file";
+-                let name = item["name"].as_str().unwrap_or("");
+-                is_file && name.ends_with(".md")
+-            })
+-            .map(|item| {
+-                GithubFileMetadata {
+-                    name: item["name"].as_str().unwrap_or("").to_string(),
++        let mut markdown_files = Vec::new();
++        
++        for item in contents {
++            if item["type"].as_str().unwrap_or("") == "file" && 
++               item["name"].as_str().unwrap_or("").ends_with(".md") {
++                let name = item["name"].as_str().unwrap_or("").to_string();
++                let last_modified = self.get_file_last_modified(&format!("{}/{}", self.base_path, name)).await?;
++                
++                markdown_files.push(GithubFileMetadata {
++                    name,
+                     sha: item["sha"].as_str().unwrap_or("").to_string(),
+                     download_url: item["download_url"].as_str().unwrap_or("").to_string(),
+                     etag: None,
+                     last_checked: Some(Utc::now()),
+-                }
+-            })
+-            .collect();
+-
+-        // Update cache
+-        {
+-            let mut cache = self.metadata_cache.write().await;
+-            cache.clear();
+-            for metadata in &markdown_files {
+-                cache.insert(metadata.name.clone(), metadata.clone());
++                    last_modified: Some(last_modified),
++                });
+             }
+         }
+ 
+-        debug!("Found {} markdown files in target directory", markdown_files.len());
+         Ok(markdown_files)
+     }
+ 
+-    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
+-        let mut headers = HeaderMap::new();
+-        headers.insert("Authorization", HeaderValue::from_str(&format!("token {}", self.token))?);
+-
+-        // Get cached ETag if available
+-        let etag = {
+-            let cache = self.metadata_cache.read().await;
+-            cache.values()
+-                .find(|m| m.download_url == download_url)
+-                .and_then(|m| m.etag.clone())
+-        };
+-
+-        if let Some(etag) = etag {
+-            headers.insert(IF_NONE_MATCH, HeaderValue::from_str(&etag)?);
+-        }
+-
+-        let response = self.client.get(download_url)
+-            .headers(headers)
+-            .send()
+-            .await?;
+-
+-        // Update ETag in cache if provided
+-        if let Some(new_etag) = response.headers().get(ETAG) {
+-            let mut cache = self.metadata_cache.write().await;
+-            if let Some(metadata) = cache.values_mut().find(|m| m.download_url == download_url) {
+-                metadata.etag = Some(new_etag.to_str()?.to_string());
+-            }
+-        }
+-
+-        if response.status() == reqwest::StatusCode::NOT_MODIFIED {
+-            // Use cached content
+-            let path = format!("{}/{}", MARKDOWN_DIR, download_url.split('/').last().unwrap_or(""));
+-            if let Ok(content) = fs::read_to_string(&path) {
+-                return Ok(content);
+-            }
+-        }
+-
+-        let content = response.text().await?;
+-        Ok(content)
+-    }
+-
+-    pub async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
+-        self.fetch_directory_contents().await
+-    }
+-
+     async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
+-        // Check cache first
+-        {
+-            let cache = self.metadata_cache.read().await;
+-            if let Some(metadata) = cache.get(file_name) {
+-                return Ok(Some(metadata.download_url.clone()));
+-            }
+-        }
+-
+         let url = format!("https://api.github.com/repos/{}/{}/contents/{}/{}", 
+             self.owner, self.repo, self.base_path, file_name);
+ 
+@@ -240,58 +157,105 @@ impl RealGitHubService {
+             Ok(None)
+         }
+     }
+-}
+ 
+-#[async_trait]
+-impl GitHubService for RealGitHubService {
+-    async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
+-        self.fetch_file_metadata().await
+-    }
++    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
++        let mut headers = HeaderMap::new();
++        headers.insert("Authorization", HeaderValue::from_str(&format!("token {}", self.token))?);
+ 
+-    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
+-        self.get_download_url(file_name).await
++        let response = self.client.get(download_url)
++            .headers(headers)
++            .send()
++            .await?;
++
++        let content = response.text().await?;
++        Ok(content)
+     }
+ 
+-    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
+-        self.fetch_file_content(download_url).await
++    async fn get_file_last_modified(&self, file_path: &str) -> Result<DateTime<Utc>, Box<dyn StdError + Send + Sync>> {
++        let url = format!(
++            "https://api.github.com/repos/{}/{}/commits",
++            self.owner, self.repo
++        );
++
++        let response = self.client.get(&url)
++            .header("Authorization", format!("token {}", self.token))
++            .query(&[("path", file_path), ("per_page", "1")])
++            .send()
++            .await?;
++
++        let commits: Vec<serde_json::Value> = response.json().await?;
++        
++        if let Some(last_commit) = commits.first() {
++            if let Some(commit) = last_commit["commit"]["committer"]["date"].as_str() {
++                if let Ok(date) = DateTime::parse_from_rfc3339(commit) {
++                    return Ok(date.with_timezone(&Utc));
++                }
++            }
++        }
++        
++        Ok(Utc::now())
+     }
+ }
+ 
+ pub struct FileService;
+ 
+ impl FileService {
+-    /// Check if we have a valid local setup
+-    fn has_valid_local_setup() -> bool {
+-        // Check if metadata.json exists and is not empty
+-        if let Ok(metadata_content) = fs::read_to_string(METADATA_PATH) {
+-            if metadata_content.trim().is_empty() {
+-                return false;
++    /// Extract both direct mentions and hyperlink references to other files
++    fn extract_references(content: &str, valid_nodes: &[String]) -> HashMap<String, ReferenceInfo> {
++        let mut references = HashMap::new();
++        
++        for node_name in valid_nodes {
++            let mut ref_info = ReferenceInfo::default();
++            
++            // Count direct mentions (case insensitive)
++            let direct_pattern = format!(r"(?i)\[\[{}]]|\b{}\b", regex::escape(node_name), regex::escape(node_name));
++            if let Ok(re) = Regex::new(&direct_pattern) {
++                ref_info.direct_mentions = re.find_iter(content).count();
+             }
+             
+-            // Try to parse metadata to ensure it's valid
+-            if let Ok(metadata_map) = serde_json::from_str::<HashMap<String, Metadata>>(&metadata_content) {
+-                if metadata_map.is_empty() {
+-                    return false;
+-                }
+-                
+-                // Check if the markdown files referenced in metadata actually exist
+-                for (filename, _) in metadata_map {
+-                    let file_path = format!("{}/{}", MARKDOWN_DIR, filename);
+-                    if !Path::new(&file_path).exists() {
+-                        return false;
+-                    }
+-                }
+-                
+-                return true;
++            // Count markdown links
++            let link_pattern = format!(r"\[([^]]*)](\(.*?{}\)|\[{}])", node_name, node_name);
++            if let Ok(re) = Regex::new(&link_pattern) {
++                ref_info.hyperlinks = re.find_iter(content).count();
++            }
++            
++            if ref_info.direct_mentions > 0 || ref_info.hyperlinks > 0 {
++                references.insert(node_name.clone(), ref_info);
+             }
+         }
+-        false
++        
++        references
++    }
++
++    /// Calculate node size using logarithmic scaling
++    fn calculate_node_size(file_size: usize) -> f64 {
++        if file_size == 0 {
++            return MIN_NODE_SIZE;
++        }
++
++        let size_f64: f64 = file_size as f64;
++        let log_size = f64::log10(size_f64 + 1.0);
++        let min_log = f64::log10(1.0);
++        let max_log = f64::log10(269425.0 + 1.0); // Maximum known file size + 1
++        
++        MIN_NODE_SIZE + (log_size - min_log) * (MAX_NODE_SIZE - MIN_NODE_SIZE) / (max_log - min_log)
++    }
++
++    /// Convert ReferenceInfo to topic_counts format
++    fn convert_references_to_topic_counts(references: HashMap<String, ReferenceInfo>) -> HashMap<String, usize> {
++        references.into_iter()
++            .map(|(name, info)| {
++                // Weight hyperlinks more heavily than direct mentions
++                let total_weight = info.direct_mentions + (info.hyperlinks * 2);
++                (name, total_weight)
++            })
++            .collect()
+     }
+ 
+     /// Initialize the local markdown directory and metadata structure.
+     pub async fn initialize_local_storage(
+         github_service: &dyn GitHubService,
+-        settings: Arc<RwLock<Settings>>,
++        _settings: Arc<RwLock<Settings>>,
+     ) -> Result<(), Box<dyn StdError + Send + Sync>> {
+         info!("Checking local storage status");
+         
+@@ -306,23 +270,16 @@ impl FileService {
+ 
+         info!("Initializing local storage with files from GitHub");
+ 
+-        // Get topics from settings
+-        let settings = settings.read().await;
+-        let topics = settings.topics.clone();
+-
+         // Step 1: Get all markdown files from GitHub
+         let github_files = github_service.fetch_file_metadata().await?;
+         info!("Found {} markdown files in GitHub", github_files.len());
+ 
+-        let mut metadata_map = HashMap::new();
+-        let mut processed_count = 0;
+-        let mut total_files = 0;
+-
+-        // Step 2: Download and process each file
++        let mut file_sizes = HashMap::new();
++        let mut file_contents = HashMap::new();
++        let mut file_metadata = HashMap::new();
++        
++        // Step 2: First pass - collect all files and their contents
+         for file_meta in github_files {
+-            total_files += 1;
+-            
+-            // Download file content
+             match github_service.fetch_file_content(&file_meta.download_url).await {
+                 Ok(content) => {
+                     // Check if file starts with "public:: true"
+@@ -332,68 +289,97 @@ impl FileService {
+                         continue;
+                     }
+ 
+-                    let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
+-                    
+-                    // Calculate SHA1 of content
+-                    let local_sha1 = Self::calculate_sha1(&content);
+-                    
+-                    // Save file content
+-                    fs::write(&file_path, &content)?;
+-                    processed_count += 1;
+-
+-                    // Extract topics from content
+-                    let topic_counts = Self::extract_topics(&content, &topics);
+-
+-                    // Create metadata entry
+-                    let metadata = Metadata {
+-                        file_name: file_meta.name.clone(),
+-                        file_size: content.len(),
+-                        hyperlink_count: Self::count_hyperlinks(&content),
+-                        sha1: local_sha1,
+-                        last_modified: Utc::now(),
+-                        perplexity_link: String::new(),
+-                        last_perplexity_process: None,
+-                        topic_counts,
+-                    };
+-
+-                    metadata_map.insert(file_meta.name.clone(), metadata);
+-                    info!("Processed public file: {}", file_meta.name);
++                    let node_name = file_meta.name.trim_end_matches(".md").to_string();
++                    file_sizes.insert(node_name.clone(), content.len());
++                    file_contents.insert(node_name, content);
++                    file_metadata.insert(file_meta.name.clone(), file_meta);
+                 }
+                 Err(e) => {
+                     error!("Failed to fetch content for {}: {}", file_meta.name, e);
+                 }
+             }
+-
+-            // Add delay for rate limiting
+             sleep(GITHUB_API_DELAY).await;
+         }
+ 
+-        // Step 3: Save metadata
++        // Get list of valid node names (filenames without .md)
++        let valid_nodes: Vec<String> = file_contents.keys().cloned().collect();
++
++        // Step 3: Second pass - extract references and create metadata
++        let mut metadata_map = HashMap::new();
++        
++        for (node_name, content) in &file_contents {
++            let file_name = format!("{}.md", node_name);
++            let file_path = format!("{}/{}", MARKDOWN_DIR, file_name);
++            
++            // Calculate SHA1 of content
++            let local_sha1 = Self::calculate_sha1(content);
++            
++            // Save file content
++            fs::write(&file_path, content)?;
++
++            // Extract references
++            let references = Self::extract_references(content, &valid_nodes);
++            let topic_counts = Self::convert_references_to_topic_counts(references);
++
++            // Get GitHub metadata
++            let github_meta = file_metadata.get(&file_name).unwrap();
++            let last_modified = github_meta.last_modified.unwrap_or_else(|| Utc::now());
++
++            // Calculate node size
++            let file_size = *file_sizes.get(node_name).unwrap();
++            let node_size = Self::calculate_node_size(file_size);
++
++            // Create metadata entry
++            let metadata = Metadata {
++                file_name: file_name.clone(),
++                file_size,
++                node_size,
++                hyperlink_count: Self::count_hyperlinks(content),
++                sha1: local_sha1,
++                last_modified,
++                perplexity_link: String::new(),
++                last_perplexity_process: None,
++                topic_counts,
++            };
++
++            metadata_map.insert(file_name, metadata);
++        }
++
++        // Step 4: Save metadata
+         info!("Saving metadata for {} public files", metadata_map.len());
+         Self::save_metadata(&metadata_map)?;
+ 
+-        info!("Initialization complete. Found {} total files, processed {} public files", 
+-            total_files, processed_count);
++        info!("Initialization complete. Processed {} public files", metadata_map.len());
+ 
+         Ok(())
+     }
+ 
+-    /// Extract topics from content
+-    fn extract_topics(content: &str, topics: &[String]) -> HashMap<String, usize> {
+-        let mut topic_counts = HashMap::new();
+-        
+-        // Convert content to lowercase for case-insensitive matching
+-        let content_lower = content.to_lowercase();
+-        
+-        for topic in topics {
+-            let topic_lower = topic.to_lowercase();
+-            let count = content_lower.matches(&topic_lower).count();
+-            if count > 0 {
+-                topic_counts.insert(topic.clone(), count);
++    /// Check if we have a valid local setup
++    fn has_valid_local_setup() -> bool {
++        // Check if metadata.json exists and is not empty
++        if let Ok(metadata_content) = fs::read_to_string(METADATA_PATH) {
++            if metadata_content.trim().is_empty() {
++                return false;
++            }
++            
++            // Try to parse metadata to ensure it's valid
++            if let Ok(metadata_map) = serde_json::from_str::<HashMap<String, Metadata>>(&metadata_content) {
++                if metadata_map.is_empty() {
++                    return false;
++                }
++                
++                // Check if the markdown files referenced in metadata actually exist
++                for (filename, _) in metadata_map {
++                    let file_path = format!("{}/{}", MARKDOWN_DIR, filename);
++                    if !Path::new(&file_path).exists() {
++                        return false;
++                    }
++                }
++                
++                return true;
+             }
+         }
+-        
+-        topic_counts
++        false
+     }
+ 
+     /// Ensures all required directories exist
+@@ -409,16 +395,12 @@ impl FileService {
+     /// Handles incremental updates after initial setup
+     pub async fn fetch_and_process_files(
+         github_service: &dyn GitHubService,
+-        settings: Arc<RwLock<Settings>>,
++        _settings: Arc<RwLock<Settings>>,
+         metadata_map: &mut HashMap<String, Metadata>,
+     ) -> Result<Vec<ProcessedFile>, Box<dyn StdError + Send + Sync>> {
+         // Ensure directories exist before any operations
+         Self::ensure_directories()?;
+ 
+-        // Get topics from settings
+-        let settings = settings.read().await;
+-        let topics = settings.topics.clone();
+-
+         // Get metadata for markdown files in target directory
+         let github_files_metadata = github_service.fetch_file_metadata().await?;
+         debug!("Fetched metadata for {} markdown files", github_files_metadata.len());
+@@ -448,7 +430,12 @@ impl FileService {
+             }
+         }
+ 
+-        // Process files in parallel with rate limiting
++        // Get list of valid node names (filenames without .md)
++        let valid_nodes: Vec<String> = github_files_metadata.iter()
++            .map(|f| f.name.trim_end_matches(".md").to_string())
++            .collect();
++
++        // Process files that need updating
+         let files_to_process: Vec<_> = github_files_metadata.into_iter()
+             .filter(|file_meta| {
+                 let local_meta = metadata_map.get(&file_meta.name);
+@@ -456,71 +443,52 @@ impl FileService {
+             })
+             .collect();
+ 
+-        let results = stream::iter(files_to_process)
+-            .map(|file_meta| {
+-                let github_service = github_service;
+-                let topics = topics.clone();
+-                async move {
+-                    // Add delay for rate limiting
+-                    sleep(GITHUB_API_DELAY).await;
+-
+-                    // Download content
+-                    match github_service.fetch_file_content(&file_meta.download_url).await {
+-                        Ok(content) => {
+-                            // Check if file starts with "public:: true"
+-                            let first_line = content.lines().next().unwrap_or("").trim();
+-                            if first_line != "public:: true" {
+-                                debug!("Skipping non-public file: {}", file_meta.name);
+-                                return Ok(None);
+-                            }
+-                            
+-                            let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
+-                            fs::write(&file_path, &content)?;
+-                            
+-                            // Extract topics from content
+-                            let topic_counts = Self::extract_topics(&content, &topics);
+-
+-                            let new_metadata = Metadata {
+-                                file_name: file_meta.name.clone(),
+-                                file_size: content.len(),
+-                                hyperlink_count: Self::count_hyperlinks(&content),
+-                                sha1: file_meta.sha.clone(),
+-                                last_modified: Utc::now(),
+-                                perplexity_link: String::new(),
+-                                last_perplexity_process: None,
+-                                topic_counts,
+-                            };
+-                            
+-                            Ok(Some(ProcessedFile {
+-                                file_name: file_meta.name,
+-                                content,
+-                                is_public: true,
+-                                metadata: new_metadata,
+-                            }))
+-                        }
+-                        Err(e) => {
+-                            error!("Failed to fetch content: {}", e);
+-                            Err(e)
+-                        }
++        // Process each file
++        for file_meta in files_to_process {
++            match github_service.fetch_file_content(&file_meta.download_url).await {
++                Ok(content) => {
++                    let first_line = content.lines().next().unwrap_or("").trim();
++                    if first_line != "public:: true" {
++                        debug!("Skipping non-public file: {}", file_meta.name);
++                        continue;
+                     }
++
++                    let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
++                    fs::write(&file_path, &content)?;
++
++                    // Extract references
++                    let references = Self::extract_references(&content, &valid_nodes);
++                    let topic_counts = Self::convert_references_to_topic_counts(references);
++
++                    // Calculate node size
++                    let file_size = content.len();
++                    let node_size = Self::calculate_node_size(file_size);
++
++                    let new_metadata = Metadata {
++                        file_name: file_meta.name.clone(),
++                        file_size,
++                        node_size,
++                        hyperlink_count: Self::count_hyperlinks(&content),
++                        sha1: Self::calculate_sha1(&content),
++                        last_modified: file_meta.last_modified.unwrap_or_else(|| Utc::now()),
++                        perplexity_link: String::new(),
++                        last_perplexity_process: None,
++                        topic_counts,
++                    };
++
++                    metadata_map.insert(file_meta.name.clone(), new_metadata.clone());
++                    processed_files.push(ProcessedFile {
++                        file_name: file_meta.name,
++                        content,
++                        is_public: true,
++                        metadata: new_metadata,
++                    });
+                 }
+-            })
+-            .buffer_unordered(MAX_CONCURRENT_DOWNLOADS)
+-            .collect::<Vec<_>>()
+-            .await;
+-
+-        // Process results
+-        for result in results {
+-            match result {
+-                Ok(Some(processed_file)) => {
+-                    let file_name = processed_file.file_name.clone();
+-                    metadata_map.insert(file_name.clone(), processed_file.metadata.clone());
+-                    processed_files.push(processed_file);
+-                    debug!("Successfully processed public file: {}", file_name);
++                Err(e) => {
++                    error!("Failed to fetch content: {}", e);
+                 }
+-                Ok(None) => {} // Skip non-public files
+-                Err(e) => error!("Error processing file: {}", e),
+             }
++            sleep(GITHUB_API_DELAY).await;
+         }
+ 
+         // Save updated metadata
+diff --git a/src/services/graph_service.rs b/src/services/graph_service.rs
+index 0fc46028..ecb9709a 100644
+--- a/src/services/graph_service.rs
++++ b/src/services/graph_service.rs
+@@ -1,5 +1,3 @@
+-// src/services/graph_service.rs
+-
+ use crate::AppState;
+ use crate::models::graph::GraphData;
+ use crate::models::node::Node;
+@@ -26,13 +24,19 @@ impl GraphService {
+         let metadata: HashMap<String, Metadata> = serde_json::from_str(&metadata_content)?;
+     
+         let mut graph = GraphData::default();
+-        let mut edge_map: HashMap<(String, String), (f32, u32)> = HashMap::new();
++        let mut edge_map: HashMap<(String, String), f32> = HashMap::new();
+     
+         // Create nodes
+         for (file_name, file_metadata) in &metadata {
+             let node_id = file_name.trim_end_matches(".md").to_string();
+             let mut node_metadata = HashMap::new();
++            
++            // Add file metadata to node
+             node_metadata.insert("file_size".to_string(), file_metadata.file_size.to_string());
++            node_metadata.insert("node_size".to_string(), file_metadata.node_size.to_string());
++            node_metadata.insert("last_modified".to_string(), file_metadata.last_modified.to_string());
++            node_metadata.insert("hyperlink_count".to_string(), file_metadata.hyperlink_count.to_string());
++            
+             graph.nodes.push(Node {
+                 id: node_id.clone(),
+                 label: node_id.clone(),
+@@ -43,38 +47,31 @@ impl GraphService {
+             graph.metadata.insert(node_id.clone(), file_metadata.clone());
+         }
+     
+-        // Build edges
++        // Build edges from topic counts
+         for (file_name, file_metadata) in &metadata {
+-            let node_id = file_name.trim_end_matches(".md").to_string();
+-            for (other_file, _) in &metadata {
+-                if file_name != other_file {
+-                    let other_node_id = other_file.trim_end_matches(".md");
+-                    let count = file_metadata.topic_counts.get(other_node_id).cloned().unwrap_or(0) as f32;
+-                    if count > 0.0 {
+-                        let edge_key = if node_id < other_node_id.to_string() {
+-                            (node_id.clone(), other_node_id.to_string())
+-                        } else {
+-                            (other_node_id.to_string(), node_id.clone())
+-                        };
+-                        edge_map.entry(edge_key)
+-                            .and_modify(|(weight, hyperlinks)| {
+-                                *weight += count;
+-                                *hyperlinks += file_metadata.hyperlink_count as u32;
+-                            })
+-                            .or_insert((count, file_metadata.hyperlink_count as u32));
+-                    }
++            let source_id = file_name.trim_end_matches(".md").to_string();
++            
++            // Look through all references in topic_counts
++            for (target_id, reference_count) in &file_metadata.topic_counts {
++                if source_id != *target_id {
++                    // Create a canonical edge key (alphabetically ordered)
++                    let edge_key = if source_id < *target_id {
++                        (source_id.clone(), target_id.clone())
++                    } else {
++                        (target_id.clone(), source_id.clone())
++                    };
++
++                    // Update edge weight
++                    edge_map.entry(edge_key)
++                        .and_modify(|weight| *weight += *reference_count as f32)
++                        .or_insert(*reference_count as f32);
+                 }
+             }
+         }
+     
+         // Convert edge_map to edges
+-        graph.edges = edge_map.into_iter().map(|((source, target), (weight, hyperlinks))| {
+-            Edge {
+-                source,
+-                target_node: target,
+-                weight,
+-                hyperlinks: hyperlinks as f32,
+-            }
++        graph.edges = edge_map.into_iter().map(|((source, target), weight)| {
++            Edge::new(source, target, weight, 0.0) // Set hyperlinks to 0 since we're not using it for forces
+         }).collect();
+         
+         info!("Graph data built with {} nodes and {} edges", graph.nodes.len(), graph.edges.len());
+@@ -106,7 +103,7 @@ impl GraphService {
+         match gpu_compute {
+             Some(gpu) => {
+                 info!("Using GPU for layout calculation");
+-                let mut gpu_compute = gpu.write().await; // Acquire write lock
++                let mut gpu_compute = gpu.write().await;
+                 gpu_compute.set_graph_data(graph)?;
+                 gpu_compute.set_force_directed_params(params)?;
+                 gpu_compute.compute_forces()?;
+@@ -166,6 +163,8 @@ impl GraphService {
+                 let dy = graph.nodes[target].y - graph.nodes[source].y;
+                 let dz = graph.nodes[target].z - graph.nodes[source].z;
+                 let distance = (dx * dx + dy * dy + dz * dz).sqrt().max(0.1);
++                
++                // Use only weight for attraction
+                 let force = attraction * distance * edge.weight;
+                 let fx = force * dx / distance;
+                 let fy = force * dy / distance;
diff --git a/src/config.rs b/src/config.rs
index 9cbc5b51..6557f566 100644
--- a/src/config.rs
+++ b/src/config.rs
@@ -1,97 +1,21 @@
-use config::{Config, ConfigError, File};
-use dotenv::dotenv;
+use config::{Config, ConfigError, Environment, File};
 use serde::{Deserialize, Serialize};
-use std::fmt;
-use std::fs;
-use std::path::Path;
 use std::env;
-
-/// Converts a color value to proper CSS hex format
-fn normalize_color(value: String) -> String {
-    if value.starts_with('#') {
-        value
-    } else if value.starts_with("0x") {
-        format!("#{}", &value[2..])
-    } else {
-        format!("#{}", value)
-    }
-}
+use std::fmt;
 
 #[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct Settings {
-    #[serde(default = "default_prompt")]
     pub prompt: String,
-    #[serde(skip_deserializing)]
-    pub topics: Vec<String>,
     pub github: GithubSettings,
-    pub ragflow: RagFlowSettings,
+    pub ragflow: RagflowSettings,
     pub perplexity: PerplexitySettings,
     pub openai: OpenAISettings,
-    #[serde(default = "default_settings")]
     pub default: DefaultSettings,
-    #[serde(default = "default_visualization")]
     pub visualization: VisualizationSettings,
-    #[serde(default = "default_bloom")]
     pub bloom: BloomSettings,
-    #[serde(default = "default_fisheye")]
     pub fisheye: FisheyeSettings,
 }
 
-fn default_prompt() -> String {
-    "Your default prompt here".to_string()
-}
-
-fn default_settings() -> DefaultSettings {
-    DefaultSettings {
-        max_concurrent_requests: 5,
-        max_retries: 3,
-        retry_delay: 5,
-        api_client_timeout: 30,
-    }
-}
-
-fn default_visualization() -> VisualizationSettings {
-    VisualizationSettings {
-        node_color: "#1A0B31".to_string(),
-        edge_color: "#FF0000".to_string(),
-        hologram_color: "#FFD700".to_string(),
-        node_size_scaling_factor: 5,
-        hologram_scale: 5,
-        hologram_opacity: 0.1,
-        edge_opacity: 0.3,
-        label_font_size: 36,
-        fog_density: 0.002,
-        force_directed_iterations: 100,
-        force_directed_spring: 0.1,
-        force_directed_repulsion: 1000.0,
-        force_directed_attraction: 0.01,
-        force_directed_damping: 0.8,
-    }
-}
-
-fn default_bloom() -> BloomSettings {
-    BloomSettings {
-        node_bloom_strength: 0.1,
-        node_bloom_radius: 0.1,
-        node_bloom_threshold: 0.0,
-        edge_bloom_strength: 0.2,
-        edge_bloom_radius: 0.3,
-        edge_bloom_threshold: 0.0,
-        environment_bloom_strength: 0.5,
-        environment_bloom_radius: 0.1,
-        environment_bloom_threshold: 0.0,
-    }
-}
-
-fn default_fisheye() -> FisheyeSettings {
-    FisheyeSettings {
-        enabled: false,
-        strength: 0.5,
-        focus_point: [0.0, 0.0, 0.0],
-        radius: 100.0,
-    }
-}
-
 #[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct GithubSettings {
     pub github_access_token: String,
@@ -101,15 +25,9 @@ pub struct GithubSettings {
 }
 
 #[derive(Debug, Serialize, Deserialize, Clone)]
-pub struct RagFlowSettings {
+pub struct RagflowSettings {
     pub ragflow_api_key: String,
-    pub ragflow_base_url: String,
-}
-
-#[derive(Debug, Serialize, Deserialize, Clone)]
-pub struct OpenAISettings {
-    pub openai_api_key: String,
-    pub openai_base_url: String,
+    pub ragflow_api_base_url: String,
 }
 
 #[derive(Debug, Serialize, Deserialize, Clone)]
@@ -117,28 +35,34 @@ pub struct PerplexitySettings {
     pub perplexity_api_key: String,
     pub perplexity_model: String,
     pub perplexity_api_url: String,
-    pub perplexity_max_tokens: u32,
+    pub perplexity_max_tokens: usize,
     pub perplexity_temperature: f32,
     pub perplexity_top_p: f32,
     pub perplexity_presence_penalty: f32,
     pub perplexity_frequency_penalty: f32,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
+pub struct OpenAISettings {
+    pub openai_api_key: String,
+    pub openai_base_url: String,
+}
+
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct DefaultSettings {
-    pub max_concurrent_requests: u32,
-    pub max_retries: u32,
-    pub retry_delay: u32,
-    pub api_client_timeout: u32,
+    pub max_concurrent_requests: usize,
+    pub max_retries: usize,
+    pub retry_delay: u64,
+    pub api_client_timeout: u64,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct VisualizationSettings {
     pub node_color: String,
     pub edge_color: String,
     pub hologram_color: String,
-    pub node_size_scaling_factor: u32,
-    pub hologram_scale: u32,
+    pub node_size_scaling_factor: f32,
+    pub hologram_scale: f32,
     pub hologram_opacity: f32,
     pub edge_opacity: f32,
     pub label_font_size: u32,
@@ -150,7 +74,7 @@ pub struct VisualizationSettings {
     pub force_directed_damping: f32,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct BloomSettings {
     pub node_bloom_strength: f32,
     pub node_bloom_radius: f32,
@@ -163,7 +87,7 @@ pub struct BloomSettings {
     pub environment_bloom_threshold: f32,
 }
 
-#[derive(Debug, Serialize, Deserialize, Clone, Default)]
+#[derive(Debug, Serialize, Deserialize, Clone)]
 pub struct FisheyeSettings {
     pub enabled: bool,
     pub strength: f32,
@@ -173,97 +97,79 @@ pub struct FisheyeSettings {
 
 impl Settings {
     pub fn new() -> Result<Self, ConfigError> {
-        if !std::env::var("DOCKER").is_ok() {
-            match dotenv() {
-                Ok(_) => log::debug!("Successfully loaded .env file"),
-                Err(e) => log::warn!("Failed to load .env file: {}", e),
-            }
-        }
+        let mut builder = Config::builder()
+            .add_source(File::with_name("settings"))
+            .add_source(Environment::with_prefix("APP"));
 
-        let run_mode = std::env::var("RUN_MODE").unwrap_or_else(|_| "development".into());
-        log::debug!("Loading configuration for mode: {}", run_mode);
-
-        let mut builder = Config::builder();
-
-        // Add settings.toml
-        builder = builder.add_source(File::with_name("settings.toml").required(false));
-
-        // Environment variable overrides
-        if let Ok(token) = env::var("GITHUB_ACCESS_TOKEN") {
-            builder = builder.set_override("github.github_access_token", token)?;
+        // Override settings with environment variables if they exist
+        if let Ok(value) = env::var("GITHUB_ACCESS_TOKEN") {
+            builder = builder.set_override("github.github_access_token", value)?;
         }
-        if let Ok(owner) = env::var("GITHUB_OWNER") {
-            builder = builder.set_override("github.github_owner", owner)?;
+        if let Ok(value) = env::var("GITHUB_OWNER") {
+            builder = builder.set_override("github.github_owner", value)?;
         }
-        if let Ok(repo) = env::var("GITHUB_REPO") {
-            builder = builder.set_override("github.github_repo", repo)?;
+        if let Ok(value) = env::var("GITHUB_REPO") {
+            builder = builder.set_override("github.github_repo", value)?;
         }
-        if let Ok(directory) = env::var("GITHUB_DIRECTORY") {
-            builder = builder.set_override("github.github_directory", directory)?;
+        if let Ok(value) = env::var("GITHUB_DIRECTORY") {
+            builder = builder.set_override("github.github_directory", value)?;
         }
-
-        if let Ok(api_key) = env::var("RAGFLOW_API_KEY") {
-            builder = builder.set_override("ragflow.ragflow_api_key", api_key)?;
+        if let Ok(value) = env::var("RAGFLOW_API_KEY") {
+            builder = builder.set_override("ragflow.ragflow_api_key", value)?;
         }
-        if let Ok(base_url) = env::var("RAGFLOW_BASE_URL") {
-            builder = builder.set_override("ragflow.ragflow_base_url", base_url)?;
+        if let Ok(value) = env::var("RAGFLOW_BASE_URL") {
+            builder = builder.set_override("ragflow.ragflow_api_base_url", value)?;
         }
-
-        if let Ok(api_key) = env::var("PERPLEXITY_API_KEY") {
-            builder = builder.set_override("perplexity.perplexity_api_key", api_key)?;
+        if let Ok(value) = env::var("PERPLEXITY_API_KEY") {
+            builder = builder.set_override("perplexity.perplexity_api_key", value)?;
         }
-        if let Ok(model) = env::var("PERPLEXITY_MODEL") {
-            builder = builder.set_override("perplexity.perplexity_model", model)?;
+        if let Ok(value) = env::var("PERPLEXITY_MODEL") {
+            builder = builder.set_override("perplexity.perplexity_model", value)?;
         }
-        if let Ok(api_url) = env::var("PERPLEXITY_API_URL") {
-            builder = builder.set_override("perplexity.perplexity_api_url", api_url)?;
+        if let Ok(value) = env::var("PERPLEXITY_API_URL") {
+            builder = builder.set_override("perplexity.perplexity_api_url", value)?;
         }
-        if let Ok(max_tokens) = env::var("PERPLEXITY_MAX_TOKENS") {
-            builder = builder.set_override("perplexity.perplexity_max_tokens", max_tokens)?;
+        if let Ok(value) = env::var("PERPLEXITY_MAX_TOKENS") {
+            builder = builder.set_override("perplexity.perplexity_max_tokens", value)?;
         }
-        if let Ok(temperature) = env::var("PERPLEXITY_TEMPERATURE") {
-            builder = builder.set_override("perplexity.perplexity_temperature", temperature)?;
+        if let Ok(value) = env::var("PERPLEXITY_TEMPERATURE") {
+            builder = builder.set_override("perplexity.perplexity_temperature", value)?;
         }
-        if let Ok(top_p) = env::var("PERPLEXITY_TOP_P") {
-            builder = builder.set_override("perplexity.perplexity_top_p", top_p)?;
+        if let Ok(value) = env::var("PERPLEXITY_TOP_P") {
+            builder = builder.set_override("perplexity.perplexity_top_p", value)?;
         }
-        if let Ok(presence_penalty) = env::var("PERPLEXITY_PRESENCE_PENALTY") {
-            builder = builder.set_override("perplexity.perplexity_presence_penalty", presence_penalty)?;
+        if let Ok(value) = env::var("PERPLEXITY_PRESENCE_PENALTY") {
+            builder = builder.set_override("perplexity.perplexity_presence_penalty", value)?;
         }
-        if let Ok(frequency_penalty) = env::var("PERPLEXITY_FREQUENCY_PENALTY") {
-            builder = builder.set_override("perplexity.perplexity_frequency_penalty", frequency_penalty)?;
+        if let Ok(value) = env::var("PERPLEXITY_FREQUENCY_PENALTY") {
+            builder = builder.set_override("perplexity.perplexity_frequency_penalty", value)?;
         }
-
-        if let Ok(api_key) = env::var("OPENAI_API_KEY") {
-            builder = builder.set_override("openai.openai_api_key", api_key)?;
+        if let Ok(value) = env::var("OPENAI_API_KEY") {
+            builder = builder.set_override("openai.openai_api_key", value)?;
         }
-        if let Ok(base_url) = env::var("OPENAI_BASE_URL") {
-            builder = builder.set_override("openai.openai_base_url", base_url)?;
+        if let Ok(value) = env::var("OPENAI_BASE_URL") {
+            builder = builder.set_override("openai.openai_base_url", value)?;
         }
-
-        // Default settings
-        if let Ok(max_requests) = env::var("MAX_CONCURRENT_REQUESTS") {
-            builder = builder.set_override("default.max_concurrent_requests", max_requests)?;
+        if let Ok(value) = env::var("MAX_CONCURRENT_REQUESTS") {
+            builder = builder.set_override("default.max_concurrent_requests", value)?;
         }
-        if let Ok(max_retries) = env::var("MAX_RETRIES") {
-            builder = builder.set_override("default.max_retries", max_retries)?;
+        if let Ok(value) = env::var("MAX_RETRIES") {
+            builder = builder.set_override("default.max_retries", value)?;
         }
-        if let Ok(retry_delay) = env::var("RETRY_DELAY") {
-            builder = builder.set_override("default.retry_delay", retry_delay)?;
+        if let Ok(value) = env::var("RETRY_DELAY") {
+            builder = builder.set_override("default.retry_delay", value)?;
         }
-        if let Ok(timeout) = env::var("API_CLIENT_TIMEOUT") {
-            builder = builder.set_override("default.api_client_timeout", timeout)?;
+        if let Ok(value) = env::var("API_CLIENT_TIMEOUT") {
+            builder = builder.set_override("default.api_client_timeout", value)?;
         }
-
-        // Visualization settings
         if let Ok(value) = env::var("NODE_COLOR") {
-            builder = builder.set_override("visualization.node_color", normalize_color(value))?;
+            builder = builder.set_override("visualization.node_color", value)?;
         }
         if let Ok(value) = env::var("EDGE_COLOR") {
-            builder = builder.set_override("visualization.edge_color", normalize_color(value))?;
+            builder = builder.set_override("visualization.edge_color", value)?;
         }
         if let Ok(value) = env::var("HOLOGRAM_COLOR") {
-            builder = builder.set_override("visualization.hologram_color", normalize_color(value))?;
+            builder = builder.set_override("visualization.hologram_color", value)?;
         }
         if let Ok(value) = env::var("NODE_SIZE_SCALING_FACTOR") {
             builder = builder.set_override("visualization.node_size_scaling_factor", value)?;
@@ -298,8 +204,6 @@ impl Settings {
         if let Ok(value) = env::var("FORCE_DIRECTED_DAMPING") {
             builder = builder.set_override("visualization.force_directed_damping", value)?;
         }
-
-        // Bloom settings
         if let Ok(value) = env::var("NODE_BLOOM_STRENGTH") {
             builder = builder.set_override("bloom.node_bloom_strength", value)?;
         }
@@ -327,8 +231,6 @@ impl Settings {
         if let Ok(value) = env::var("ENVIRONMENT_BLOOM_THRESHOLD") {
             builder = builder.set_override("bloom.environment_bloom_threshold", value)?;
         }
-
-        // Fisheye settings
         if let Ok(value) = env::var("FISHEYE_ENABLED") {
             builder = builder.set_override("fisheye.enabled", value)?;
         }
@@ -338,122 +240,75 @@ impl Settings {
         if let Ok(value) = env::var("FISHEYE_RADIUS") {
             builder = builder.set_override("fisheye.radius", value)?;
         }
-        if let Ok(value) = env::var("FISHEYE_FOCUS_X") {
-            builder = builder.set_override("fisheye.focus_point[0]", value)?;
-        }
-        if let Ok(value) = env::var("FISHEYE_FOCUS_Y") {
-            builder = builder.set_override("fisheye.focus_point[1]", value)?;
-        }
-        if let Ok(value) = env::var("FISHEYE_FOCUS_Z") {
-            builder = builder.set_override("fisheye.focus_point[2]", value)?;
-        }
-
-        let config = builder.build()?;
 
-        match config.try_deserialize::<Settings>() {
-            Ok(mut settings) => {
-                settings.topics = load_topics_from_markdown();
-                Ok(settings)
-            }
-            Err(e) => {
-                log::error!("Failed to deserialize settings: {}", e);
-                Err(e)
-            }
-        }
-    }
-}
-
-fn load_topics_from_markdown() -> Vec<String> {
-    let markdown_dir = Path::new("/app/data/markdown");
-    if !markdown_dir.exists() {
-        return vec!["default_topic".to_string()];
-    }
-
-    match fs::read_dir(markdown_dir) {
-        Ok(entries) => {
-            let mut topics: Vec<String> = entries
-                .filter_map(|entry| {
-                    entry.ok().and_then(|e| {
-                        let path = e.path();
-                        if let Some(ext) = path.extension() {
-                            if ext == "md" {
-                                path.file_stem()
-                                    .and_then(|s| s.to_str())
-                                    .map(|s| s.to_string())
-                            } else {
-                                None
-                            }
-                        } else {
-                            None
-                        }
-                    })
-                })
-                .collect();
-
-            if topics.is_empty() {
-                vec!["default_topic".to_string()]
-            } else {
-                topics.sort();
-                topics
-            }
-        }
-        Err(_) => vec!["default_topic".to_string()],
-    }
-}
-
-impl fmt::Display for GithubSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "GithubSettings {{ access_token: [REDACTED], owner: {}, repo: {}, directory: {} }}", 
-               self.github_owner, self.github_repo, self.github_directory)
-    }
-}
-
-impl fmt::Display for RagFlowSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "RagFlowSettings {{ base_url: {}, api_key: [REDACTED] }}", 
-               self.ragflow_base_url)
-    }
-}
-
-impl fmt::Display for OpenAISettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "OpenAISettings {{ base_url: {}, api_key: [REDACTED] }}", 
-               self.openai_base_url)
-    }
-}
-
-impl fmt::Display for PerplexitySettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "PerplexitySettings {{ api_url: {}, api_key: [REDACTED], model: {} }}", 
-               self.perplexity_api_url, self.perplexity_model)
-    }
-}
-
-impl fmt::Display for DefaultSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "DefaultSettings {{ max_concurrent_requests: {}, max_retries: {}, retry_delay: {}, api_client_timeout: {} }}", 
-               self.max_concurrent_requests, self.max_retries, self.retry_delay, self.api_client_timeout)
+        builder.build()?.try_deserialize()
     }
 }
 
 impl fmt::Display for VisualizationSettings {
     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "VisualizationSettings {{ node_color: {}, edge_color: {}, iterations: {}, repulsion: {}, attraction: {} }}", 
-               self.node_color, self.edge_color, self.force_directed_iterations, 
+        write!(f, "VisualizationSettings {{ node_color: {}, edge_color: {}, iterations: {}, repulsion: {}, attraction: {} }}",
+               self.node_color, self.edge_color, self.force_directed_iterations,
                self.force_directed_repulsion, self.force_directed_attraction)
     }
 }
 
-impl fmt::Display for BloomSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "BloomSettings {{ node_strength: {}, edge_strength: {}, environment_strength: {} }}", 
-               self.node_bloom_strength, self.edge_bloom_strength, self.environment_bloom_strength)
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_simulation_params_from_config() {
+        let config = VisualizationSettings {
+            node_color: "0x1A0B31".to_string(),
+            edge_color: "0xff0000".to_string(),
+            hologram_color: "0xFFD700".to_string(),
+            node_size_scaling_factor: 1.0,
+            hologram_scale: 5.0,
+            hologram_opacity: 0.1,
+            edge_opacity: 0.3,
+            label_font_size: 16,
+            fog_density: 0.002,
+            force_directed_iterations: 100,
+            force_directed_spring: 0.1,
+            force_directed_repulsion: 1000.0,
+            force_directed_attraction: 0.01,
+            force_directed_damping: 0.8,
+        };
+
+        let params = crate::models::simulation_params::SimulationParams::from_config(&config);
+        assert_eq!(params.iterations, 100);
+        assert_eq!(params.repulsion_strength, 1000.0);
+        assert_eq!(params.attraction_strength, 0.01);
+        assert_eq!(params.damping, 0.8);
     }
-}
 
-impl fmt::Display for FisheyeSettings {
-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
-        write!(f, "FisheyeSettings {{ enabled: {}, strength: {}, radius: {} }}", 
-               self.enabled, self.strength, self.radius)
+    #[test]
+    fn test_simulation_params_clamping() {
+        let params = crate::models::simulation_params::SimulationParams::new(5, 50.0, 0.001, 0.05);
+        assert_eq!(params.iterations, 10); // Clamped to min
+        assert_eq!(params.repulsion_strength, 100.0); // Clamped to min
+        assert_eq!(params.attraction_strength, 0.01); // Clamped to min
+        assert_eq!(params.damping, 0.1); // Clamped to min
+
+        let params = crate::models::simulation_params::SimulationParams::new(1000, 10000.0, 2.0, 1.0);
+        assert_eq!(params.iterations, 500); // Clamped to max
+        assert_eq!(params.repulsion_strength, 5000.0); // Clamped to max
+        assert_eq!(params.attraction_strength, 1.0); // Clamped to max
+        assert_eq!(params.damping, 0.9); // Clamped to max
+    }
+
+    #[test]
+    fn test_simulation_params_builder() {
+        let params = crate::models::simulation_params::SimulationParams::default()
+            .with_iterations(200)
+            .with_repulsion(2000.0)
+            .with_attraction(0.05)
+            .with_damping(0.7);
+
+        assert_eq!(params.iterations, 200);
+        assert_eq!(params.repulsion_strength, 2000.0);
+        assert_eq!(params.attraction_strength, 0.05);
+        assert_eq!(params.damping, 0.7);
     }
 }
diff --git a/src/models/edge.rs b/src/models/edge.rs
index f4b47acb..e5b2d4f1 100644
--- a/src/models/edge.rs
+++ b/src/models/edge.rs
@@ -7,7 +7,6 @@ pub struct Edge {
     pub source: String,
     pub target_node: String,
     pub weight: f32,
-    pub hyperlinks: f32,
 }
 
 // GPU representation of an edge, must match the shader's Edge struct
@@ -25,12 +24,11 @@ pub struct GPUEdge {
 }
 
 impl Edge {
-    pub fn new(source: String, target_node: String, weight: f32, hyperlinks: f32) -> Self {
+    pub fn new(source: String, target_node: String, weight: f32) -> Self {
         Self {
             source,
             target_node,
             weight,
-            hyperlinks,
         }
     }
 
diff --git a/src/models/metadata.rs b/src/models/metadata.rs
index 6d40b03a..8cfd67f3 100644
--- a/src/models/metadata.rs
+++ b/src/models/metadata.rs
@@ -7,6 +7,7 @@ use std::collections::HashMap;
 pub struct Metadata {
     pub file_name: String,
     pub file_size: usize,
+    pub node_size: f64,  // Added field for scaled node size
     pub hyperlink_count: usize,
     pub sha1: String,
     pub perplexity_link: String,
diff --git a/src/services/file_service.rs b/src/services/file_service.rs
index 5a2b62bf..107fdf07 100644
--- a/src/services/file_service.rs
+++ b/src/services/file_service.rs
@@ -1,7 +1,8 @@
 use crate::models::metadata::Metadata;
 use crate::config::Settings;
 use serde::{Deserialize, Serialize};
-use reqwest::{Client, header::{HeaderMap, HeaderValue, IF_NONE_MATCH, ETAG}};
+use reqwest::Client;
+use reqwest::header::{HeaderMap, HeaderValue, IF_NONE_MATCH, ETAG};
 use async_trait::async_trait;
 use log::{info, debug, error};
 use regex::Regex;
@@ -13,16 +14,16 @@ use chrono::{Utc, DateTime};
 use std::sync::Arc;
 use tokio::sync::RwLock;
 use std::error::Error as StdError;
-use futures::stream::{self, StreamExt};
+use futures::stream::StreamExt;
 use std::time::Duration;
 use tokio::time::sleep;
 
-// Rest of the file remains unchanged
+// Constants
 const METADATA_PATH: &str = "data/markdown/metadata.json";
 const MARKDOWN_DIR: &str = "data/markdown";
-const CACHE_DURATION: Duration = Duration::from_secs(300); // 5 minutes
-const MAX_CONCURRENT_DOWNLOADS: usize = 5;
 const GITHUB_API_DELAY: Duration = Duration::from_millis(100); // Rate limiting delay
+const MIN_NODE_SIZE: f64 = 5.0;
+const MAX_NODE_SIZE: f64 = 50.0;
 
 #[derive(Serialize, Deserialize, Clone)]
 pub struct GithubFile {
@@ -40,6 +41,8 @@ pub struct GithubFileMetadata {
     pub etag: Option<String>,
     #[serde(with = "chrono::serde::ts_seconds_option")]
     pub last_checked: Option<DateTime<Utc>>,
+    #[serde(with = "chrono::serde::ts_seconds_option")]
+    pub last_modified: Option<DateTime<Utc>>,
 }
 
 #[derive(Serialize, Deserialize, Clone)]
@@ -50,21 +53,10 @@ pub struct ProcessedFile {
     pub metadata: Metadata,
 }
 
-#[derive(Debug, Serialize, Deserialize)]
-struct TreeResponse {
-    sha: String,
-    tree: Vec<TreeItem>,
-    truncated: bool,
-}
-
-#[derive(Debug, Serialize, Deserialize)]
-struct TreeItem {
-    path: String,
-    mode: String,
-    #[serde(rename = "type")]
-    item_type: String,
-    sha: String,
-    url: Option<String>,
+// Structure to hold reference information
+#[derive(Default)]
+struct ReferenceInfo {
+    direct_mentions: usize,
 }
 
 #[async_trait]
@@ -72,6 +64,7 @@ pub trait GitHubService: Send + Sync {
     async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>>;
     async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>>;
     async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>>;
+    async fn get_file_last_modified(&self, file_path: &str) -> Result<DateTime<Utc>, Box<dyn StdError + Send + Sync>>;
 }
 
 pub struct RealGitHubService {
@@ -108,27 +101,11 @@ impl RealGitHubService {
             metadata_cache: Arc::new(RwLock::new(HashMap::new())),
         })
     }
+}
 
-    async fn fetch_directory_contents(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
-        // First, check the cache
-        {
-            let cache = self.metadata_cache.read().await;
-            let now = Utc::now();
-            
-            // If cache is fresh and not empty, use it
-            if !cache.is_empty() {
-                if let Some(first_item) = cache.values().next() {
-                    if let Some(last_checked) = first_item.last_checked {
-                        if (now - last_checked) < chrono::Duration::from_std(CACHE_DURATION).unwrap() {
-                            debug!("Using cached metadata for files");
-                            return Ok(cache.values().cloned().collect());
-                        }
-                    }
-                }
-            }
-        }
-
-        // Cache is stale or empty, fetch from GitHub
+#[async_trait]
+impl GitHubService for RealGitHubService {
+    async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
         let url = format!(
             "https://api.github.com/repos/{}/{}/contents/{}",
             self.owner, self.repo, self.base_path
@@ -141,90 +118,44 @@ impl RealGitHubService {
 
         let contents: Vec<serde_json::Value> = response.json().await?;
         
-        let markdown_files: Vec<GithubFileMetadata> = contents.into_iter()
-            .filter(|item| {
-                let is_file = item["type"].as_str().unwrap_or("") == "file";
-                let name = item["name"].as_str().unwrap_or("");
-                is_file && name.ends_with(".md")
-            })
-            .map(|item| {
-                GithubFileMetadata {
-                    name: item["name"].as_str().unwrap_or("").to_string(),
+        let mut markdown_files = Vec::new();
+        
+        for item in contents {
+            if item["type"].as_str().unwrap_or("") == "file" && 
+               item["name"].as_str().unwrap_or("").ends_with(".md") {
+                let name = item["name"].as_str().unwrap_or("").to_string();
+                let last_modified = self.get_file_last_modified(&format!("{}/{}", self.base_path, name)).await?;
+                
+                markdown_files.push(GithubFileMetadata {
+                    name,
                     sha: item["sha"].as_str().unwrap_or("").to_string(),
                     download_url: item["download_url"].as_str().unwrap_or("").to_string(),
                     etag: None,
                     last_checked: Some(Utc::now()),
-                }
-            })
-            .collect();
-
-        // Update cache
-        {
-            let mut cache = self.metadata_cache.write().await;
-            cache.clear();
-            for metadata in &markdown_files {
-                cache.insert(metadata.name.clone(), metadata.clone());
+                    last_modified: Some(last_modified),
+                });
             }
         }
 
-        debug!("Found {} markdown files in target directory", markdown_files.len());
         Ok(markdown_files)
     }
 
-    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
-        let mut headers = HeaderMap::new();
-        headers.insert("Authorization", HeaderValue::from_str(&format!("token {}", self.token))?);
-
-        // Get cached ETag if available
-        let etag = {
-            let cache = self.metadata_cache.read().await;
-            cache.values()
-                .find(|m| m.download_url == download_url)
-                .and_then(|m| m.etag.clone())
-        };
-
-        if let Some(etag) = etag {
-            headers.insert(IF_NONE_MATCH, HeaderValue::from_str(&etag)?);
-        }
-
-        let response = self.client.get(download_url)
-            .headers(headers)
-            .send()
-            .await?;
-
-        // Update ETag in cache if provided
-        if let Some(new_etag) = response.headers().get(ETAG) {
-            let mut cache = self.metadata_cache.write().await;
-            if let Some(metadata) = cache.values_mut().find(|m| m.download_url == download_url) {
-                metadata.etag = Some(new_etag.to_str()?.to_string());
-            }
-        }
-
-        if response.status() == reqwest::StatusCode::NOT_MODIFIED {
-            // Use cached content
-            let path = format!("{}/{}", MARKDOWN_DIR, download_url.split('/').last().unwrap_or(""));
-            if let Ok(content) = fs::read_to_string(&path) {
-                return Ok(content);
-            }
-        }
-
-        let content = response.text().await?;
-        Ok(content)
+    // Add the calculate_node_size function to FileService impl
+fn calculate_node_size(file_size: usize) -> f64 {
+    if file_size == 0 {
+        return MIN_NODE_SIZE;
     }
 
-    pub async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
-        self.fetch_directory_contents().await
-    }
+    let size_f64: f64 = file_size as f64;
+    let log_size = f64::log10(size_f64 + 1.0);
+    let min_log = f64::log10(1.0);
+    let max_log = f64::log10(269425.0 + 1.0); // Maximum known file size + 1
+    
+    MIN_NODE_SIZE + (log_size - min_log) * (MAX_NODE_SIZE - MIN_NODE_SIZE) / (max_log - min_log)
+}
 
-    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
-        // Check cache first
-        {
-            let cache = self.metadata_cache.read().await;
-            if let Some(metadata) = cache.get(file_name) {
-                return Ok(Some(metadata.download_url.clone()));
-            }
-        }
 
+    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
         let url = format!("https://api.github.com/repos/{}/{}/contents/{}/{}", 
             self.owner, self.repo, self.base_path, file_name);
 
@@ -240,58 +171,80 @@ impl RealGitHubService {
             Ok(None)
         }
     }
-}
 
-#[async_trait]
-impl GitHubService for RealGitHubService {
-    async fn fetch_file_metadata(&self) -> Result<Vec<GithubFileMetadata>, Box<dyn StdError + Send + Sync>> {
-        self.fetch_file_metadata().await
-    }
+    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
+        let mut headers = HeaderMap::new();
+        headers.insert("Authorization", HeaderValue::from_str(&format!("token {}", self.token))?);
 
-    async fn get_download_url(&self, file_name: &str) -> Result<Option<String>, Box<dyn StdError + Send + Sync>> {
-        self.get_download_url(file_name).await
+        let response = self.client.get(download_url)
+            .headers(headers)
+            .send()
+            .await?;
+
+        let content = response.text().await?;
+        Ok(content)
     }
 
-    async fn fetch_file_content(&self, download_url: &str) -> Result<String, Box<dyn StdError + Send + Sync>> {
-        self.fetch_file_content(download_url).await
+    async fn get_file_last_modified(&self, file_path: &str) -> Result<DateTime<Utc>, Box<dyn StdError + Send + Sync>> {
+        let url = format!(
+            "https://api.github.com/repos/{}/{}/commits",
+            self.owner, self.repo
+        );
+
+        let response = self.client.get(&url)
+            .header("Authorization", format!("token {}", self.token))
+            .query(&[("path", file_path), ("per_page", "1")])
+            .send()
+            .await?;
+
+        let commits: Vec<serde_json::Value> = response.json().await?;
+        
+        if let Some(last_commit) = commits.first() {
+            if let Some(commit) = last_commit["commit"]["committer"]["date"].as_str() {
+                if let Ok(date) = DateTime::parse_from_rfc3339(commit) {
+                    return Ok(date.with_timezone(&Utc));
+                }
+            }
+        }
+        
+        Ok(Utc::now())
     }
 }
 
 pub struct FileService;
 
 impl FileService {
-    /// Check if we have a valid local setup
-    fn has_valid_local_setup() -> bool {
-        // Check if metadata.json exists and is not empty
-        if let Ok(metadata_content) = fs::read_to_string(METADATA_PATH) {
-            if metadata_content.trim().is_empty() {
-                return false;
-            }
-            
-            // Try to parse metadata to ensure it's valid
-            if let Ok(metadata_map) = serde_json::from_str::<HashMap<String, Metadata>>(&metadata_content) {
-                if metadata_map.is_empty() {
-                    return false;
-                }
-                
-                // Check if the markdown files referenced in metadata actually exist
-                for (filename, _) in metadata_map {
-                    let file_path = format!("{}/{}", MARKDOWN_DIR, filename);
-                    if !Path::new(&file_path).exists() {
-                        return false;
-                    }
-                }
-                
-                return true;
-            }
+    /// Extract both direct mentions and hyperlink references to other files
+fn extract_references(content: &str, valid_nodes: &[String]) -> HashMap<String, ReferenceInfo> {
+    let mut references = HashMap::new();
+    
+    for node_name in valid_nodes {
+        let mut ref_info = ReferenceInfo::default();
+        
+        // Count direct mentions (case insensitive)
+        let direct_pattern = format!(r"(?i)\[\[{}]]|\b{}\b", regex::escape(node_name), regex::escape(node_name));
+        if let Ok(re) = Regex::new(&direct_pattern) {
+            ref_info.direct_mentions = re.find_iter(content).count();
+        }
+        
+        if ref_info.direct_mentions > 0 {
+            references.insert(node_name.clone(), ref_info);
         }
-        false
+    }
+    
+    references
+}
+
+fn convert_references_to_topic_counts(references: HashMap<String, ReferenceInfo>) -> HashMap<String, usize> {
+    references.into_iter()
+        .map(|(name, info)| (name, info.direct_mentions))
+        .collect()
     }
 
     /// Initialize the local markdown directory and metadata structure.
     pub async fn initialize_local_storage(
         github_service: &dyn GitHubService,
-        settings: Arc<RwLock<Settings>>,
+        _settings: Arc<RwLock<Settings>>,
     ) -> Result<(), Box<dyn StdError + Send + Sync>> {
         info!("Checking local storage status");
         
@@ -306,23 +259,16 @@ impl FileService {
 
         info!("Initializing local storage with files from GitHub");
 
-        // Get topics from settings
-        let settings = settings.read().await;
-        let topics = settings.topics.clone();
-
         // Step 1: Get all markdown files from GitHub
         let github_files = github_service.fetch_file_metadata().await?;
         info!("Found {} markdown files in GitHub", github_files.len());
 
-        let mut metadata_map = HashMap::new();
-        let mut processed_count = 0;
-        let mut total_files = 0;
-
-        // Step 2: Download and process each file
+        let mut file_sizes = HashMap::new();
+        let mut file_contents = HashMap::new();
+        let mut file_metadata = HashMap::new();
+        
+        // Step 2: First pass - collect all files and their contents
         for file_meta in github_files {
-            total_files += 1;
-            
-            // Download file content
             match github_service.fetch_file_content(&file_meta.download_url).await {
                 Ok(content) => {
                     // Check if file starts with "public:: true"
@@ -332,68 +278,97 @@ impl FileService {
                         continue;
                     }
 
-                    let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
-                    
-                    // Calculate SHA1 of content
-                    let local_sha1 = Self::calculate_sha1(&content);
-                    
-                    // Save file content
-                    fs::write(&file_path, &content)?;
-                    processed_count += 1;
-
-                    // Extract topics from content
-                    let topic_counts = Self::extract_topics(&content, &topics);
-
-                    // Create metadata entry
-                    let metadata = Metadata {
-                        file_name: file_meta.name.clone(),
-                        file_size: content.len(),
-                        hyperlink_count: Self::count_hyperlinks(&content),
-                        sha1: local_sha1,
-                        last_modified: Utc::now(),
-                        perplexity_link: String::new(),
-                        last_perplexity_process: None,
-                        topic_counts,
-                    };
-
-                    metadata_map.insert(file_meta.name.clone(), metadata);
-                    info!("Processed public file: {}", file_meta.name);
+                    let node_name = file_meta.name.trim_end_matches(".md").to_string();
+                    file_sizes.insert(node_name.clone(), content.len());
+                    file_contents.insert(node_name, content);
+                    file_metadata.insert(file_meta.name.clone(), file_meta);
                 }
                 Err(e) => {
                     error!("Failed to fetch content for {}: {}", file_meta.name, e);
                 }
             }
-
-            // Add delay for rate limiting
             sleep(GITHUB_API_DELAY).await;
         }
 
-        // Step 3: Save metadata
+        // Get list of valid node names (filenames without .md)
+        let valid_nodes: Vec<String> = file_contents.keys().cloned().collect();
+
+        // Step 3: Second pass - extract references and create metadata
+        let mut metadata_map = HashMap::new();
+        
+        for (node_name, content) in &file_contents {
+            let file_name = format!("{}.md", node_name);
+            let file_path = format!("{}/{}", MARKDOWN_DIR, file_name);
+            
+            // Calculate SHA1 of content
+            let local_sha1 = Self::calculate_sha1(content);
+            
+            // Save file content
+            fs::write(&file_path, content)?;
+
+            // Extract references
+            let references = Self::extract_references(content, &valid_nodes);
+            let topic_counts = Self::convert_references_to_topic_counts(references);
+
+            // Get GitHub metadata
+            let github_meta = file_metadata.get(&file_name).unwrap();
+            let last_modified = github_meta.last_modified.unwrap_or_else(|| Utc::now());
+
+            // Calculate node size
+            let file_size = *file_sizes.get(node_name).unwrap();
+            let node_size = Self::calculate_node_size(file_size);
+
+            // Create metadata entry
+            let metadata = Metadata {
+                file_name: file_name.clone(),
+                file_size,
+                node_size,
+                hyperlink_count: Self::count_hyperlinks(content),
+                sha1: local_sha1,
+                last_modified,
+                perplexity_link: String::new(),
+                last_perplexity_process: None,
+                topic_counts,
+            };
+
+            metadata_map.insert(file_name, metadata);
+        }
+
+        // Step 4: Save metadata
         info!("Saving metadata for {} public files", metadata_map.len());
         Self::save_metadata(&metadata_map)?;
 
-        info!("Initialization complete. Found {} total files, processed {} public files", 
-            total_files, processed_count);
+        info!("Initialization complete. Processed {} public files", metadata_map.len());
 
         Ok(())
     }
 
-    /// Extract topics from content
-    fn extract_topics(content: &str, topics: &[String]) -> HashMap<String, usize> {
-        let mut topic_counts = HashMap::new();
-        
-        // Convert content to lowercase for case-insensitive matching
-        let content_lower = content.to_lowercase();
-        
-        for topic in topics {
-            let topic_lower = topic.to_lowercase();
-            let count = content_lower.matches(&topic_lower).count();
-            if count > 0 {
-                topic_counts.insert(topic.clone(), count);
+    /// Check if we have a valid local setup
+    fn has_valid_local_setup() -> bool {
+        // Check if metadata.json exists and is not empty
+        if let Ok(metadata_content) = fs::read_to_string(METADATA_PATH) {
+            if metadata_content.trim().is_empty() {
+                return false;
+            }
+            
+            // Try to parse metadata to ensure it's valid
+            if let Ok(metadata_map) = serde_json::from_str::<HashMap<String, Metadata>>(&metadata_content) {
+                if metadata_map.is_empty() {
+                    return false;
+                }
+                
+                // Check if the markdown files referenced in metadata actually exist
+                for (filename, _) in metadata_map {
+                    let file_path = format!("{}/{}", MARKDOWN_DIR, filename);
+                    if !Path::new(&file_path).exists() {
+                        return false;
+                    }
+                }
+                
+                return true;
             }
         }
-        
-        topic_counts
+        false
     }
 
     /// Ensures all required directories exist
@@ -409,16 +384,12 @@ impl FileService {
     /// Handles incremental updates after initial setup
     pub async fn fetch_and_process_files(
         github_service: &dyn GitHubService,
-        settings: Arc<RwLock<Settings>>,
+        _settings: Arc<RwLock<Settings>>,
         metadata_map: &mut HashMap<String, Metadata>,
     ) -> Result<Vec<ProcessedFile>, Box<dyn StdError + Send + Sync>> {
         // Ensure directories exist before any operations
         Self::ensure_directories()?;
 
-        // Get topics from settings
-        let settings = settings.read().await;
-        let topics = settings.topics.clone();
-
         // Get metadata for markdown files in target directory
         let github_files_metadata = github_service.fetch_file_metadata().await?;
         debug!("Fetched metadata for {} markdown files", github_files_metadata.len());
@@ -448,7 +419,12 @@ impl FileService {
             }
         }
 
-        // Process files in parallel with rate limiting
+        // Get list of valid node names (filenames without .md)
+        let valid_nodes: Vec<String> = github_files_metadata.iter()
+            .map(|f| f.name.trim_end_matches(".md").to_string())
+            .collect();
+
+        // Process files that need updating
         let files_to_process: Vec<_> = github_files_metadata.into_iter()
             .filter(|file_meta| {
                 let local_meta = metadata_map.get(&file_meta.name);
@@ -456,71 +432,52 @@ impl FileService {
             })
             .collect();
 
-        let results = stream::iter(files_to_process)
-            .map(|file_meta| {
-                let github_service = github_service;
-                let topics = topics.clone();
-                async move {
-                    // Add delay for rate limiting
-                    sleep(GITHUB_API_DELAY).await;
-
-                    // Download content
-                    match github_service.fetch_file_content(&file_meta.download_url).await {
-                        Ok(content) => {
-                            // Check if file starts with "public:: true"
-                            let first_line = content.lines().next().unwrap_or("").trim();
-                            if first_line != "public:: true" {
-                                debug!("Skipping non-public file: {}", file_meta.name);
-                                return Ok(None);
-                            }
-                            
-                            let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
-                            fs::write(&file_path, &content)?;
-                            
-                            // Extract topics from content
-                            let topic_counts = Self::extract_topics(&content, &topics);
-
-                            let new_metadata = Metadata {
-                                file_name: file_meta.name.clone(),
-                                file_size: content.len(),
-                                hyperlink_count: Self::count_hyperlinks(&content),
-                                sha1: file_meta.sha.clone(),
-                                last_modified: Utc::now(),
-                                perplexity_link: String::new(),
-                                last_perplexity_process: None,
-                                topic_counts,
-                            };
-                            
-                            Ok(Some(ProcessedFile {
-                                file_name: file_meta.name,
-                                content,
-                                is_public: true,
-                                metadata: new_metadata,
-                            }))
-                        }
-                        Err(e) => {
-                            error!("Failed to fetch content: {}", e);
-                            Err(e)
-                        }
+        // Process each file
+        for file_meta in files_to_process {
+            match github_service.fetch_file_content(&file_meta.download_url).await {
+                Ok(content) => {
+                    let first_line = content.lines().next().unwrap_or("").trim();
+                    if first_line != "public:: true" {
+                        debug!("Skipping non-public file: {}", file_meta.name);
+                        continue;
                     }
+
+                    let file_path = format!("{}/{}", MARKDOWN_DIR, file_meta.name);
+                    fs::write(&file_path, &content)?;
+
+                    // Extract references
+                    let references = Self::extract_references(&content, &valid_nodes);
+                    let topic_counts = Self::convert_references_to_topic_counts(references);
+
+                    // Calculate node size
+                    let file_size = content.len();
+                    let node_size = Self::calculate_node_size(file_size);
+
+                    let new_metadata = Metadata {
+                        file_name: file_meta.name.clone(),
+                        file_size,
+                        node_size,
+                        hyperlink_count: Self::count_hyperlinks(&content),
+                        sha1: Self::calculate_sha1(&content),
+                        last_modified: file_meta.last_modified.unwrap_or_else(|| Utc::now()),
+                        perplexity_link: String::new(),
+                        last_perplexity_process: None,
+                        topic_counts,
+                    };
+
+                    metadata_map.insert(file_meta.name.clone(), new_metadata.clone());
+                    processed_files.push(ProcessedFile {
+                        file_name: file_meta.name,
+                        content,
+                        is_public: true,
+                        metadata: new_metadata,
+                    });
                 }
-            })
-            .buffer_unordered(MAX_CONCURRENT_DOWNLOADS)
-            .collect::<Vec<_>>()
-            .await;
-
-        // Process results
-        for result in results {
-            match result {
-                Ok(Some(processed_file)) => {
-                    let file_name = processed_file.file_name.clone();
-                    metadata_map.insert(file_name.clone(), processed_file.metadata.clone());
-                    processed_files.push(processed_file);
-                    debug!("Successfully processed public file: {}", file_name);
+                Err(e) => {
+                    error!("Failed to fetch content: {}", e);
                 }
-                Ok(None) => {} // Skip non-public files
-                Err(e) => error!("Error processing file: {}", e),
             }
+            sleep(GITHUB_API_DELAY).await;
         }
 
         // Save updated metadata
diff --git a/src/services/graph_service.rs b/src/services/graph_service.rs
index 0fc46028..62d99d85 100644
--- a/src/services/graph_service.rs
+++ b/src/services/graph_service.rs
@@ -1,5 +1,3 @@
-// src/services/graph_service.rs
-
 use crate::AppState;
 use crate::models::graph::GraphData;
 use crate::models::node::Node;
@@ -26,13 +24,19 @@ impl GraphService {
         let metadata: HashMap<String, Metadata> = serde_json::from_str(&metadata_content)?;
     
         let mut graph = GraphData::default();
-        let mut edge_map: HashMap<(String, String), (f32, u32)> = HashMap::new();
+        let mut edge_map: HashMap<(String, String), f32> = HashMap::new();
     
         // Create nodes
         for (file_name, file_metadata) in &metadata {
             let node_id = file_name.trim_end_matches(".md").to_string();
             let mut node_metadata = HashMap::new();
+            
+            // Add file metadata to node
             node_metadata.insert("file_size".to_string(), file_metadata.file_size.to_string());
+            node_metadata.insert("node_size".to_string(), file_metadata.node_size.to_string());
+            node_metadata.insert("last_modified".to_string(), file_metadata.last_modified.to_string());
+            node_metadata.insert("hyperlink_count".to_string(), file_metadata.hyperlink_count.to_string());
+            
             graph.nodes.push(Node {
                 id: node_id.clone(),
                 label: node_id.clone(),
@@ -43,38 +47,31 @@ impl GraphService {
             graph.metadata.insert(node_id.clone(), file_metadata.clone());
         }
     
-        // Build edges
+        // Build edges from topic counts
         for (file_name, file_metadata) in &metadata {
-            let node_id = file_name.trim_end_matches(".md").to_string();
-            for (other_file, _) in &metadata {
-                if file_name != other_file {
-                    let other_node_id = other_file.trim_end_matches(".md");
-                    let count = file_metadata.topic_counts.get(other_node_id).cloned().unwrap_or(0) as f32;
-                    if count > 0.0 {
-                        let edge_key = if node_id < other_node_id.to_string() {
-                            (node_id.clone(), other_node_id.to_string())
-                        } else {
-                            (other_node_id.to_string(), node_id.clone())
-                        };
-                        edge_map.entry(edge_key)
-                            .and_modify(|(weight, hyperlinks)| {
-                                *weight += count;
-                                *hyperlinks += file_metadata.hyperlink_count as u32;
-                            })
-                            .or_insert((count, file_metadata.hyperlink_count as u32));
-                    }
+            let source_id = file_name.trim_end_matches(".md").to_string();
+            
+            // Look through all references in topic_counts
+            for (target_id, reference_count) in &file_metadata.topic_counts {
+                if source_id != *target_id {
+                    // Create a canonical edge key (alphabetically ordered)
+                    let edge_key = if source_id < *target_id {
+                        (source_id.clone(), target_id.clone())
+                    } else {
+                        (target_id.clone(), source_id.clone())
+                    };
+
+                    // Update edge weight
+                    edge_map.entry(edge_key)
+                        .and_modify(|weight| *weight += *reference_count as f32)
+                        .or_insert(*reference_count as f32);
                 }
             }
         }
     
         // Convert edge_map to edges
-        graph.edges = edge_map.into_iter().map(|((source, target), (weight, hyperlinks))| {
-            Edge {
-                source,
-                target_node: target,
-                weight,
-                hyperlinks: hyperlinks as f32,
-            }
+        graph.edges = edge_map.into_iter().map(|((source, target), weight)| {
+            Edge::new(source, target, weight)
         }).collect();
         
         info!("Graph data built with {} nodes and {} edges", graph.nodes.len(), graph.edges.len());
@@ -106,7 +103,7 @@ impl GraphService {
         match gpu_compute {
             Some(gpu) => {
                 info!("Using GPU for layout calculation");
-                let mut gpu_compute = gpu.write().await; // Acquire write lock
+                let mut gpu_compute = gpu.write().await;
                 gpu_compute.set_graph_data(graph)?;
                 gpu_compute.set_force_directed_params(params)?;
                 gpu_compute.compute_forces()?;
@@ -166,6 +163,8 @@ impl GraphService {
                 let dy = graph.nodes[target].y - graph.nodes[source].y;
                 let dz = graph.nodes[target].z - graph.nodes[source].z;
                 let distance = (dx * dx + dy * dy + dz * dz).sqrt().max(0.1);
+                
+                // Use only weight for attraction
                 let force = attraction * distance * edge.weight;
                 let fx = force * dx / distance;
                 let fy = force * dy / distance;
