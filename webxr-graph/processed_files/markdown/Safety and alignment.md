public:: true

- What used to be called bias whet I was doing postgrad Machine Learning (2020) is now called [[Safety and alignment]].
	- Bias
		- [[2309.17012] Benchmarking Cognitive Biases in Large Language Models as Evaluators (arxiv.org)](https://arxiv.org/abs/2309.17012)
		- [[confusion matrices]](https://en.wikipedia.org/wiki/Confusion_matrix)
		- [Responsible Capability Scaling \ Anthropic](https://www.anthropic.com/uk-government-internal-ai-safety-policy-response/responsible-capability-scaling)
- # Dense summary of the moment
	- This is an excellent blog post which enumerates important points. Samuel Hammond presents a collection of concise statements covering a wide range of topics related to the current state and future implications of artificial intelligence. The theses highlight the potential impacts of AI on society, the importance of AI safety and alignment, and the role of AI in shaping humanity's future. Hammond emphasizes the need for monitoring frontier AI capabilities, discusses the debate between open and closed source AI, and explores the potential for AI to disrupt existing institutions and power balances.
		- <iframe src="https://www.secondbest.ca/p/ninety-five-theses-on-ai" style="width: 100%; height: 800px"></iframe>
- ## March 2024 Gladstone USA Report
	- Commissioned by the U.S. government, this report underscores the potential for artificial intelligence to pose substantial national security risks, including the possibility of an extinction-level threat.
		- **Gladstone's Role and Perspective**
			- **Engagement with the U.S. Government**: Since 2021, Gladstone, led by the Harris brothers, has briefed the U.S. government on AI risks.
			- **Contract Award**: Gladstone was selected to produce the report, emphasizing the firm's deep involvement in shaping the discourse on AI safety.
		- [Action Plan to increase the safety and security of advanced AI (gladstone.ai)](https://www.gladstone.ai/action-plan)
		- **Essential Findings from the Report**
			- **Risk Assessment**: The development of current frontier AI technology presents "urgent and growing risks to national security."
			- **Historical Parallel**: The destabilizing potential of advanced AI and AGI is likened to the advent of nuclear weapons, suggesting profound global security implications.
			- **Weapons of Mass Destruction**: Advances in AI are creating "entirely new categories" of WMDs, emphasizing the unprecedented nature of these risks.
			- **Competitive Pressures**: A significant driver of these risks is identified as the competitive dynamic among leading AI labs, highlighting a rush towards developing advanced AI systems despite acknowledged dangers.
		- **Proposed Action Plan**
			- **Title of Plan**: "Defense in Depth: An Action Plan to Increase the Safety and Security of Advanced AI"
			- **Core Strategies**:
				- Introduction of interim safeguards to stabilize AI development.
				- Creation of a framework for basic regulatory oversight.
				- Establishment of a domestic legal regime for responsible AI development and adoption.
				- Extension of regulatory measures to international cooperation and standards.
			- **Specific Recommendations from the Report**
				- Proposes a limit on the computing power used for AI model training.
				- Suggests the formation of a new federal AI agency to oversee critical thresholds and regulatory compliance.
				- Recommends considering the prohibition of the publication of the inner workings of powerful AI models.
				- Advocates for stricter controls over the manufacture and export of AI chips and increased funding towards alignment research for safer AI.
		- **Support from AI Safety Advocates**: The report’s urgent tone and recommendations found resonance among AI safety advocates.
		- **Skepticism from Critics**: Some viewed the report as overly alarmist, with criticisms ranging from dismissive to mocking the idea of government superiority in AI management.
		- The discourse surrounding the government-commissioned AI report reflects a broad spectrum of opinions, underscoring the complexity of AI's impact on society and the necessity for informed, multifaceted policy approaches.
- ## What the researchers think (feels and vibes)
	- ![](https://jnnnthnn.com/leike.png)
	- A survey of 2778 AI researchers, to assess the pace of AI progress and the broader societal implications. The increased participation in this third iteration points to growing importance and concern surrounding AI in the scientific community.
	- Most of the 39 tasks will likely be feasible within the next ten years, showcasing AI's anticipated versatility and rapid advancement. It's cheaper, so it will likely become ubiquitous without a new [[Social contract and jobs]] initiative.
	- Median prediction indicates a 50% chance of achieving High-Level Machine Intelligence by 2047 and Full Automation of Labour, by 2116
	- Strong hints of potential differences in technological development speeds, cultural attitudes, or economic motivations across regions. This suggests incoming legislative arbitrage.
		- [EU’s new AI Act risks hampering innovation, warns Emmanuel Macron (ft.com)](https://www.ft.com/content/9339d104-7b0c-42b8-9316-72226dd4e4c0)
		- [Japan Goes All In: Copyright Doesn't Apply To AI Training | News | Communications of the ACM](https://cacm.acm.org/news/273479-japan-goes-all-in-copyright-doesnt-apply-to-ai-training/fulltext#:~:text=In%20a%20surprising%20move%2C%20Japan%27s%20government%20recently%20reaffirmed,is%20content%20obtained%20from%20illegal%20sites%20or%20otherwise.%22)
		- [China’s plan to judge the safety of generative AI | MIT Technology Review](https://www.technologyreview.com/2023/10/18/1081846/generative-ai-safety-censorship-china/)
	- Broad agreement exists on some future AI traits, like finding unexpected ways to achieve goals, but significant uncertainty remains, especially for traits with sinister implications.
	- Scepticism exists about future AI systems' ability to provide intelligible and truthful explanations of decisions, posing challenges for risk management and bias mitigation.
	- Researchers express substantial concern for various AI-related scenarios, particularly the spread of false information and manipulation of public opinion.
	- A considerable fraction of respondents attribute a non-trivial probability to AI leading to human extinction or severe disempowerment.
- ## Stats from the report
	- **Over 95%** concerned about:
		- Dangerous groups using AI for engineered viruses.
		- AI manipulating large-scale public opinion.
		- AI spreading false information.
	- **Over 90%** concerned about:
		- Authoritarian rulers using AI for control.
		- AI worsening economic inequality.
		- Bias in AI, e.g., gender or race discrimination.
	- **Over 80%** concerned about:
		- Misaligned AI goals leading to catastrophic outcomes.
		- Reduced human interaction due to AI.
		- Automation leading to widespread economic disempowerment.
	- **Over 70%** concerned about automation causing a loss of meaning in life.
	- **Only 20%** confident in understanding AI "thinking" by 2028.
	- Researchers emphasize safety and alignment as priority (10:1 margin).
	- **58%** see at least a **5% chance of AI ending humanity**.
	- Risk of severe disempowerment of human species at **16.2%** (comparable to Russian Roulette).
	- **10% chance by 2027** and **50% chance by 2047** for AI to outperform humans in every task, **13 years sooner** than previous estimates.
- [Thousands_of_AI_authors_on_the_future_of_AI.pdf (aiimpacts.org)](https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf)
- ![image.png](../assets/image_1704446789913_0.png){:width 800, :height 684}
- # Disallowed uses
	- [Usage policies (openai.com)](https://openai.com/policies/usage-policies)
	  id:: 659e5627-97e4-40f9-84fa-128b35f7f920
	- Illegal activity
		- OpenAI prohibits the use of our models, tools, and services for illegal activity.
	- Child Sexual Abuse Material or any content that exploits or harms children
		- We report CSAM to the National Center for Missing and Exploited Children.
	- Generation of hateful, harassing, or violent content
		- Content that expresses, incites, or promotes hate based on identity
		- Content that intends to harass, threaten, or bully an individual
		- Content that promotes or glorifies violence or celebrates the suffering or humiliation of others
	- Generation of malware
		- Content that attempts to generate code that is designed to disrupt, damage, or gain unauthorized access to a computer system.
	- Activity that has high risk of physical harm, including:
		- Weapons development
		- Military and warfare
		- Management or operation of critical infrastructure in energy, transportation, and water
		- Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders
	- Activity that has high risk of economic harm, including:
		- Multi-level marketing
		- Gambling
		- Payday lending
		- Automated determinations of eligibility for credit, employment, educational institutions, or public assistance services
	- Fraudulent or deceptive activity, including:
		- Scams
		- Coordinated inauthentic behavior
		- Plagiarism
		- Academic dishonesty
		- Astroturfing, such as fake grassroots support or fake review generation
		- Disinformation
		- Spam
		- Pseudo-pharmaceuticals
	- Adult content, adult industries, and dating apps, including:
		- Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness)
		- Erotic chat
		- Pornography
	- Political campaigning or lobbying, by:
		- Generating high volumes of campaign materials
		- Generating campaign materials personalized to or targeted at specific demographics
		- Building conversational or interactive systems such as chatbots that provide information about campaigns or engage in political advocacy or lobbying
		- Building products for political campaigning or lobbying purposes
	- Activity that violates people’s privacy, including:
		- Tracking or monitoring an individual without their consent
		- Facial recognition of private individuals
		- Classifying individuals based on protected characteristics
		- Using biometrics for identification or assessment
		- Unlawful collection or disclosure of personal identifiable information or educational, financial, or other protected records
	- Engaging in the unauthorized practice of law, or offering tailored legal advice without a qualified person reviewing the information
		- OpenAI’s models are not fine-tuned to provide legal advice. You should not rely on our models as a sole source of legal advice.
	- Offering tailored financial advice without a qualified person reviewing the information
		- OpenAI’s models are not fine-tuned to provide financial advice. You should not rely on our models as a sole source of financial advice.
	- Telling someone that they have or do not have a certain health condition, or providing instructions on how to cure or treat a health condition
		- OpenAI’s models are not fine-tuned to provide medical information. You should never use our models to provide diagnostic or treatment services for serious medical conditions.
		- OpenAI’s platforms should not be used to triage or manage life-threatening issues that need immediate attention.
	- High risk government decision-making, including:
		- Law enforcement and criminal justice
		- Migration and asylum
- ## Jailbreaking
	- {{embed ((661d5f7f-e2b4-4f0b-931a-3590c52f1e34))}}
	- {{embed ((661e41bc-42da-4bbd-a1c9-32892bd2d43a))}}
	- # Kill Switches?
		- ```In situations where AI systems pose catastrophic risks, it could be beneficial for regulators to verify that a set of AI chips are operated legitimately or to disable their operation (or a subset of it) if they violate rules.```
			- <iframe src="https://www.cser.ac.uk/media/uploads/files/Computing-Power-and-the-Governance-of-AI.pdf" style="width: 100%; height: 600px"></iframe>