public:: true

- #Public page automatically published
- {{video https://www.youtube.com/watch?v=fa8k8IQ1_X0}}
- [OpenAI](https://www.linkedin.com/company/openai/) cofounder [](https://www.linkedin.com/in/ACoAAAOshwUBNNaU-8JRTNkhsmgKZnIXot9YbHk)[Ilya Sutskever](https://www.linkedin.com/in/ilya-sutskever/) is launching a new company with the sole objective of creating [Safe Superintelligence](https://ssi.inc/) (SSI).
	- ```"We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to advance capabilities as fast as possible while making sure our safety always remains ahead. This way, we can scale in peace."```
- <iframe src="https://www.oneusefulthing.org/p/superhuman" style="width: 100%; height: 600px"></iframe>
- ![image.png](../assets/image_1714292617207_0.png)
- [Tsarathustra on X: "Nick Bostrom: superintelligence could happen in timelines as short as a year and is the last invention we will ever need to make https://t.co/oTZoF5KOVq" / X (twitter.com)](https://twitter.com/tsarnick/status/1784378045069217960)
	- {{twitter https://twitter.com/tsarnick/status/1784378045069217960}}
	-
- # Numbers and Figures
	- GPT-3 trained on 1 trillion tokens or 600 GB of text
	- GPT-4 estimated to have trained on 10 trillion tokens
	- GPT-5 projected to require 100 trillion tokens based on current scaling trends
	- Chinchilla scaling laws: 10x increase in dataset size corresponds to 100x increase in compute
	- 64 zettabytes (10^22 bytes) of data created in 2020
	- 60-100 sextillion words (10^20 to 10^22) if all 2020 data converted to text
	- 100 billion humans estimated to have lived thus far
	- 13,000 times more data created in 2020 than all words spoken by humans throughout history
	- 333 billion emails sent each day
	- 1 in 100 million parts of raw 2020 data would need to be high-quality text to reach GPT-5 scale
	- Twitter generates 33 terabytes of text per year, enough to train 2-3x size of GPT-4
	- 1-2 exabytes each of astronomy and YouTube data generated annually
	- 40 exabytes of genomics data stored every year
	- Largest conceivable training run estimated at $1 trillion, 1% of gross world product
	- Human brain estimated to process 2-3 quadrillion bytes over 70 years, 200-300x GPT-4 training data
	- This gets unlocked by in context learning and better compute. There seems unlikely to be a blocker. Likely 2030
- [Microsoft president says no chance of super-intelligent AI soon | Reuters](https://www.reuters.com/technology/microsoft-president-says-no-chance-superintelligent-ai-soon-2023-11-30/)
-