public:: true

-
- This paper was submitted to arXiv on June 17, 2021. It proposes a method called **Low-Rank Adaptation, or LoRA, that reduces the number of trainable parameters for downstream tasks in natural language processing. LoRA injects trainable rank decomposition matrices into each layer of the Transformer architecture, which greatly reduces the number of trainable parameters. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency (time to output). It also works for image diffusion.
-
- [Introducing DoRA, a High-Performing Alternative to LoRA for Fine-Tuning | NVIDIA Technical Blog](https://developer.nvidia.com/blog/introducing-dora-a-high-performing-alternative-to-lora-for-fine-tuning/)
-
- [catid/dora: Implementation of DoRA (github.com)](https://github.com/catid/dora) new kid?
	- ![GGd8HodXoAAL27r.png](../assets/GGd8HodXoAAL27r_1708191335486_0.png)
-
-
-
- [LoRA training scripts of the world, unite! (huggingface.co)](https://huggingface.co/blog/sdxl_lora_advanced_script)
- [[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models (arxiv.org)](https://arxiv.org/abs/2106.09685)
-
- Huggingface [[Large language models]] [[LoRA DoRA etc]] and [[LoRA DoRA etc]] can be found with a simply filter.
	- [Models
		- Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&other=lora&sort=trending)