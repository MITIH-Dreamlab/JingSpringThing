public:: true


#Public page

	- automatically published
- # Spatial and Augment Reality
- ## Apple Vision Pro
	- [This thing is mind-blowing ðŸ¤¯ This demo is better than Apple's actual ads for the Vision Pro. Apple launched the Vision Pro a few days ago,â€¦ | Instagram](https://www.instagram.com/reel/C2-G8WgsLSj/?igsh=am93cXF4OWppa29l)
	-
	- This review (for me) asks the right questions, finally, of "spatial".
		- https://www.theverge.com/24054862/apple-vision-pro-review-vr-ar-headset-features-price
	- Is using the Vision Pro so good that Iâ€™m willing to mess up my hair every time I put it on?
	- Is it so good that I want to lug it around in its giant carrying case instead of my laptop bag?
	- Is it so good that I want to look at the world around me through screens instead of with my own eyes?
	- Do I prefer using a computerÂ in thereÂ rather thanÂ out here.
	  
	  I stopped working in VR when I tore down my venerable 17 year old augmented virtuality system. That was the right mix, for me, but untenably expensive. Most people will just continue to regard the Apple HMD as an interface that does Apple credit, that they will probably never use. 
	  
	  They don't know that most of the owners will never use them either, but I do.
- {{video https://www.youtube.com/watch?v=aqTIB_q40bo}}
- [twitter link to the render loading below](https://twitter.com/bilawalsidhu/status/1759060086977143132)
  {{twitter https://twitter.com/bilawalsidhu/status/1759060086977143132}}
	- Meta research paper finally nails down focus with waveguide AR glasses [Waveguide holography for 3D augmented reality glasses | Nature Communications](https://www.nature.com/articles/s41467-023-44032-1)
- [CES (Pt. 2), Sony XR, DigiLens, Vuzix, Solos, EverySight, Mojie, TCL color ÂµLED â€“ KGOnTech (kguttag.com)](https://kguttag.com/2024/01/24/ces-pt-2-sony-xr-digilens-vuzix-solos-everysight-mojie-tcl-color-%c2%b5led/)
- [Campus](https://techcrunch-com.cdn.ampproject.org/c/s/techcrunch.com/2022/09/09/roblox-rdc-2022/amp/)
- Vircadia
- O3DE
- [server](https://www.reddit.com/r/O3DE/comments/pbovl9/can_i_develop_my_own_dedicated_server_with_o3de/)
- [Global lighting](https://www.co3dex.com/blog/image-based-lighting-1/#/)
- Unreal
- [Technically this might be a decade away since like everything the primay user base will be mobile mixed realitym, which is contingent on 5G](https://www.matthewball.vc/all/forwardtothemetaverseprimer)
- book, the metaverse and how it will revolutise everything
	- ball2022metaverse
	- challenges
- bandwidth
	- latency
	- global shared truth
	- form factor
	- gpu processing
- [[Humans, Avatars , Character]]
- [the many challenges of XR hardware](https://www.matthewball.vc/all/why-vrar-gets-farther-away-as-it-comes-into-focus)
- HCI
- MoveAI
- [Meta's wrist reader](https://www.from-the-interface.com/wrist-interfaces/)
- [Touch music interface](https://scitechdaily.com/groundbreaking-new-technology-allows-people-to-listen-to-music-through-touch/)
- Interface and tracking
- Pose estimations
- [Standable](https://www.standablevr.com/)
- [Dense face fields from Microsoft](https://microsoft.github.io/DenseLandmarks/)
- Viveverse web3 nonsense
- [Meetungs ARE the work](https://medium.com/@ElizAyer/meetings-are-the-work-9e429dde6aa3)
- Identity
- [Strongnode identity article on venturebeat](https://venturebeat.com/virtual/identity-in-the-metaverse-creating-a-global-identity-system/)
- [Solid-lite](https://solid-lite.org/)
- legal / governance / privacy / safeguarding
- legal jeopardy for celebrities
- [Gang sexual assault vice article](https://www.vice.com/en/article/3abpg3/woman-says-she-was-virtually-gang-raped-in-facebooks-metaverse?)
- [not enough training on safety in africa](https://www.thecable.ng/safeguarding-africans-safety-and-freedom-in-the-metaverse/amp)
- [How Regulation Will Apply To The Metaverse](https://www.forbes.com/sites/nisaamoils/2023/03/01/how-regulation-will-apply-to-the-metaverse/?)
- [Podcast on the law](https://www.reply.com/en/metaminutes-s3-e5-legal-challenges-and-regulation-for-the-metaverse)
- [dai.ki blog post](https://dai.ki/navigating-ai-governance-a-comprehensive-look-at-existing-and-new-eu-and-us-ai-regulations/)
- Bio vertical
	- [NGL protein fold model viewer](https://github.com/nglviewer/ngl)
	- [OpenBioML discord](discord.gg/AMRdyPjwBb)
	- [Nanome on quest pro](https://www.youtube.com/watch?v=Q-V5EQ-FBMc)
	- [Openfold github](https://github.com/aqlaboratory/openfold)
	- [Pymol2 open source visualisation](https://github.com/schrodinger/pymol-open-source)
	- Alphafold OpenAI
	- [Biological structure diffusion](https://github.com/RosettaCommons/RFdiffusion)
- The RFdiffusion code allows for the running of RFdiffusion simulations. The code is written in Python and is available on GitHub. The code includes a number of features, such as the ability to run simulations on multiple processors and the ability to output results in a variety of formats.
- [Diagnostics](https://www.amazon.co.uk/AI-Revolution-Medicine-GPT-4-Beyond/dp/0138200130)
- Market research
- [Addidas](https://www.adidas.com/into_the_metaverse/mint)
- [Bubblepunk interiors ML art](https://www.bubblepunk.io/bubblepunk-interiors)
- [What is a chief metaverse officer (bloomberg)](https://www.bloomberg.com/news/articles/2022-09-22/what-is-a-chief-metaverse-officer-and-do-you-need-one)
- [Userbase struggles (coindesk)](https://www.coindesk.com/markets/2022/04/06/metaverse-majors-struggle-as-user-base-falls-short-of-market-expectations/?outputType=amp)
- [Protecting Brands in the Metaverseâ€™s Uncertain Legal Landscape](https://wwd.com/business-news/technology/metaverse-lawsuit-nike-stockx-hermes-metabirken-fashion-1235247763/)
- [Market research global impact](https://www.analysisgroup.com/globalassets/insights/publishing/2022-the-potential-global-economic-impact-of-the-metaverse.pdf)
- [McDonalds in the metaverse](https://www.businessinsider.com/mcdonalds-metaverse-virtual-online-restaurant-trademark-delivers-food-web3-nft-2022-2)
- [Universal music metaverse / web3 team](https://www.musicweek.com/labels/read/universal-music-group-s-digital-strategy-team-creates-key-roles-for-web3-and-the-metaverse/087103)
- Narratives and convergence
- [With the help of generative AI it may be possible to democratise the externalisation of complex narratives, with these new narratives shaping the outcomes of society through the medium of the metaverse](https://www.epsilontheory.com/narrative-and-metaverse-pt-3-the-luther-protocol/#.YjyHbnLIE5k.twitter)
- [A lot of metaverse recently has just been convergence as companies take their existing simulation and repackage it for the moment.](https://thedriven.io/2022/05/23/nissan-and-mitsubishi-unveil-electric-mini-vehicles-and-test-drives-in-metaverse/)
- Games is the main convergence: from globalblock ""More companies are entering the metaverse as global electronics giant Sony has announced their own metaverse push in the latest annual corporate strategy meeting. Sony said this will be a more focused approach, as they aim to use metaverse-inspired experiences to engage users. As Sony owns PlayStation Brands, one of the biggest install userbases in the world, they are in an amazing position to make an impact. They have also revealed that prior investments in Epic, makers of the Fortnite game, and Bungie, another gaming studio, are part of this push."
- -
- [Epsilomn theory thesis on metaverse](https://www.epsilontheory.com/narrative-and-metaverse-pt-3-the-luther-protocol/#.YjyHbnLIE5k.twitter)
- [Epic games programming language for the metaverse](https://www.geekmetaverse.com/epic-games-launches-verse-the-metaverse-programming-language/)
- [Fortnite is the metaverse](https://www.ign.com/articles/how-fortnite-is-the-antidote-to-metaverse-skepticism)
- [epic unreal for fortnite](https://store.epicgames.com/en-US/p/fortnite--uefn?)
- Why you should use now generative AI in your metaverse company. Or maybe not
	- The Ghost Howls https://skarredghost.com/2023/02/11/generative-ai-metaverse-company/
- BlackRock digs further into crypto with metaverse ETF https://financefeeds.com/blackrock-digs-further-into-crypto-with-metaverse-etf/
- Chinaâ€™s iPhone production hub of Henan bets its future on the metaverse | South China Morning Post https://www.scmp.com/tech/policy/article/3194092/chinas-iphone-production-hub-henan-bets-its-future-metaverse
- Cutting Through the Hotel Hype of the Blockchain, Web3 and the Metaverse | https://hoteltechnologynews.com/2022/08/cutting-through-the-hotel-hype-of-the-blockchain-web3-and-the-metaverse/
- Experts highlight trust and safety practices for the metaverse https://www.techtarget.com/searchcio/news/252525336/Experts-highlight-trust-and-safety-practices-for-the-metaverse
- Global Metaverse Market Analysis Report 2022: Blockchain https://www.globenewswire.com/news-release/2022/08/30/2506629/0/en/Global-Metaverse-Market-Analysis-Report-2022-Blockchain-Solutions-in-Support-of-the-Metaverse-Market-will-Reach-148-6-Billion-by-2027.html
- Identity Management Institute Launches the Metaverse Security Center and Certified Metaverse Security Consultant (CMSC)â„¢ Certification https://www.prnewswire.com/news-releases/identity-management-institute-launches-the-metaverse-security-center-and-certified-metaverse-security-consultant-cmsc-certification-301689276.html
- Is the metaverse good for business? Why blending the virtual and the real remains a matter of much debate | Fortune https://fortune.com/2022/07/13/business-metaverse-dropbox-brainstorm-tech/
- Laws and Issues in the Metaverse (2)
	- Lexology https://www.lexology.com/library/detail.aspx?g=5a0cc4c0-a876-474e-a719-f528b71b68ee
- Laying the Foundation of the Metaverse, Streaming Video, Social, Gaming, and Broader Digital Advertising Markets to Collectively Clear US$2 Trillion by 2030 https://www.abiresearch.com/press/laying-the-foundation-of-the-metaverse-streaming-video-social-gaming-and-broader-digital-advertising-markets-to-collectively-clear-us2-trillion-by-2030/
- Major crypto exchange announces its arrival in the metaverse https://cointelegraph.com/news/major-crypto-exchange-announces-its-arrival-in-the-metaverse
- Metaverse exploitation and abuse to rise in 2023: Kaspersky https://cointelegraph.com/news/metaverse-exploitation-and-abuse-to-rise-in-2023-kaspersky
- Metaverse Market Size, Share & Industry Report 2020-2030 https://www.strategicmarketresearch.com/market-report/metaverse-market
- Metaverse Real Estate Gets Reality Check https://therealdeal.com/2022/08/04/metaverse-real-estate-gets-reality-check/
- Nissan and Mitsubishi unveil electric mini vehicles, and test drives in metaverse https://thedriven.io/2022/05/23/nissan-and-mitsubishi-unveil-electric-mini-vehicles-and-test-drives-in-metaverse/
- Nvidia Sees a Metaverse Populated With Lifelike Chatbot Avatars
	- CNET https://www.cnet.com/tech/computing/nvidia-sees-a-metaverse-populated-with-lifelike-chatbot-avatars/
- Room' Offers a Non-Facebook Way to Connect Coworkers in the Metaverse https://uk.pcmag.com/vr-1/143198/room-offers-a-non-facebook-way-to-connect-coworkers-in-the-metaverse
- The Architecture of the Metaverse (So Far) | ArchDaily https://www.archdaily.com/988639/the-architecture-of-the-metaverse-so-far
- The battle to build a child-friendly metaverse | Tech News https://tech.hindustantimes.com/tech/news/the-battle-to-build-a-child-friendly-metaverse-71655616713236.html
- The Metaverse Casino That Wasnâ€™t https://www.coindesk.com/layer2/sinweek/2022/08/29/the-metaverse-casino-that-wasnt/
- The World's First Virtual Reality Avatar Fashion Week Is On The Metaverse This Week https://womenlovetech.com/the-worlds-first-virtual-reality-avatar-fashion-week-is-on-the-metaverse-this-week/
- Top 10 Metaverse Platforms that will Replace Social Media in Future https://www.analyticsinsight.net/top-10-metaverse-platforms-that-will-replace-social-media-in-future/
- Top 15 Metaverse Companies To Watch Out For !! https://www.cryptotimes.io/top-15-metaverse-companies-to-watch-out-for/
- We're Ready for the Metaverse but the Technology Is Not. Here's Why.
	- Decrypt https://decrypt.co/100781/were-ready-for-the-metaverse-but-the-technology-is-not-heres-why
- [The Photorealistic Metaverse | Welcome to Cornerstone, Cornerstone is a gamified metaverse experience distributed to you via the web browser. Create, co-develop, and monetize your creative idea in the new digital realm.](https://cornerstone.land/)
- [Mega Yacht Sold for $650,000 in Metaverse, Becomes Most-Expensive NFT in Sandbox Virtual World | Technology News , A mega yacht has been sold for a whopping $650,000 (roughly Rs. 4.8 crore) in the Sandbox virtual gaming world. The pricey digital asset was released by metaverse developer Republic Realm as part of a luxury NFT series.](https://gadgets.ndtv.com/cryptocurrency/news/mega-yacht-sold-usd-650000-metaverse-most-expensive-nft-sandbox-virtual-world-2630187)
- [Facebook whistleblower warns Metaverse will repeat â€˜all the harmsâ€™,Frances Haugen says she is worried about privacy and safety inside Metaâ€™s impending Metaverse. Her accusations of profit before safety are still red hot.](https://cointelegraph.com/news/facebook-whistleblower-warns-metaverse-will-repeat-the-harms)
- [Virtual production and the future of generative art](https://virtualproducer.io/generative-ai-and-the-future-of-filmmaking/)
- [Zuckerburg disengages from metaverse](https://www.thestreet.com/technology/mark-zuckerberg-quietly-buries-the-metaverse)
- [Metahouse Could be First of Many
	- Los Angeles Business Journal digital twin of a mansion](https://labusinessjournal.com/featured/metahouse-could-be-first-of-many/)
- [The Future is a Dead Mall
	- Decentraland and the Metaverse: Clickbait Title: I spent three months living in the metaverse and now I'm starvingThe metaverse salespeople have a weird fixation with Animal Crossing, in sp...](https://www.youtube.com/watch?v=EiZhdpLXZ8Q)
- https://www.infosys.com/iki/perspectives/metaverse-insider-guide.html
- [Everyone abandoning metaverse](https://www.reddit.com/r/CryptoCurrency/comments/128hqkw/meta_microsoft_and_disney_are_reversing_their/)
- [The Internet Is Ruined. The Metaverse Can Still Be Saved: In this nascent stage, there are opportunities for virtual worlds to avoid the mistakes of the past.](https://www.wired.com/story/metaverse-ethics/)
- The article discusses how the internet has ruined the Metaverse, and how it can still be saved. It argues that the internet has made the Metaverse less accessible and has made it more difficult to find information. However, it also states that the Metaverse can still be saved if people are willing to work together to make it more accessible and user-friendly.
- Omniverse
- [Free to individuals](https://blogs.nvidia.com/blog/2022/01/04/omniverse-available-free-to-creators/)
- [Full RTX rendering](https://www.youtube.com/watch?v=Jm155QkRjl0&feature=youtu.be)
- [AI assisted blended character plugin](https://blogs.nvidia.com/blog/2023/02/15/blender-alpha-release-omniverse/?ncid=so-link-466434#cid=ov01_so-link_en-us)
- [NVIDIA Unveils Powerful AI, Simulation and Creative Tools for Creators and Developers of Virtual Worlds | NVIDIA Blog](https://blogs.nvidia.com/blog/2022/08/09/omniverse-siggraph/)
- Open metaverse
- [Open metaverse discord from linux foundation](https://discord.gg/openmetaverse)
- [Free 1 Million objects](https://huggingface.co/datasets/allenai/objaverse)
- Usability
- [bridging the real and the virtual like mcdonalds home delivery](https://www.businessinsider.com/mcdonalds-metaverse-virtual-online-restaurant-trademark-delivers-food-web3-nft-2022-2)
- Virtual land
- virtual
- [hybrid land linking real and virtual (including digital twin)](https://labusinessjournal.com/featured/metahouse-could-be-first-of-many/)
- Simple geo-referencing of physical place in mixed reality
- Digital assistants
- [MultiOn digital assistant](https://multion.ai/)
- [LEON open source assistant](https://docs.getleon.ai/)
- [Open source assistant github of issues](https://github.com/LAION-AI/Open-Assistant/issues)
- Avatars
- [Free VRChat Models & Avatars | VRCMods (other)](https://vrcmods.com/)
- [CLIP-Actor Text-Driven Recommendation and Stylization for Animating Human Meshes](https://clip-actor.github.io/)
- Displaytech
- [CREAL lens display](https://creal.com/2023/05/19/creals-breakthrough-ar-display-real-depth-with-a-classic-lens/)
- Autostereoscopic
- [nanomaterials paper](https://www.mdpi.com/2079-4991/12/3/429#)
- [Bubbles and speakers](https://www.youtube.com/watch?v=7VLdMXnM0lU)
- [lightfield labs](https://www.lightfieldlab.com/)
- The Wall Street Journal has reported that Meta Quest, formerly known as Oculus, had over 6 million monthly active users as of October 2022. This news has generated discussion on the r/virtualreality subreddit, with users speculating about the future of VR and Meta Quest's upcoming products. Some users believe that VR needs more content to attract more users, while others express excitement over the release of Undead Citadel and the possibility of a Starfield VR game. Users also discuss the potential of streaming headsets like the Quest working on the PlayStation 5 and the importance of storytelling in VR games. Some users express disappointment in the launch of the remastered edition of San Andreas and its impact on Quest sales, while others joke about the perceived death of VR. https://www.reddit.com/r/virtualreality/comments/12lpsvf/rumor_meta_quest_had_more_than_6_million_monthly/ Meta urged to prevent minors entering the metaverse https://www.bloomberg.com/news/articles/2023-04-14/meta-urged-to-halt-plans-allowing-minors-into-the-metaverse?leadSource=uverify%20wa
- It's a valuable tool for video gaming, film industry, and metaverse applications that require 3D scenes. The article discusses how the internet has ruined the metaverse, and how it can still be saved. It argues that the internet has made the metaverse less accessible and more difficult to navigate, and that this has had a negative impact on its potential. The article suggests that the metaverse can still be saved if we take steps to improve its accessibility and make it easier to use.
   <https://www.wired.com/story/metaverse-ethics/> Exploring Why the Metaverse Hasn't Taken Off as Expected: The metaverse has quickly turned from a profitable utopia into a cash-guzzling dystopia.The text provides an overview of the Metaverse, a virtual world that has not yet taken off as expected. The text describes the potential reasons for this, including the lack of a clear business model and the difficulty of creating an immersive experience.
   <https://www.bbntimes.com/technology/exploring-why-the-metaverse-hasn-t-taken-off-as-expected> Unfortunately, the requested website (mirror-next-hop.forbes.com) is not accessible and the current session has been terminated with an access denied error (403). Further information can be obtained by contacting the website administrators using the provided reference code (217.138.196.24 2023-06-26T16:51:46.121Z). https://www.forbes.com/sites/charliefink/2023/05/07/this-week-in-xr-after-ai-sucks-the-air-out-of-the-metaverse-it-will-remake-xr/ The Building the Metaverse YouTube channel features a range of videos discussing the use of generative AI and large language models (LLMs) in game development and world-building. One video features a discussion with Kayla Comalli, co-founder and CEO of Lovelace Studios, about their platform Nyric, which generates entire worlds from a text prompt using generative AI technologies like ChatGPT. Other videos on the channel cover topics such as generative art assets for games, AI storytelling and narrative, generative graphics workflow for games, and the use of generative AI in game production. The channel also features discussions with individuals in related fields, such as Edward Saatchi of Fable Studio, who talks about virtual beings and simulated worlds, and Adam B. Levine of Blockade Labs, who discusses Skybox AI and game development. Additionally, there are videos discussing the potential applications of AI in other areas, such as defense and longevity. The channel provides a valuable resource for those interested in learning about the intersection of generative AI and gaming, as well as related topics. https://www.youtube.com/watch?v=fsg83BvsXww The article discusses the potential risks and opportunities of motion tracking data in extended reality (XR) and the metaverse. While this data is often presumed to be innocuous, recent studies have shown that it has the potential to profile and deanonymize XR users, posing a significant threat to security and privacy in the metaverse. The article highlights the need for increased awareness and caution regarding the collection and use of motion data in XR and metaverse experiences. https://arxiv.org/abs/2306.06459 Beauty company Coty has created a metaverse for its 11,000 global employees, using technology from Spatial. The virtual campus is based on 3D tech and tools from Spatial, and aims to develop upskilling and future innovation for Coty brands. The campus includes features such as text and vocal chat forums, screen and filesharing, customisable avatars and location exploration and quest fulfilment, as well as a "phygital" rewards system. Coty and Spatial said the campus was a significant milestone in crafting "new models for scalable gamified experiences". https://drugstorenews.com/coty-enters-metaverse-campus-global-workforce Tencent Cloud has announced its commitment to support the development of the Web3 ecosystem at the first global Web3 summit. The cloud business of Tencent has unveiled a development roadmap for a full suite of blockchain API services and its Tencent Cloud Metaverse-in-a-Box offering. The platform will provide technical support for Web3 and work with industry partners to nurture the Web3 ecosystem. Tencent Cloud plans to promote sustainable growth by offering Web3 builders cloud solutions credits, marketing workshops and publicity opportunities. It has also signed a Memorandum of Understanding with Ankr, a Web3 infrastructure provider, to jointly develop a full suite of blockchain API services. The API services will cover security, storage, identity management, middleware, development tools, and data analytics, among other areas. https://www.tencentcloud.com/dynamic/news-details/100437?lang=en&pg=
   <https://www.tencentcloud.com/dynamic/news-details/100437?lang=en&amp;pg=>
- The concept of a "predictive metaverse", an artificial intelligence (AI)-powered advanced form of a virtual world that could predict and anticipate its users' intentions and behaviors, is offering new ways to improve engagement, creativity and personalised learning in [[Education and AI]], according to an article in eSchool News by Roger James Hamilton, Founder and CEO of Genius Group. As virtual reality worlds become increasingly sophisticated and realistic, they are also becoming more intelligent and able to analyse data in real-time to deliver tailored recommendations and feedback to individual users, for example to optimise the virtual marketplace to improve user experience and increase sales. https://www.eschoolnews.com/educational-leadership/2023/04/21/predictive-metaverse-the-future-of-guided-learning/
- Reddit and its partners use cookies and similar technologies to improve the quality of its website, personalize content and advertising, measure advertising effectiveness, and ensure the proper functionality of its platform. By accepting all cookies, users agree to the use of cookies. However, by rejecting non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of its platform. Reddit's Cookie Notice and Privacy Policy provide more information about its use of cookies. Recently, Meta, Microsoft, and Disney have reversed their bets on the metaverse. Posts related to this topic on r/CryptoCurrency have been removed by moderators. https://www.reddit.com/r/CryptoCurrency/comments/128hqkw/meta_microsoft_and_disney_are_reversing_their/ Exokit, Adrian's opensource mixed reality toolkit for web <https://github.com/exokitxr/exokit>
- [[Apple]] has acquired Mira, a Los Angeles-based startup that creates AR headsets for various companies and the US military. The acquisition, which was confirmed by [[Apple]], follows the company's launch of its $3,499 mixed reality headset, the Vision Pro. Two former Mira employees said that Jony Ive, [[Apple]]'s former design chief, was an advisor to the startup at one point. Mira's military contracts include a small agreement with the US Air Force and a $702,351 agreement with the Navy while its contract with Nintendo World provides headsets for the Mario Kart ride at its theme parks in Japan and LA's Universal Studios. It is currently unknown if [[Apple]] will continue Mira's military contracts. https://www.theverge.com/2023/6/6/23751350/[[apple]]-mira-ar-headset-startup
- These posts include insights on the development of XR technology, possible applications, and innovations in the field. https://www.linkedin.com/posts/reneschulte_visionpro-ar-vr-activity-7072953336156602369-_2pL?utm_source=share&utm_medium=member_android
- [This text is a brief description of a position paper published by the OMA3 Portaling and Mapping Working Group (PMWG) on the transformative technology of portals in the metaverse. The position paper provides insights into the potential of a universal portal system to reshape digital interaction for consumers, businesses, and platforms. It highlights the development process, strategic approach, and vision of OMA3 in setting new standards for the Web3 universe. The paper invites readers to comment on it by creating an issue or commenting in the Google Doc. It also encourages individuals to join OMA3 and participate in the Portaling and Mapping Working Group if they would like to contribute to the project. The paper is licensed under a Creative Commons Attribution 4.0 International License.](https://github.com/oma3dao/portal-position-paper)
- [The Open Metaverse Alliance for Web3 (OMA3) has announced the Inter-World Portaling System (IWPS) project, aimed at creating standards for seamless travel between metaverse platforms. OMA3, based in Zug, Switzerland, is a consortium of top metaverse companies in Web3. The IWPS project will allow users to walk through inter-world portals and travel between metaverse platforms such as Alien Worlds, My Neighbor Alice, and Sandbox. OMA3 believes that IWPS has the potential to enhance accessibility and engagement within the digital realm by bridging disparate metaverse environments. They compare IWPS to the development of transportation technology like railroads and highways in the industrial revolution and the introduction of the HTTP standard in the digital realm, both of which facilitated the free flow of goods, services, and information. OMA3 has released a position paper outlining the importance of IWPS and inviting participation and comments from the Web3 metaverse community. They see the development and standardization of IWPS as the next frontier in the evolution of the metaverse, enabling new levels of connectivity, commerce, and shared experiences.](https://venturebeat.com/games/oma3-offers-way-for-users-to-travel-between-blockchain-gaming-worlds-in-the-metaverse/)
- [Beatoven.ai is a website that uses advanced AI music generation techniques to create unique, mood-based music for videos and podcasts. Users can start by choosing a genre or style that suits their theme and then make cuts to reflect different moods throughout their content. With a rich selection of 16 moods to choose from, users can easily find the right mood for each cut. Once the desired moods have been selected, users can hit compose and let the AI algorithm generate a unique track for them.   The website is useful for various types of content creators, including agency/production houses, YouTube creators, podcast creators, indie game developers, audiobook producers, and web3 and metaverse companies. It offers a range of benefits, such as packing a punch in videos, creating a signature sound for YouTube channels, making intro and outro sections special for podcasts, designing themes and background music for games, elevating audio books with atmospheric music, and providing background music for metaverse experiences.   Beatoven.ai also allows users to customize the length, genre, mood, and instruments of their tracks. The resulting music is production-ready with industry-standard mixing and mastering.   The licensing terms for the music on Beatoven.ai grant users a perpetual license for usage on their chosen platforms. All copyrights for the music created on the website belong to Beatoven Private Limited.   The website offers a free membership option, allowing users to create and download music for the first 15 minutes of their projects. There is also a premium pricing plan available for unlimited usage.   Overall, Beatoven.ai is a versatile and user-friendly platform that offers customized, royalty-free music for a wide range of content creators.](https://www.beatoven.ai/)
- [A predictive metaverse is an advanced virtual world powered by AI and machine learning algorithms. It can predict and anticipate the actions and behaviors of its users, allowing for personalized recommendations, predictions, and feedback. This concept is becoming increasingly appealing to content creators and educators in the field of education, as it can improve engagement and creativity and create personalized learning programs.  In a predictive metaverse, AI algorithms can analyze real-time data to understand the preferences, behaviors, and intentions of users. This information can then be used to optimize the virtual world and improve the user experience. For example, an AI algorithm could predict user behavior in a virtual marketplace, such as what they are likely to buy and when they are likely to buy it. This data can be used to optimize the marketplace and increase sales.  While the term metaverse is often associated with the gaming industry, its potential applications in education are significant. The predictive metaverse can enhance virtual learning by providing personalized guidance and support to students. It can help create immersive learning experiences and improve student engagement and motivation.  As virtual worlds become more sophisticated and realistic, the predictive metaverse holds great promise for the future of guided learning. By harnessing the power of AI and machine learning, educators can create personalized learning experiences that cater to the unique needs and preferences of each student. This technology has the potential to revolutionize education by providing tailored instruction, real-time feedback, and personalized recommendations, ultimately improving student outcomes and overall learning experiences.](https://www.eschoolnews.com/educational-leadership/2023/04/21/predictive-metaverse-the-future-of-guided-learning/)
- [This text is a Reddit post from the r/CryptoCurrency subreddit. The post mentions that Meta (formerly known as Facebook) as well as Microsoft and Disney are reversing their bets on the metaverse. However, the post has been removed by the subreddit moderators. The comments in the post discuss the current hype around artificial intelligence (AI) and the need for companies to hop on that trend. Some users express their opinions that these companies went about their approach to the metaverse in the wrong way. The post also includes comments about the ownership of a bot that has received a high number of moons (a cryptocurrency earned on the Reddit platform) and speculation on the future of meta platforms like Meta. The post is followed by a list of related crypto news articles from various sources, covering topics such as refunds in crypto scams, acquisitions of Bitcoin, changes in cryptocurrency taxes, and the launch of web3 games by Ubisoft.](https://www.reddit.com/r/CryptoCurrency/comments/128hqkw/meta_microsoft_and_disney_are_reversing_their/)
- [Tencent Cloud, the cloud business of global tech company Tencent, has announced its commitment to support the development of the Web3 ecosystem. The company unveiled its development roadmap for a full suite of blockchain API services and its Tencent Cloud Metaverse-in-a-Box offerings. It aims to provide a strong technological foundation for Web3 builders and be the digital enabler for the Web3 industry. Tencent Cloud will collaborate with Web3 partners to accelerate the adoption of Web3. The company also signed a Memorandum of Understanding (MoU) with Web3 infrastructure provider Ankr to jointly develop a full suite of blockchain API services. Additionally, Tencent Cloud announced strategic collaborations with Avalanche, Scroll, and Sui, three other Web3 blockchain partners, to build a stronger foundational infrastructure for global builders. The collaboration with Avalanche will explore blockchain solutions for enterprise customers, while the partnership with Scroll aims to scale Ethereum through an open-sourced zk-Rollup. The collaboration with Sui will optimize the on-chain gaming experience. Furthermore, Tencent Cloud introduced Tencent Cloud Metaverse-in-a-Box, a comprehensive solution that integrates infrastructure, products, SDKs, and low-code solutions. The Metaverse-in-a-Box allows businesses to develop metaverse applications rapidly. Tencent Cloud hosted its first global Web3 summit, Tencent Cloud Web3 Build Day, to discuss the latest blockchain landscape and development trends in Web3 games and social networks.](https://www.tencentcloud.com/dynamic/news-details/100437?lang=enandpg=)
-
- BrahmGAN is a cutting-edge 3D content creation tool that uses advanced technology such as NeRF, GAN, and Blockchain to create 3D content effortlessly, without requiring technical expertise or complex software. The tool is designed for industries such as eCommerce, XR, and Gaming. BrahmGAN's NeRF solutions for VR Services are capable of creating stunning worlds within days. The company is based in Bengaluru and Singapore. Interested parties can contact them via email at info@brahmgan.com or join their mailing list to stay updated on their latest developments. http://www.brahmGAN.ai
- Hallway is an app that empowers creators to tell their stories using avatars instead of their real faces, making it easy for anyone to express their creativity without the limitations of video as a medium. It is a single-camera app that supports a variety of avatars and requires no custom hardware or clunky setups. The app is currently taking VTuber and 2D/3D artist intakes to join the waitlist for early access. Hallway provides a new avenue for creators to express themselves and join the next generation of creators. http://joinhallway.com/
- BrahmGAN is an AI-based 3D content creation platform that uses advanced technologies like NeRF, GAN, and Blockchain for quick and decentralized content creation. BrahmGAN's text-to-3D tools enable users to create 3D content without technical expertise or complex software. BrahmGAN caters to industries like XR, gaming, and eCommerce and uses NeRF for video to 3D, and Blockchain for democratizing 3D content creation globally. BrahmGAN has offices in Bangalore and Singapore and has clients like Taanga Studios. http://www.brahmGAN.ai
- [In this post on Reddit, a user shares their experience developing a C++ library for running Stable Diffusion, an AI image generation model. They explain that the library does not rely on Python and can use the GPU for executing the AI models involved. The user's main motivation for developing this library was to use its image synthesis capabilities in real-time 3D software written in C++.   The user shares their first results, which include a simple library available as an integration-ready MIT licensed Nuget package, capable of running Stable Diffusion models in ONNX format. They note that the code is currently targeting Windows, but only a small portion related to image editing tasks relies on the WinAPI, which can easily be replaced for other platforms.  Several redditors comment on the post, expressing interest in the library and discussing their own experiences with Stable Diffusion and C++ implementations in machine learning. Some users appreciate the user interface design of the library, while others discuss the advantages and disadvantages of using Python for machine learning tasks.  The user also mentions that they are working on an Xbox release and have already generated Stable Diffusion images on the Xbox. There are further discussions on related topics, such as RAID arrays, graphics libraries for Rust, and C++ language support in Xcode 15.  Overall, the post provides an overview of a C++ library for running Stable Diffusion and highlights the user's experiences and progress in developing it.](https://www.reddit.com/r/cpp/comments/143olej/an_open_source_library_for_running_stable/)
- [The text is a Reddit post discussing the process of generating and applying AI-generated images to a 3D model. The poster shares the results of their image generation using AI and provides details about the rendering process. They mention using Blender and a custom UV map to project the generated image onto the face of the 3D model.  The poster explains that they manually removed diffuse and specular reflections during the AI image generation stage and conducted additional tests to bring out more details. They aim to create a color map that can be used without adjustments.  Other users in the comments ask about the training process and suggest alternative methods for projection mapping. The poster responds, stating that they did additional training with images that suppress shadows, light, and AO. They also mention that the AI-generated image was baked into a custom UV map before rendering.  Some users express interest in learning the process and suggest creating a tutorial. The original poster mentions that the process takes a lot of time and effort and that the results may vary. They recommend referring to their previous test articles for more information on AI image generation.  The post concludes with a list of related posts from other subreddits discussing topics such as computer vision, AI-generated avatars, and visualizations in Stable Diffusion.](https://www.reddit.com/r/StableDiffusion/comments/11ol47u/3d_model_face_color_map_generation_test3/)
- The article discusses Reddit's use of cookies and similar technologies to provide a better experience to users. By accepting all cookies, users agree to Reddit's use of cookies to provide and maintain their services, personalize content and advertising, and measure advertising effectiveness. If users reject non-essential cookies, Reddit may still use certain cookies to ensure the proper functionality of the platform. Additionally, the article includes a user-generated post on using Mixamo, a free website with a large library of 3D animations, for character posing. Several commenters offer their own suggestions and recommendations for related tools and workflows on different subreddits. https://www.reddit.com/r/StableDiffusion/comments/11owo31/something_that_might_help_ppl_with_posing/
- Mixed reality, spatial, metaverse and telecollaboration
- [The text discusses the use of cookies and similar technologies on Reddit's platform. By accepting all cookies, users agree to the use of cookies to improve their experience, deliver and maintain services, personalize content and advertising, and measure the effectiveness of advertising. Rejecting non-essential cookies still allows Reddit to use certain cookies for proper platform functionality. The text also mentions that more information can be found in the Cookie Notice and Privacy Policy.  The rest of the text is a Reddit post from the r/StableDiffusion subreddit. The post discusses a generalist model that the OP (original poster) will be releasing soon. The model is capable of creating images with resolutions ranging from 1024 to 1080p, and it is fine-tuned on SD 2.1 768X. The model can generate a variety of images, including photorealism, paintings, and anime. The OP shares some example images generated during the training process and invites others to test prompts for image creation. The post receives several comments and discussions about the model's capabilities, potential uses, and suggestions for improvement.  Additionally, the text includes a list of other posts from various subreddits such as r/3dsmax, r/colorists, and r/StableDiffusion. These posts cover topics related to rendering, color grading, using AI-generated visuals, and showcasing artistic works.](https://www.reddit.com/r/StableDiffusion/comments/13j78fo/some_examples_of_the_generalist_model_i_will_be/)
- Scan the World is a museum of sculptures, artifacts, and statues made possible through 3D scanning and printing technology. The scans of these historical pieces are free to download for accessibility, educational and cultural heritage purposes. Scan the World uses photogrammetry to capture these high-resolution scans, from digital archaeology to downloadable monuments and buildings, 3D printing enthusiasts will be able to find it at the museum. The museum offers more than 20 categories ranging from Africa to South America and everything in between, and it ranks objects based on popularity, date published, and views. Visitors have instant access to the MyMiniFactory library, which is community-powered with users able to upload their 3D printable designs as well. The ultimate goal of Scan the World is to make historical and artistic objects more widely available to people around the world. Itâ€™s the perfect solution for those who want to own an authentic, accurate replica of an important sculpture, artifact, or more. https://www.myminifactory.com/category/scan-the-world
- LinkedIn uses both essential and non-essential cookies to provide, secure, analyze and improve their services, as well as show users relevant ads on and off the platform, according to their Cookie Policy. Users can accept or reject non-essential cookies for this use and can update their choices at any time in their settings. In a post by Rene Schulte, the Head of 3D & Quantum CoPs at Microsoft, he shared resources for Unity developers to prepare for developing for Apple's VisionPro and visionOS. The post sparked conversations and comments from other professionals in the AR/VR industry, including discussions about AR's utility, developments in volumetric video and tracking, and the use of digital twins in building. https://www.linkedin.com/posts/reneschulte_visionpro-ar-vr-activity-7072953336156602369-_2pL?utm_source=share&utm_medium=member_android
- [The text discusses a research paper on a diffusion model called 3DiM for 3D novel view synthesis. The model takes a single input view and generates consistent and sharp completions across many views. It uses a pose-conditional image-to-image diffusion model that takes a source view and pose as inputs and generates a novel view for a target pose as output. The model employs stochastic conditioning, where a random conditioning view is selected from previously generated views at each denoising step, to improve 3D consistency. The paper introduces a new evaluation methodology called 3D consistency scoring to assess the 3D consistency of the generated objects. The model is geometry-free, does not rely on hyper-networks or test-time optimization, and can easily scale to a large number of scenes.  The paper presents samples generated by 3DiM trained on the ShapeNet dataset. The model achieves high fidelity and approximate 3D consistency in generating completions from a single view. It also demonstrates the model's effectiveness by generating 3D objects from in-the-wild images downloaded from the internet. The paper compares 3DiM to prior work on the SRN ShapeNet benchmark and shows that 3DiM outperforms other methods in terms of generating sharp samples. The paper also discusses the technical details of 3DiM, including its generation process using stochastic conditioning and the modifications made to the image-to-image UNet model to achieve high-quality results.  Overall, the paper highlights the effectiveness of diffusion models for 3D novel view synthesis and introduces novel techniques, such as stochastic conditioning and 3D consistency scoring, to improve the quality and consistency of generated views. The proposed model, 3DiM, shows promising results in generating realistic and consistent 3D objects from a single input view.](https://3d-diffusion.github.io/)
- [The paper presents a monocular depth estimation method using denoising diffusion models. The goal is to generate accurate depth maps from single RGB images. The authors address the problem of noisy and incomplete depth maps in the training data by using step-unrolled denoising diffusion, an L1 loss, and depth infilling during training.  To overcome the limited availability of supervised training data, the authors leverage pre-training on self-supervised image-to-image translation tasks. Despite the simplicity of the approach, their model achieves state-of-the-art (SOTA) performance on the indoor NYU dataset and near SOTA results on the outdoor KITTI dataset.  The approach involves infilling missing depth in ground truth depth maps using nearest neighbor interpolation. Then, noise is added to the depth map and a neural network is trained to predict the noise given the RGB image and noisy depth map. During fine-tuning, one step of the forward pass is unrolled and the ground truth depth map is replaced with the model's prediction.  The DepthGen model achieves an absolute relative error of 0.074 on the indoor NYU dataset and a competitive relative error of 0.064 on the outdoor KITTI dataset, demonstrating its accuracy in depth estimation.  The paper also introduces a text-to-3D pipeline that combines DepthGen with off-the-shelf text-to-image and text-conditioned image completion models. This pipeline allows for generating 3D point clouds from text prompts.  In conclusion, the proposed method of monocular depth estimation using diffusion models achieves state-of-the-art performance, even with limited supervised training data. The approach is simple yet effective and can be integrated into a text-to-3D pipeline for generating 3D scenes from text prompts.](https://depth-gen.github.io/)
- [The text provided is a collection of video titles and descriptions related to Blender, AI, and 3D design. The videos cover topics such as creating isometric rooms, using AI in 3D design, Unreal Engine, toon shading in Blender, QR code art, GPT (Generative Pre-trained Transformer) engineering, creating Ghibli-style characters, new features in Blender 3.6, animation in Blender, and adding vegetation in Twinmotion. The videos are created by various individuals and brands, including vertex vendor, Unreal Sensei, Quick QR Art, ENFANT TERRIBLE, Matt Wolfe, Ian Wootten, Brandon's Drawings, Polyfjord, Charlie Barber, and vishal panjeta. The text also mentions a Google company and provides information about cookie usage and privacy settings when using Google services.](https://www.youtube.com/watch?v=GZO7TAlVE_8)
- [WebXR is a device API that allows for VR/AR experiences through web browsers. However, monetization has been a major issue for the platform, with indie creators struggling to capture value. Most WebXR apps appear as prototypes because developers find it difficult to justify investing more resources into the ecosystem. The current ways people pay for WebXR content include purchasing tickets, using cryptocurrency for virtual land, and accessing certain features by login or ownership of bot handles. The process of paying for WebXR content can be made easier and more frictionless by integrating payment methods like Apple Pay or Google Pay while in VR. Artists can get paid through various means such as commissions, Patreon, grants, VC investment, and event tickets. Non-payment based monetization strategies like advertising are also being explored. A list of 101 ideas for WebXR monetization includes platforms like Patreon and Github Sponsors, virtual market stalls, virtual land parcels, and in-world advertising. Other strategies include payment processing integration with platforms like PayPal or Discord, creating virtual actors and performers, storytelling, and podcast sponsorships. Advertisements targeted at 18-44 year old males interested in software, gaming, and VR have shown promising results. A Github repository for WebXR monetization examples is in progress. Despite these efforts, monetization in the WebXR ecosystem is still a work in progress, and more exploration and innovation is needed.](https://hackmd.io/@xr/monetization)
- The paper proposes a system called CLIP-Actor, which animates a 3D human mesh to conform to a text prompt by recommending a motion sequence and optimizing mesh style attributes. The system's novelty lies in its ability to recommend motion that conforms to the prompt in a pose-agnostic and temporally-consistent manner while leveraging multi-frame human motion and rejecting poorly rendered views. The authors demonstrate that CLIP-Actor produces plausible and human-recognizable style 3D human mesh in motion with detailed geometry and texture solely from a natural language prompt. The paper's methodology shows that CLIP-Actor is an effective and efficient way to generate plausible results when the pose of an artist-designed mesh does not conform to the text prompt from the beginning. The research has been sponsored by the Korean government's grant funded by the Institute of Information & communications Technology Planning & Evaluation (IITP). https://clip-actor.github.io
- [The paper CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes proposes a system for animating human meshes based on text prompts. The system, called CLIP-Actor, generates motion sequences and optimizes mesh style attributes to conform to a given text prompt.  The authors highlight a limitation of previous work, which struggled to produce realistic results when the starting pose of a pre-designed mesh did not align with the text prompt. To address this issue, CLIP-Actor leverages a large-scale human motion dataset with language labels to build a text-driven human motion recommendation system. It suggests a motion sequence that aligns with the given prompt in a coarse-to-fine manner.  In addition, the authors introduce a novel neural style optimization technique that adds detail and texture to the recommended mesh sequence in a temporally-consistent and pose-agnostic manner. They also propose spatio-temporal view augmentation and mask-weighted embedding attention techniques to stabilize the optimization process by incorporating multi-frame human motion and rejecting poorly rendered views.  The results of CLIP-Actor demonstrate its ability to generate plausible and human-recognizable 3D human meshes in motion with detailed geometry and texture solely from natural language prompts.  The paper includes the BibTeX citation for academic referencing and acknowledges the support received from the Institute of Information and Communications Technology Planning and Evaluation (IITP) in Korea for funding the research.  The website containing the paper and code is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. The source code for the system is mainly borrowed from Keunhong Park's Nerfies website, and feedback and questions can be directed to Kim Ji-Yeon.](https://clip-actor.github.io)
- [Magic123 is a two-stage solution for generating high-quality, textured 3D meshes from a single unposed image. The first stage optimizes a neural radiance field to create a coarse geometry, while the second stage uses a memory-efficient differentiable mesh representation to produce a high-resolution mesh with realistic textures. The 3D content is learned through reference view supervision and guided by both 2D and 3D diffusion priors. The system includes a tradeoff parameter that controls the balance between exploring novel geometries and achieving precise results. Textual inversion and monocular depth regularization are employed to ensure consistent appearances across views and prevent degenerate solutions. Magic123 outperforms previous image-to-3D techniques, as demonstrated through experiments on synthetic benchmarks and real-world images.  The Magic123 pipeline consists of two stages: coarse and fine. In the coarse stage, an Instant-NGP neural radiance field is optimized to reconstruct a rough geometry. In the fine stage, a DMTet mesh is initialized from the coarse output and optimized to generate a high-resolution mesh with textures. Textural inversion is used in both stages to preserve object geometry and ensure consistent textures across views.  Example generated objects from Magic123 show photo-realistic 3D representations created from single images. The system offers a tradeoff between 2D and 3D priors, allowing for exploration and imagination in geometry generation or precise results with reduced details. By combining both priors and adjusting the tradeoff parameter, Magic123 consistently produces identity-preserving 3D content with fine-grained geometry and visually appealing textures.  Quantitative evaluations on the NeRF4 and RealFusion15 datasets demonstrate the effectiveness of Magic123 compared to previous state-of-the-art approaches. The system achieves top performance across various metrics, showcasing its ability to generate high-quality 3D representations.  The article credits DreamFusion authors for their website templates, and the text is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.](https://guochengqian.github.io/project/magic123/)
- [The paper Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors presents a two-stage approach for generating high-quality, textured 3D meshes from a single unposed image.   In the first stage, a neural radiance field is optimized to create a coarse geometry. In the second stage, a memory-efficient differentiable mesh representation is used to generate a high-resolution mesh with visually appealing texture.   To learn the 3D content, reference view supervision and novel views guided by a combination of 2D and 3D diffusion priors are employed in both stages. A trade-off parameter controls the balance between exploration and exploitation of the generated geometry.   Textual inversion and monocular depth regularization techniques are also used to ensure consistent appearances across views and prevent degenerate solutions.   The approach, called Magic123, outperforms previous image-to-3D techniques according to extensive experiments on synthetic benchmarks and real-world images.   The code, models, and generated 3D assets are available on GitHub.](https://huggingface.co/papers/2306.17843)
- This is a conversation and code change history on the GitHub platform for the "dream-textures" repository. It appears that the main topic of discussion is the addition of a new feature called "Project Dream Texture operator". This feature involves the use of depth-to-image projection to apply a texture to a mesh based on user input. The conversation includes comments and feedback from users who have tested the feature, as well as responses from the repository owner who is implementing the changes. There are also code commits and updates to various files related to the feature and its implementation. https://github.com/carson-katri/dream-textures/pull/409
- This text appears to be a GitHub pull request for the "dream-textures" repository, with the pull request titled "Add Project Dream Texture operator". The pull request adds functionality that allows users to project a texture onto a mesh using a text prompt and depth data. The pull request includes a log of commits and comments from users who have tested the functionality and provided feedback. It seems that the pull request has been approved by at least two reviewers and has been merged into the main branch of the repository. https://github.com/carson-katri/dream-textures/pull/409
         | 
       |
-
-
- [SDFStudio is a unified and modular framework for neural implicit surface reconstruction. It provides an implementation of three major implicit surface reconstruction methods: UniSurf, VolSDF, and NeuS. The framework also supports various scene representations, point sampling strategies, and incorporates advances in monocular cues, geometry regularization, and multi-view consistency. The modular implementation of SDFStudio makes it easy to transfer ideas from one method to another. The repository includes documentation, datasets, and examples for users to get started.  To use SDFStudio, users need to set up the environment by installing CUDA and creating a Conda environment. The framework requires Python 3.7 or higher. Users can install the necessary dependencies, including PyTorch and tiny-cuda-nn, using pip. After installing the dependencies, users can clone the SDFStudio repository and install it using pip. Tab completion can be enabled for better user experience.  To train a model, users can download test data and train a model on a specific dataset using the provided commands. SDFStudio supports different models and parameters can be modified to train different models. The training progress can be tracked using visualization tools such as the viewer, Tensorboard, or Weights and Biases.  Once a model is trained, users can export the mesh and render it. The repository provides commands for extracting the mesh and rendering it. Video rendering and customization of the camera path are also supported. Advanced options include training models other than NeuS-facto and modifying the configuration.  SDFStudio is built on top of the Nerfstudio project and incorporates contributions from various developers. If the library is used or the documentation is found useful, the authors request users to consider citation.  The repository includes a comprehensive README file that provides detailed instructions and explanations for using SDFStudio. It also provides information about the contributors, license, and other resources.](https://github.com/autonomousvision/sdfstudio)
- [The text summarizes a GitHub repository called CLIP-Actor, which is a pytorch implementation for the ECCV 2022 paper, CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. CLIP-Actor is a system for text-driven motion recommendation and neural mesh stylization for human mesh animation. The repository contains code developed on Ubuntu 18.04 with Python 3.7, CUDA 10.2, and PyTorch 1.9.0. The system requirements include Python 3.7, CUDA 10.2, and a single GPU with a minimum of 24 GB RAM. The repository provides instructions for setting up the environment and installing the required dependencies. It also provides instructions for downloading the necessary body models and datasets. The repository includes example commands to generate stylized 4D human avatars based on prompts, such as a scuba diver is scuba diving or Freddie Mercury is dancing. The outputs include final video files, stylized .obj files, colored render views, and screenshots. The repository includes a citation for the paper and acknowledges the prior work that inspired the CLIP-Actor implementation.](https://github.com/youwang-kim/clip-actor)
- [Mixed reality design guidelines from Meta](https://developer.oculus.com/resources/mr-design-guideline/)