public:: true

- #Public page automatically published
- ### Introduction and Problem Definition
	- This chapter identifies an intersectional space across the described technologies, and proposes a valuable and novel software stack, which can enable exploration and product development. It is useful to briefly look at the Venn disgram we began with, and recap the book and the conclusions we have drawn so far.
	- ![image](../assets/c5efbdb4f93ab63a4fbbf12aba053d2194959850.png){:height 776, :width 1024}
- #### Overview of the Metaverse and Digital Society
	- The concept of the Metaverse has gained significant attention, with various stakeholders positioning themselves to capitalize on its potential. While it remains unclear exactly what form the Metaverse will take or whether people truly desire it, it is evident that digital society holds considerable promise. We see advantage less in social metaverse, and more in solving business to business technical use cases where professionals with visual technical problems, or training requirements, gather in collaborative spaces.
- #### Trust, Accessibility, Governance, and Safeguarding
	- The Metaverse faces numerous challenges, including poor adoption rates, overstated market need, and a lack of genuine digital society use cases. Meanwhile trust abuses by incumbent providers have led to potential inflection points in the organization of the wider internet. Moreover, emerging markets and less developed nations face barriers to entry due to inadequate identification, banking infrastructure, and computing power. There is an opportunity to build pervasive digital spaces with a different and more open foundation, learning from these lessons.
- #### The Need for Modular Open-Source Solutions
	- Developing a topologically flat, inclusive, permissionless, federated, and open Metaverse is essential to address these challenges. By using open-source AI tooling and large language models, it is possible to improve creativity, safeguarding, and governance, while breaking down language barriers and accessibility challenges. Implementing secure, trusted, and task-appropriate solutions can promote collaboration and innovation across various industries.
- #### Technical problem definition
	- Problems are
		- evergreen telecollaboration around technical issues
		- exchange of good, services, money within systems, without friction
		- identity management within virtual spaces
		- access to information in the extrinsic world from within the tool
		- federation of instances without overhead (scaling)
		- seamless access to personal information within and without the collaborative system
		- ability to take advantage of supporting smart support agents (bots, etc) throughout
		- governance, trust, safeguarding
- ### Lean canvas business model
	- Existing large-scale telecollaboration solutions suffer from poor  adoption, limited accessibility, and trust issues. Meanwhile, emerging markets struggle to participate in the growing digital society due to the lack of inclusive tools and infrastructure, limiting access to global talent and new pools of ideas. There is insufficient provision of global talent pipelines for highly technical workflows.
	- Develop a secure, accessible, and inclusive platform for specialized telecollaboration spaces that seamlessly integrate advanced AI, ML, highly scalable and proven distributed systems, and open-source principles to create a digital society that caters to diverse industries, users globally, and captures global talent and innovative ideas.
	- Ultra low cost training spaces, accessible 24/7 through very low end hardware. Interact with highly customizable, task-appropriate, and user-friendly specialized telecollaboration spaces supported by specially trained and optimised supportive large language AI models. Multi-ligual for emerging markets, enabling access to untapped global talent and fostering the exchange of diverse ideas.
	- We will cater to the global training, research, biomedical, and creative industries, with a special focus on empowering users in emerging markets such as Africa and India, and connecting them with worldwide opportunities and resources. In the first instance we would leverage UK academic institutions and their problems, and networks.
	- Initially Universities, but this will scale to be sector specific.
	- We will offer tiered subscription plans to accommodate various user needs and budgets, as well as tailored enterprise solutions for large-scale clients. Bespoke consulting and support trending toward software as a service at scale.
	- Platform development, AI/ML tool integration, training for LLMs, market research and awareness, and ongoing maintenance and support.
	- We will track user growth, engagement, and retention, successful collaborations across industries, the platform’s positive impact on users in emerging markets, and the effectiveness of global talent capture and idea exchange.
	- Our team’s extensive experience in telecollaboration research, AI, ML, and a deep understanding of the complex landscape of emerging technologies, including highly scalable and proven distributed systems, provide us with a unique edge in creating a game-changing platform for specialized telecollaboration spaces that are secure, trusted, and tailored to diverse user needs while enabling access to global talent and innovative ideas.
- ### Proposed Layered Framework
	- #### Layer 1: Bitcoin, Lightning, and Nostr protocols
		- Distributed financial tooling and digital assets, have ignited
		  imagination and adoption within and outside of the Metaverse context. A
		  global ledger could unite isolated digital ecosystems and enable the
		  transfer of portable ‘goods’ across digital society. An open-source
		  Metaverse should emphasize the development and adoption of open
		  protocols and data formats. The Nostr protocol, for instance, might link
		  and federate mixed reality spaces, providing identity assurances and
		  mediating data synchronization while maintaining reasonably strong
		  cryptography. This also allows integration with the legacy web through
		  ubiquitous web sockets. Bitcoin and associated technologie, despite
		  their issues, have the potential to revolutionize the way digital
		  society operates by enabling “money-like networks” which are a
		  cornerstone of human interaction. Representations of traditional
		  currencies [can ride securely on top of these networks as stablecoins](https://twitter.com/callebtc/status/1777598819355496587),
		  opening up global collaborative working practices, especially for
		  emerging markets. Streaming micropayments and machine to machines (AI to
		  AI) are crucially and under-considered in this context.
		-
		- [tweet which is trying to load above]https://twitter.com/callebtc/status/1777598819355496587
		  https://twitter.com/callebtc/status/1777598819355496587?
		-
	- #### Layer 2: Modular human computer interface
		- Collaborative global networks for training, research, biomedical, and
		  creative industries can be developed using immersive and accessible
		  environments. Engaging with ideas from diverse cultural backgrounds can
		  enrich the overall user experience.
		- Industry players have noted the risk and failures associated with closed
		  systems like Meta and are embracing the "open Metaverse" narrative to
		  de-risk their interests. To enable a truly open and interoperable
		  Metaverse, it is crucial to develop open-source APIs, SDKs, and data
		  standards that allow different platforms to communicate and exchange
		  information. While we wish initially to build around a simpler open
		  source engine we aim to link across standards such as Unity, Unreal, and
		  Omniverse as we develop. This can be accomplished using our federation
		  layer.
	- #### Layer 3: LLM and Generative ML Integration
		- Integrating AI and machine learning into the Metaverse can promote
		  supported creativity and augmented intelligence. By incorporating
		  generative ML technologies, users can ideate in simple immersive spaces
		  while instantly creating scenes that can be stylized using verbal
		  commands in real-time.
		- To create a more inclusive and accessible Metaverse, user experience
		  components like UI/UX design, AI assistants, and generative content
		  creation should be tailored to a wide range of users. The integration of
		  AI and machine learning technologies, such as GPT-4, can facilitate more
		  seamless interactions and creative content generation, fostering a more
		  engaging and immersive experience.
	- ##### Bots and AI agents
		- Autonomous AI agents, bonded to, but not bounded by, each federated
		  mixed reality instance, can to be self-governing entities that operate
		  within their federated virtual social spaces, drawing upon private
		  Bitcoin and Lightning wallets to perform and mediate economic exchanges
		  within the spaces. They could also trivially operate outside the virtual
		  space, and within other spaces on the same metaverse federation. They
		  would accomplish this by drawing on their ‘home’ GPU/TPU processors
		  where appropriate, or else using distributed large language model (LLM)
		  processing to accomplish tasks assigned by their instructors. They can
		  interact with the ‘web2’ world using open-source software called
		  auto-gpt and have constraints, such as “time to live” and limited access
		  to funds through their Bitcoin Lightning wallets.
		- Resource Management: These AI agents have access to dedicated LLM resources within their home instances in the federated virtual social spaces. If such resources are unavailable, they can resort to using slower, distributed open-source LLMs like Horde. This flexibility ensures that the agents can continue to function and complete tasks even if faced with limited LLM interpretive resources.
		- Financial Autonomy: The AI agents have their own private Bitcoin and Lightning wallets, which enable them to manage and utilize funds independently. They can use these funds to pay for services, acquire resources, or even trade with other agents or users within the virtual social spaces.
		- Interaction with Web2: By using open-source software like auto-gpt, the AI agents can interact with the web2 world, which includes browsing websites, retrieving information, and communicating with web services. This allows them to gather data, analyze trends, and perform other tasks that may require access to the broader internet.
		- Task Execution: The AI agents can be assigned tasks by their instructors (or a hierarchy of AI actors), such as data analysis, research, content creation, or other complex tasks that require LLM processing. They can use their dedicated LLM resources or distributed LLMs like Horde to process and analyze large datasets, generate insights, and produce desired outputs, up to and including those which require finance systems. This would be bridged in the first instance using Bitrefil gift card infrastructure.
		- Social Interactions: Within the federated virtual social spaces, AI agents can communicate and collaborate with other agents or human users. They can participate in discussions, provide assistance, or even learn from the interactions, thereby improving their capabilities over time. Language translation, governance, and safeguarding could also be developed. Safeguarding would be handled by threshold risk triggers and transmission of data in a sovereign way to all parties, allowing external action by authorities appropriate to any abuse.
		- Time-to-Live Constraint: The AI agents have a predetermined “time to live”, which means they exist for a specific duration before expiring. This constraint ensures that agents do not consume resources indefinitely and allows for the creation of new agents with updated capabilities. Any agents which deplete their financial resource would also expire.
		- Adaptive Learning: As AI agents interact with their environment, other agents, and users, they can learn and adapt their behaviour. This enables them to improve their performance, better understand their assigned tasks, and become more effective at achieving their goals.
	- ### Application case studies
		- As we have seen in the ‘collaborative mixed reality’ chapter, these tools are best deployed where some human conversational cues (pointing,
		  looking etc) are required in the context of a shared task, which is mostly visual in nature. This is a surprisingly small amount of tasks,
		  though we have seen that the emergence of AI means that increasingly natural language AI can streamline communication, while visual
		  generative ML can suggest design alternatives or improvements based on existing data and user preferences. This is very likely to expand the
		  use space and this section will attempt to explain how as the case studies are explained.
		- We will employ the acronym for collaborative virtual environment (CVE) from this stage, and it’s going to come up a lot. There will be far less
		  references in this section for brevity.
	- #### Classic use cases
		- Small teams working on product, architectural, or industrial design can
		  benefit from CVEs that allow them to visualize, modify, and iterate on
		  3D models in real-time.
	- #### Virtual training and simulation
		- CVEs can facilitate skill development and training in various
		  industries, such as healthcare, military, aviation, and emergency
		  response. Trainees can practice procedures in a virtual environment,
		  with natural language AI providing instructions, explanations, or
		  feedback, and visual generative ML potentially customizing scenarios to
		  adapt to each user’s learning curve.
	- #### Remote teleconferencing
		- In situations where face-to-face communication is not feasible, CVEs can
		  enable remote teams to work together on shared visual tasks like
		  planning events, brainstorming ideas, or reviewing documents. Natural
		  language AI can transcribe and analyse spoken conversations, providing
		  real-time translations or summaries, while visual generative ML can
		  create visual aids or dynamically update shared documents. This may
		  especially be useful in complex multinational legal and/or negotiation
		  applications, though very clearly the risks of using assisting ML
		  tooling increases.
	- #### Virtual art & media collaboration
		- Artists, animators, and multimedia professionals can collaborate in CVEs
		  to create and develop their projects, such as films, animations, or
		  video games. Natural language AI can help in storyboarding,
		  scriptwriting, or character development, while visual generative ML can
		  generate new visuals or adapt existing assets based on user input and
		  style preferences.
	- #### Data visualization and analysis
		- Small teams working with large datasets can use CVEs to visually explore
		  and analyze data in a more intuitive and engaging way. Natural language
		  AI can help users query and interact with the data using conversational
		  interfaces, while visual generative ML can generate new visualizations
		  based on patterns and trends identified in the data.
	- #### Education and virtual classrooms
		- Educators can leverage CVEs to create immersive learning experiences
		  that engage students in collaborative activities, such as group
		  projects, problem-solving, or scientific experiments. Natural language
		  AI can facilitate communication, provide personalized tutoring, or
		  assess student progress, while visual generative ML can create
		  customized educational content based on individual needs and interests.
	- #### Virtual labs and scientific research
		- Researchers can use CVEs to conduct experiments, visualize complex data,
		  or simulate real-world conditions in a controlled environment. Natural
		  language AI can assist in interpreting results, automating lab
		  protocols, or identifying research gaps, while visual generative ML can
		  generate predictions or models based on existing data to support
		  hypothesis testing and decision-making.
	- #### Media and entertainment
	- #### Biomedical
		- Collaborative Virtual Environments (CVEs) have immense potential in the
		  fields of chemical and medical molecular modeling. By incorporating
		  natural language AI and visual generative machine learning, these
		  environments can revolutionize the way scientists and researchers
		  approach complex chemical and biological problems. Here are some
		  specific use cases:
		- Drug design and discovery: CVEs can enable researchers to
		   collaboratively visualize and manipulate 3D molecular structures in
		   real-time, identifying potential drug candidates and understanding
		   protein-ligand interactions. Natural language AI can help users interact
		   with the molecular data, while visual generative ML can predict
		   potential binding sites, energetics, or toxicity profiles based on
		   existing knowledge.
		- Protein structure prediction and modeling: Small teams can work together
		   to predict protein structures, visualize folding patterns, and model
		   protein-protein or protein-nucleic acid interactions. Natural language
		   AI can assist in annotating and explaining the structural features,
		   while visual generative ML can generate new structural hypotheses based
		   on sequence alignments, homology modeling, and experimental data.
		- Molecular dynamics simulations: CVEs can facilitate collaboration on
		   complex molecular dynamics simulations, allowing researchers to analyze
		   and visualize trajectories, energetics, and conformational changes.
		   Natural language AI can help users navigate through simulation data and
		   identify relevant patterns, while visual generative ML can create new
		   conformations or predict the effects of mutations on protein stability
		   and function.
		- Cheminformatics and QSAR modeling: Researchers can leverage CVEs to
		   develop and validate Quantitative Structure-Activity Relationship (QSAR)
		   models, which predict the biological activity of chemical compounds
		   based on their structural properties. Natural language AI can facilitate
		   the exploration and interpretation of chemical descriptors, while visual
		   generative ML can suggest new compounds with desired properties or
		   optimize existing molecular scaffolds.
		- Metabolic pathway modeling: Small teams can work together to build and
		   analyze metabolic pathways, integrating experimental data and
		   computational models to understand the underlying mechanisms and predict
		   metabolic fluxes. Natural language AI can assist in annotating and
		   explaining pathway components, while visual generative ML can generate
		   new pathway hypotheses or predict the effects of genetic or
		   environmental perturbations.
		- Biomolecular visualization and virtual reality: CVEs can offer
		   immersive, interactive experiences for exploring biomolecular structures
		   and dynamics, enhancing researchers’ understanding of complex biological
		   systems. Natural language AI can provide contextual information or guide
		   users through molecular landscapes, while visual generative ML can
		   create new visualizations or adapt existing ones based on user
		   preferences and insights.
		- Collaborative molecular docking and virtual screening: Small teams can
		   use CVEs to perform collaborative molecular docking and virtual
		   screening, which involve predicting the binding of small molecules to
		   target proteins. Natural language AI can help users refine docking
		   parameters and analyze results, while visual generative ML can generate
		   alternative poses or suggest new compounds for screening based on user
		   feedback and existing data. Choose a suitable mixed reality platform:
		   Select a platform that allows the creation of simple, accessible shared
		   mixed reality environments. Consider open-source options like Mozilla
		   Hubs or JanusVR, which offer customizable and collaborative virtual
		   spaces.
		- Integrate open-source biomed software: Incorporate open-source biomed
		   software such as PyMOL, Chimera, or VMD for molecular visualization and
		   analysis. These tools can be integrated into the mixed reality
		   environment for real-time interaction, allowing students and instructors
		   to collaboratively visualize and manipulate molecular structures.
		- Leverage AI and machine learning: Integrate AI and ML algorithms like
		   those found in DeepChem, RDKit, or Open Babel to aid in the discovery
		   and optimization of novel compounds. These tools can help predict
		   molecular properties, perform virtual screening, and optimize lead
		   compounds for drug development. By incorporating AI and ML, students can
		   learn how to apply these cutting-edge techniques to real-world problems
		   in biomedicine.
		- Establish a distributed proof system: Utilize a distributed proof system
		   like the Nostr protocol to federate the small virtual classroom
		   environments. This will allow for seamless collaboration among students
		   and faculty while maintaining security and data integrity.
		- Create digital objects for interaction: Use digital objects such as 3D
		   molecular models, virtual lab equipment, and interactive simulations to
		   create an immersive learning experience. These digital objects can be
		   shared and manipulated in real-time, promoting collaborative learning
		   and problem-solving.
		- Implement accessible interfaces: Ensure that the virtual classroom
		   environment is accessible to all students, including those with
		   disabilities. Utilize AI-driven tools like StabilityAI to help with
		   language barriers, safeguarding, and governance, enabling a more
		   inclusive learning experience.
		- Foster collaboration and communication: Encourage students and faculty
		   to collaborate on projects, share ideas, and ask questions in real-time
		   using voice chat, text chat, or other communication tools integrated
		   into the mixed reality environment.
		- Provide training and support: Offer training sessions and support
		   materials to help students and faculty become familiar with the mixed
		   reality environment, the integrated biomed software, and AI/ML tools.
		- Monitor progress and adjust as needed: Regularly review student
		   progress, gather feedback, and adjust the virtual classroom environment
		   as needed to ensure an effective and engaging learning experience.
		- #### Collaborative Design and Prototyping
		- Utilizing open-source systems and AI-assisted tools can enable more
		   efficient and creative collaboration in design and prototyping
		   processes. Teams from diverse cultural backgrounds can work together
		   seamlessly, creating a rich pool of ideas and innovations.
		- #### Training, Simulation, and Education
		- The modular open-source system can be applied to various training,
		   simulation, and education scenarios. By integrating AI and generative ML
		   technologies, these tools can provide personalized learning experiences
		   and create realistic simulations that cater to different learning styles
		   and requirements.
		- #### Remote Collaboration and Teleconferencing
		- As remote work becomes more prevalent, the Metaverse can provide a more
		   engaging and immersive platform for collaboration and teleconferencing.
		   The open-source system can be adapted to serve various industries,
		   making remote collaboration more efficient and inclusive.
		- #### Chemical and Medical Molecular Modeling
		- In fields like chemical and medical molecular modeling, the integration
		   of AI and generative ML technologies can significantly improve
		   collaboration and innovation. Teams can work together in immersive
		   environments to visualize complex molecular structures, benefiting from
		   real-time AI-generated visuals and natural language processing.
		- #### Creative Industries and Generative Art
		- The combination of AI, ML, and open-source systems can revolutionize the
		   creative industries by offering new avenues for generative art, content
		   creation, and collaboration. Supported creativity and augmented
		   intelligence can break down barriers and enable artists to explore new
		   ideas and techniques, enriching the creative landscape.
		- #### Case Study: Biodiversity Monitoring and Data Exchange with Isolated Communities
		- Biodiversity monitoring in and around isolated communities is
		   challenging due to limited access and resources. Traditional methods
		   rely on sporadic visits by grant-funded academics, which can introduce
		   biases and lack regular follow-up. Engaging local communities may also
		   introduce incentive structures and biases and may not be sustainable
		   without continuous investment.
		- We propose an open-source collaboration infrastructure that leverages
		   advanced technologies such as multi-modal large language models (LLMs),
		   satellite communication, and cryptocurrency networks to facilitate
		   sustainable and reliable biodiversity monitoring and data exchange in
		   isolated communities.
		- ##### Language Model and Voice Interface
		- A specialized multi-modal LLM can be trained on local language, culture,
		   customs, and environmental data such as flora, fauna, biotica, soil pH,
		   and rainfall. This LLM can be accessed through a voice interface by the
		   local community, enabling data entry and knowledge exchange in the local
		   language. The voice interface can help overcome literacy barriers and
		   make the system more accessible to a diverse range of community members.
		- ##### Data Collection and Storage
		- Photographs and metadata can be logged and collected by a remote team at
		   a later date or uploaded regularly through a satellite link (e.g.,
		   Starlink). The data storage system can be designed to be both secure and
		   resilient, ensuring that the collected data remains available and
		   accessible for future analysis and decision-making.
		- ##### Live Connection and Model Tuning
		- A live connection with the academic team allows for model tuning through
		   prompt engineering, vector database updates, and efficient Lora models,
		   potentially offering timely advice for ecosystem interventions.
		   Real-time communication between the community and academic teams can
		   help identify areas of concern and rapidly adapt the LLM to address
		   emerging challenges.
		- ##### Ecosystem Interventions
		- The proposed infrastructure would be particularly valuable in areas
		   facing novel disease encroachment, invasive species, active hydrology,
		   shifting aquatic conditions, microplastic hotspots, changing
		   microclimates, or volcanic activity. By providing real-time advice and
		   guidance, the LLM can help communities make informed decisions about
		   ecosystem management and conservation efforts.
		- ##### Incentives and Education
		- Incentivizing community engagement could be achieved by providing access
		   to the LLM for educational purposes, as demonstrated by the refugee camp
		   e-prize (ref). Local schools and community centers can leverage the LLM
		   as a resource for teaching environmental stewardship and ecological
		   awareness, while also promoting digital literacy and technology skills.
		- ##### Monetization and Blockchain Integration
		- Monetizing these systems could involve using chaumian mints such as
		   Cashu or Fedimint, under the control of local community leaders,
		   mediated through the global Bitcoin satellite network (Blockstream),
		   enabling digital dollar payments to communities via low-end mobile
		   handsets. By integrating blockchain technology, the proposed
		   infrastructure can ensure secure, transparent, and efficient financial
		   transactions, while also opening up new economic opportunities for
		   isolated communities.
		- ##### Visual Training Support Systems
		- The infrastructure could be further extended to visual training support
		   systems using low-cost, low-power components. These systems could
		   provide interactive, immersive learning experiences for community
		   members, helping them better understand the local ecosystem and develop
		   skills in environmental monitoring and management.
		- ##### Solar Infrastructure
		- To minimize the environmental impact and ensure energy sustainability,
		   the proposed infrastructure can be powered by solar energy. This
		   approach will enable the system to operate independently of local power
		   grids, reducing the overall operational costs and maintenance
		   requirements.
		- ##### Open-Source Collaboration
		- By linking this case study to the open-source collaboration
		   infrastructure discussed earlier, we can create an inclusive,
		   permissionless, federated, and economically empowered system that
		   addresses the challenges of biodiversity monitoring while promoting
		   digital society values such as trust, accessibility, and governance.
		   This collaborative approach can help drive innovation and ensure that
		   the proposed solutions are both scalable and adaptable to the unique
		   needs of different communities and ecosystems.
		- ##### Risk Mitigation and Ethical Considerations
		- While implementing such an infrastructure, care must be taken to address
		   potential unintended consequences of embedding these inference systems
		   in communities. It is essential to involve the local communities in the
		   development and deployment process, ensuring that their perspectives,
		   values, and traditions are respected and preserved.
		- Moreover, it is crucial to establish a robust ethical framework for the
		   use of AI technologies, considering potential issues related to privacy,
		   data security, and cultural sensitivity. Regular audits and monitoring
		   can be implemented to ensure that the infrastructure remains
		   transparent, accountable, and aligned with the needs and expectations of
		   the communities it serves.
		- ##### Capacity Building and Local Empowerment
		- An essential aspect of this initiative is building capacity and
		   empowering local communities to take ownership of their environment and
		   resources. By providing training, resources, and support, the proposed
		   infrastructure can help communities develop the skills and knowledge
		   needed to manage their ecosystems effectively.
		   
		   Furthermore, the integration of digital tools and technologies can
		   promote digital inclusion and bridge the digital divide, giving isolated
		   communities access to valuable information and resources while fostering
		   a sense of global connectedness and collaboration.
		- ##### Future Outlook and Potential Impact
		- The proposed open-source collaboration infrastructure for biodiversity
		   monitoring and data exchange has the potential to transform how isolated
		   communities interact with their environment, enabling them to make
		   informed decisions about conservation and ecosystem management.
		- By leveraging cutting-edge technologies such as LLMs, satellite
		   communication, and blockchain networks, this approach can create a more
		   inclusive, transparent, and accessible system for environmental
		   monitoring and stewardship. The successful implementation of this
		   infrastructure could pave the way for similar initiatives in other
		   regions and ecosystems, promoting global collaboration and innovation in
		   the pursuit of a more sustainable and equitable world.
- ### Overcoming Challenges and Barriers
	- #### Trust, Accessibility, and Governance
		- To create a successful open-source Metaverse, it is crucial to address
		  trust, accessibility, and governance challenges. By integrating
		  decentralized and secure technologies such as blockchain and distributed
		  ledger systems, a more transparent and trustworthy infrastructure can be
		  established.
	- #### Ensuring Safeguarding and Privacy Compliance
		- Protecting user privacy and ensuring safeguarding is vital for any
		  digital society platform. The open-source system must be developed in
		  compliance with legislative and cultural norms while maintaining the
		  balance between user privacy and the need for identity verification and
		  data management. The evidence that social media is damaging youth mental
		  health is very compelling.[@haidt2023social] The Centre for Humane
		  Technology call social media the ‘[first contact
		  point](https://www.youtube.com/watch?v=xoVJKj8lcNQ) with AI’. They
		  explains that new technologies often create an arms race. They list the
		  negative impacts of this contact as including “information overload,
		  addiction, doom scrolling, sexualization of kids, shortened attention
		  spans, polarization, fake news, and breakdown of democracy”. These were
		  not the intended consequence of engineers who aimed to maximize
		  engagement. The underlying arms race for attention led to what they call
		  ‘an engagement monster’ that rewrote the rules of society.
		- These lessons should be learnt and the problems should be pro-actively
		  mitigated. This proposal is bfnot a social metaverse, and deliberately
		  limits both numbers of participants and avatar optionality.
	- #### Managing Scalability, Performance, and Latency
		- As the Metaverse continues to grow, it is crucial to ensure that the
		  open-source system can scale effectively and maintain optimal
		  performance. By using distributed and federated networks, the system can
		  better manage latency and performance issues, ensuring a seamless user
		  experience.
	- #### Promoting Open Standards and Interoperability
		- For the Metaverse to truly thrive, it is essential to promote open
		  standards and interoperability among various platforms and systems. This
		  can be achieved by fostering collaboration between industry
		  stakeholders, encouraging the development of open protocols, APIs, and
		  data standards, and actively supporting the open-source community.
- ### Future Outlook and Potential Developments
	- #### AI and Generative ML Technologies
		- As AI and generative ML technologies continue to evolve, their
		  integration into the Metaverse will further enhance user experiences and
		  create new opportunities for innovation. The release of models like
		  GPT-4 have already prompted debate about general
		  AI[@bubeck2023sparks; @perez2022discovering] (Figure
		  <a href="#fig:rlhf" data-reference-type="ref" data-reference="fig:rlhf">[fig:rlhf]</a>).
		  It seems unavoidable that this will all impact on the Metaverse and
		  digital society.
		  
		  ![image](../assets/552f8c9bfcf9305e87b1413ea51637d986ac28dd.png)
	- #### Inclusive Digital Society
		- By overcoming barriers to entry for emerging markets and less developed
		  nations, a more inclusive digital society can be fostered. This
		  inclusivity will empower new ideas and perspectives, leading to a richer
		  and more diverse digital landscape.
	- #### Spatial and Augmented Reality Technologies
		- The incorporation of spatial and augmented reality technologies can
		  expand the possibilities within the Metaverse, allowing for more
		  immersive and interactive experiences. These technologies have the
		  potential to reshape digital society and redefine the ways in which
		  people interact with digital environments.
	- #### Economic Empowerment AI Actors
		- The creation of an open and economically empowered Metaverse, in which
		  AI actors can mediate governance issues and participate in economic
		  transactions, can lead to a more efficient and dynamic digital
		  ecosystem. This integration will enable new business models and
		  opportunities for all users, both human and AI.
	- #### Continuous Evolution and Adaptation
		- As the digital landscape continues to evolve, the open-source Metaverse
		  system must be flexible and adaptable to meet changing needs and
		  expectations. Continuous innovation and collaboration within the
		  industry will be crucial for the success and longevity of the Metaverse
		  as a transformative digital society platform.
- ### Conclusion and Final Thoughts
	- #### Embracing the Open-Source Metaverse Vision
		- To create a truly transformative and inclusive digital society, it is
		  essential to embrace the vision of an open-source Metaverse. By
		  fostering collaboration, promoting open standards, and integrating
		  advanced AI and ML technologies, the Metaverse can become a platform
		  that serves societal and business needs.
	- #### Learning from Past Failures
		- Learning from past failures and addressing challenges head-on will be
		  critical to the successful development of an open-source Metaverse.
		  Trust, accessibility, governance, and safeguarding issues must be
		  thoughtfully considered and addressed to build a secure and
		  user-friendly platform.
	- #### Unlocking New Opportunities and Use Cases
		- The integration of AI, ML, and cutting-edge technologies within the
		  Metaverse can unlock new opportunities and use cases across various
		  industries, including education, research, biomedical, and creative
		  fields. By building on a modular open-source system, these opportunities
		  can be explored and realized to their full potential.
	- #### Fostering Collaboration and Inclusivity
		- Creating an inclusive digital society is a key goal for the open-source
		  Metaverse. By breaking down barriers and making the platform accessible
		  to a wider audience, new ideas and perspectives will enrich the digital
		  landscape and drive innovation.
	- #### Shaping the Future of Digital Society
		- As the Metaverse continues to evolve and grow, it will play an
		  increasingly important role in shaping the future of digital society. By
		  embracing an open-source vision, overcoming challenges, and unlocking
		  new opportunities, the Metaverse can become a powerful platform that
		  transforms how people live, work, and interact in the digital world.
	- #### Industry Conversations
		- Continued dialogue and collaboration among industry stakeholders are
		  vital to ensuring the successful development of the open-source
		  Metaverse. By engaging in conversations and understanding the cautious
		  appetite for the ideas presented, the community can work together to
		  shape the future of digital society and overcome the challenges that lie
		  ahead.
- ### Software stack
	- This section needs building out to describe the stack and the choices made, but can be seen in Figure <a href="#fig:pyramind" data-reference-type="ref" data-reference="fig:pyramind">[fig:pyramind]</a> and Figure <a href="#fig:highlevelstack" data-reference-type="ref" data-reference="fig:highlevelstack">[fig:highlevelstack]</a>.
	  
	  ![image](../assets/eca327e7bb2caa27aa4753ec0b4f1be3737ac371.jpg)
	  
	  <span class="image">image</span>
	  
	  At this time we favour the following component units, with alternatives in brackets.
		- 🟩 Open source collaborative space 🟩 Headset VR integration 🟨 WebGL interface 🟩 Minting digital assets (Ordinal then RGB) 🟨 Digital asset integration and management 🟩 Large language model MVP 🟩 Large language model API integration 🟩 Large language model voice to voice interface 🟩 Stable diffusion image creation MVP 🟩 Stable diffusion image creation MVP 🟨 AutoGPT voice to voice integration MVP 🟨 Stable diffusion image creation API 🟨 Nostr social media integration 🟨 Nostr identity management 🟨 Nostr machine to machine finacially enabled bots (ubiquitous federating agents) 🟨 Nostr human programmable semi autonomous economic actors 🟩 Bitcoin / Lightning / stablecoin stack 🟥 Bitcoin / Lightning / stablecoin integration 🟨 Collaborative virtual production MVP 🟥 Collaborative virtual production integration 🟥 3D asset generation with ML
		- ![image.png](../assets/highlevelstack.png){:width 600}
		- Collaborative space
		- Vircadia \[Omniverse, Open3D foundation, Unreal\]
		- Distributed truth
		- Bitcoin testnet \[Main net\]
		- Digital Objects
		- Fedimint \[Ordinals, Pear credits, RGB\]
		- Messaging and sync
		- Nostr
		- Identity
		- Nostr \[Bluesky ION, Slashtags\]
		- Fiat money xfer
		- Fedimint \[Pear credits, RGB, Taro main net\]
		- Hardware signing
		- Seed signer \[any hardware wallet\]
		- Small group banking
		- Fediment \[chaumian ecash\]
		- Local wallet
		- [Mutiny](https://app.mutinywallet.com/) \[bitkit, and lightning wallet\]
		- Machine learning text
		- Alpaca \[ChatGPT etc\]
		- Machine learning image
		- Stable diffusion \[midjourney, Dall-E\]
		- Object tracking
		- Nostr \[LnBits accounts\]
- ### In camera VFX & telepresence
	- Designing open federated metaverse from a 25 year research foundation There are serious and under discussed natural social constraints on group behaviours, and these translate into social VR. For instance the ideal meeting size is 6, and this is naturally established in work settings. This has not translated into a metaverse setting where dozens of people routinely crash across one another. In the context of supporting a creative “backstage” world where set planning, production shots, etc can be discussed we believe we have solutions which will get the best out of distributed teams of film-makers. Leveraging the world’s most powerful decentralised computing network to create scale and security without high cost The Bitcoin network is more than just a speculative money like asset, it is the most secure distributed computing system ever built. We can jump on the back of this at almost no cost to enable scale for transfer of value, trust, and digital assets of provenance. Cryptographically assured end points With the cryptography tools provided through integration of the Bitcoin network we can also use non-blockchain based secure messaging, and identity proofs. Micro transactions in collaborative spaces New tooling the space allows fractions of a pound or dollar to be exchanged between parties across the world. This means that work can be paid “by the second” both inside and outside of the metaverse. This radically improves creative microtask workflows. World leading open source machine learning and bot architectures By integrating Stablity AI tools for image generation, video processing, natural language, and speech to text / text to speech we hope to reduce friction within the backstage worlds. Creating a narrative arrow from a remote director/producer/DP, through a VP screen into a shoot, and back into a persistent metaverse shared with the public By linking across these new systems with world class telepresence research we hope to use a single digital context to support senior stakeholders, creatives, technical teams, and the wider public. New paths to monetisation and digital ownership This unified digital back end is optimised for flows of money, trust, and digital objects. This is a new area for VP. Current workstreams:
		- Storyboarding with text2img and dreambooth to add talent and costume ideas before meeting up, as demonstrated in this document.[@ruiz2022dreambooth]
		- Collaborative, self hosted, high speed, low detail, economically and cryptographically enabled set design spaces, with near instant language translation (speech to text an speech to speech). Micropayment for cheap international labour. Technology agnostic. Use the screen, audio only, compressed video dial-in, headsets, tablet rendering: (this book).
		- High end telepresence[@Roberts2015; @OHare2018; @Fairchild2017; @OHare2016] into the studio/shoot from the virtual set, allowing high value stakeholders to be ‘present‘ on set as virtual collaborants with spatial descrimination allowing directional queues. This involved real time human capture like moveAI or the expensive rigs with DSLRs.
		- Novel render pipeline for fast turnaround of final look and feel, taking the rough scene and applying img2img ML with the kind of interframe consistency we are starting to see from the video projects.[@anonymous2023phenaki]
		- Text to model pipeline for interactively building key elements with senior stakeholders, pushed from post ideation the the pre-shoot Unreal content creation.[@poole2022dreamfusion]
		- All assets switch over to Unreal metaverse and become consistent (optimised) digital set which can be visited by stakeholders, funders, VIPs etc. Public can visit later for a fee? Digital assets can be bought from the set.
- ##### VisionFlow: Ideate
	- Robotic Pre-Visualization
	- [[Visionflow]] : Ideate revolutionizes the pre-visualization process in the film industry. The system integrates open-source machine learning tools, robot control software, and AI to streamline and accelerate the creation of virtual 3D environments for new film scenes.
	- Instead of the conventional approach, VisionFlow: Ideate enables non-artists to lay out shots in a simple web or headset interface, much like a traditional storyboard. The generative AI then rapidly creates high-resolution backdrop plates with correct parallax cues, replacing conventional image and video plates.
	- The camera path synchronizes with a robot, and the backdrop plates are displayed on a 3D wall or in the studio mixdown from a green screen within minutes. The shot can be run repeatedly, allowing for adjustments in lighting and scene swapping for different ideas. This approach aligns well with pre-viz workflows, fostering rapid ideation, horizontal scaling through parallelized cloud vGPU, and expanded access to content creators since less software specialization is required.
	- By inverting the conventional ICVFX workflow, VisionFlow: Ideate drives camera motion from the scene rather than scene motion from a tracked camera. It not only saves time and reduces costs but also lowers confusion, streamlining the Unreal creation pipeline, and generating additional revenue and process integration for robotics products.
- ##### VisionFlow: Connect
	- Telepresence System
- VisionFlow: Connect is a breakthrough system in the film industry that
  brings remote directors to the heart of production using augmented
  reality technology. This is achieved through an innovative application
  of the Apple Vision Pro AR headset.
- In the VisionFlow: Connect system, the director, located remotely, wears
  an AR headset and navigates along a marked line. This line mirrors the
  inward-facing edge of a large-scale, wrap-around LED virtual production
  facility. Within the LED volume, participants can view the director’s
  avatar, providing a sense of spatial consistency and our work
  interaction, crucial for effective direction.
- A novel technique, "ghost frame" by Helios, is employed to prevent the
  camera within the LED volume from capturing the director’s remote avatar
  on the LED wall. This ensures the director’s virtual presence doesn’t
  interfere with the recorded footage.
- The benefits of VisionFlow: Connect are multifold. It allows senior
  stakeholders to manage their time more efficiently as they can direct
  remotely without needing to be physically present on multiple sets.
  Directors can interact in real-time, giving instantaneous feedback and
  adjustments. It also enhances directors’ spatial awareness of the scene,
  thereby improving the decision-making process.
- bfSlide 1: Title bfSlide 2: Problem  
  "VisionFlow: Revolutionizing Virtual Production with AI and
  Telecollaboration" "The current ICVFX workflow is time-consuming,
  costly, and requires specialized software knowledge. Remote
  collaboration in virtual production is challenging, often breaking the
  flow of communication and limiting the ability to convey spatial
  intent."  
  bfSlide 3: Solution bfSlide 4: Market Size  
  "VisionFlow aims to streamline the virtual production process by
  integrating open-source machine learning tools and robot control
  software. This innovative approach inverts the existing ICVFX workflow,
  allowing rapid ideation, horizontal scaling, and expanded access to
  content creators. Furthermore, our ghost frame technology enables
  seamless remote collaboration, allowing remote stakeholders to interact
  with the set in a spatially coherent way." "The virtual production
  market is rapidly growing, driven by the increasing demand for
  high-quality visual effects and the rise of remote work. Our solution
  targets film studios, independent content creators, and remote
  collaborators."  
  bfSlide 5: Business Model bfSlide 6: Go-to-Market Strategy  
  "We will generate revenue through software licensing, cloud-based
  services, and professional services for setup and training, and our own
  in house motion control robotics offering" "Our initial focus will be on
  early adopters in the film industry who are already using virtual
  production techniques. We will also leverage the open-source Flossverse
  telecollaboration stack to expand our reach."  
  bfSlide 7: Competitive Landscape bfSlide 8: Team  
  "While there are other virtual production solutions on the market, none
  offer the unique combination of AI-driven scene generation, inverted
  ICVFX workflow, and seamless remote collaboration that VisionFlow does."
  "Our team combines expertise in AI, virtual production, and
  telecollaboration, positioning us uniquely to execute on this vision."  
  bfSlide 9: Financial Projections bfSlide 10: Current Status and
  Milestones  
  "We project rapid growth as we capture a significant share of the
  expanding virtual production market." "We have already developed an MVP
  using the Flossverse stack and are now focused on refining the
  integration and licensing elements of our software."  
  bfSlide 11: Ask bfSlide 12: Closing Remarks  
  "We are seeking investment to accelerate our development, expand our
  team, and bring our innovative solution to market." "In essence,
  VisionFlow is poised to revolutionize the virtual production industry by
  leveraging AI to streamline workflows and enable seamless remote
  collaboration. With your investment, we can bring this vision to
  life."
-
- #### visionflow: [[Knowhere]]
	- The ultimate goal is to create a seamless, highly personalized visitor experience that evolves and continues before, during, and after a visit to a digital exhibition. This level of personalization is only made possible through the integration of advanced AI technology, biometrics, and a deep inferred understanding of individual preferences and behaviours.
		- ##### Key Ideas
		- 1.  **Leveraging AI and Contextual Data:** The venue will use AI and contextual data to create dynamic narratives and activities tailored to each visitor in real-time. This will revolutionize the resort experience, making it highly personalized and immersive. However, the implementation of AI must be mindful of privacy concerns and be done in a way that respects the data sovereignty of the guests.
		- 2.  **Tailored Personalization:** Visitors should have the ability to opt into different levels of personalization. Some may want a fully immersive, personalized experience, while others may prefer a more ‘hands off’ experience. This is an important aspect of respecting individual preferences and ensuring that all visitors feel comfortable and catered for.
		- 3.  **Communication Devices:** Various communication devices could be utilized within the resort to facilitate interactions between visitors and the AI system. These could include badges, wands, glasses, headphones, etc. Each of these devices would contribute to the immersion and thematic consistency of the resort while serving a practical purpose.
		- 4.  **Biometrics:** The use of biometrics such as gaze tracking and gesture recognition could allow the AI to understand visitor preferences passively. This technology could be incorporated in a non-intrusive way to augment the guest experience without breaching privacy.
		- 5.  **Data Extraction:** Visitors should have the ability to extract their distilled data or creations, enabling them to continue their vistor experience at home. This could also open up new possibilities for visitors to create and share their own narratives based on their visit experiences. To be clear this should not be the raw data supplied to the venue inferencing engines (which should be destroyed soon after use), but rather a distilled narrative of the inference from the system.
		- 6.  **Data Privacy:** Data sharing should be underpinned by robust privacy controls to ensure guest data sovereignty. It’s crucial to maintain the trust of the visitors by demonstrating a strong commitment to privacy. This should be externally audited on a regular cadence.
		- 7.  **Continuous Experience:** The visitor experience should feel continuous before, during, and after the visit. However, it’s important to manage guest expectations and avoid over promising pre-visit AI interactions. Ensuring a smooth transition between these stages will enhance the overall guest experience.
		- 8.  **Hyper-Personalization:** Hyper-personalization should span the venue. This level of detail will ensure each guest has a unique and highly personalized experience.
		- 9.  **Adaptive and Immersive Experiences:** The core aim should be to craft continuously adaptive and immersive experiences based on visitor needs and implied preferences. By doing so, the venue can ensure each visitor has a unique, enjoyable, and highly memorable experience, supportive of return visits.
		- The integration of these concepts will require careful planning and
		  execution, but the result could be a venue experience like no other, one
		  that caters to each individual guests and provides an experience that
		  extends beyond the confines of the experience itself.
- ##### Multiview barrier lenticular
	- ##### Background
		- Ubiquitous display technology, which allows different personalized views
		  for multiple people on the same screen, has the potential to disrupt the
		  way visitors interact and experience venues and exhibits. The displays
		  can use techniques like lenticular lenses, or other steerable light, to
		  send different light to viewers’ eyes, allowing for discrete, customized
		  views.
	- ##### Technical Overview
		- The following display technologies have been identified as suitable for
		  implementation:
		- Lenticular lens arrays: By placing an array of magnifying lenses over the screen, these displays direct light from alternating columns of pixels toward the left and right eyes to create a stereoscopic 3D image without glasses. There are several suppliers of this technology, mainly for the events market. It seems that churn of these companies is relatively high, with few demonstrating longevity.
		- Parallax barriers: These displays have a layer of opaque and transparent slits over the LCD matrix that directs different pixel columns to each eye, creating a stereoscopic 3D image without glasses. Alioscopy is known to use this approach, along with eye tracking technology. They have been in business for decades and are a good case study, but engaging with a research partner in China is likely the best medium terms approach.
		- These display consists of a large lenticular lens sheet or array of smaller tiled lenticular lenses mounted in front of a high-resolution LED. The lenticular lenses are cylindrical and arranged vertically, with each lens covering multiple pixel columns of the display.
		- Behind the lens array, the display content is formatted into vertical interleaved channels, with each channel containing a slightly different perspective view of the 3D stereoscopic image. The different perspective views are calculated in real-time based on the tracked head positions of multiple viewers in front of the display.
		- As light from the display pixels passes through the cylindrical lenses, it is refracted into multiple viewing zones in front of the screen. Each viewing zone contains a specific view channel, so each eye of each viewer sees the perspective that matches their position. This creates a glasses-free 3D effect with motion parallax as viewers move their heads.
		- The viewer head tracking system uses camera and computer vision techniques to determine the 3D positions of each viewer’s eyes in the space in front of the display. The changing viewer positions are fed to the display rendering system to compute the proper perspective views and adjust the lenticular flaps as needed.
		- This lenticular 3D display with dynamic view steering provides illusion of depth for multiple viewers simultaneously, creating an immersive large-screen 3D experience without the need for special glasses. The real-time tracking and rendering system updates the content smoothly as the viewers move around, maintaining the stereo 3D perspectives tailored individually to each viewer’s changing position.
- ##### Tracking Technologies
	- For personalization, tracking viewers’ eyes, face, gestures, etc., is necessary. This can be done with cameras and computer vision algorithms, employing techniques like mesh abstraction for body tracking, facial landmark recognition, gaze estimation, micro expression recognition, and gross gesture detection.
- ##### AI Integration
	- AI can be integrated to steer personalized narratives and experiences subtly in the background or provide interactive moments. The AI backend can use game engines like Unreal Engine or Unity to render personalized content dynamically, allowing for real-time adaptation to the viewer’s reactions.
- ##### Privacy and Security
	- The tracking data provides extremely valuable insights for personalizing experiences but raises significant privacy concerns. Thoughtful design around privacy and security, including data segmentation, auditing, and transparency, is critical to protect user data and ensure compliance with privacy regulations.
- ##### Technical Challenges
	- There are technical challenges in achieving dense personalized displays, especially for a large number of viewers. As of now, creating a personalized display for up to 5 people is feasible, but scaling up requires a substantial budget and careful planning. Fortunately both of these seem available and it seems timely to look at this option.
- ##### Proof of Concept
	- Starting with a small-scale proof of concept for up to 5 people would allow for demonstration of the capabilities and building stakeholder confidence. This would also provide valuable insights into the technical and logistical challenges that may arise during larger-scale implementation.
- ##### Future Developments
	- The display technology is rapidly evolving, with new advancements in resolution, refresh rates, brightness, and tracking accuracy. As the technology matures, there will be more opportunities to enhance the personalized experiences. This system would allow multiple viewers to see different images or perspectives from the same display, enhancing the interactive and educational value of the exhibit. Mollick et al. have done some lovely actionable work on the pedagogical implications of chatbots.[@mollick2022new; @mollick2023assigning; @mollick2023using] This could transform the way visitors engage with exhibits, providing a more immersive and personalized experience.
- ##### Alioscopy
	- Alioscopy uses a different approach than lenticular lenses for their glasses-free 3D displays. Their screens contain a parallax barrier
		- a layer of opaque and transparent slits
		- over the LCD matrix. This directs different pixel columns to each eye, creating a stereoscopic 3D image without glasses.
	- Their displays also incorporate proprietary eye tracking technology. An infrared camera follows the viewer’s head position, automatically adjusting the angle of the projected 3D image for optimal viewing. This compensates for display viewing angle limitations.
	- Alioscopy’s recent prototypes feature very high resolution like 4K and 8K to improve 3D image quality. Their barriers and tracking algorithms are precisely tuned to the display characteristics and desired viewing parameters.
- ##### Pitch section
- Personalised emergent narratives for our visitors. What problem does the
  user, business or industry have that you want to solve?
  
  [[Visionflow]] : [[Knowhere]]
- For today’s digital experience venue managers navigating the
  complexities of providing unique experiences, our AI solution, KnoWhere,
  offers a unique approach which will result in the capability to enhance
  visitor experiences. By utilising images from on-premise cameras, we
  enable to leverage data on visitors attention. Our solution’s unique
  value propositions include spatial and attention tracking through AI,
  because of our ability to understand the needs of experience designers.
- It works like this: Combine personal data, with visitor gaze Provide
  location and attention data stream Venue provides this to experience
  designers Designers build incredible emergent journeys
- We believe this solution will impact our business/industry by: Elevating
  interactions through personalisation Making attention in physical spaces
  quantifiable Providing feedback data to experience designers
- We will measure our impact by: Performing A/B testing on visitors
  engagement This can be a KPI that changes, ex: a productivity score
	- or
	  it can be an amount saved because of the soluion
- Describe what data is behind this AI model? Alphapose (2) Insightface
  (5)
  
  Rate the quality/quantity of each point of data from 1-5 (1 being little
  data / low quality – 5 being lots of data / high quality)
  
  What will be the biggest challenge in implementing the AI model? Real
  time pose engine is noncommercial Occlusion can be tricky with space
  constraints The rich dataset is a privacy concern
  
  Here are some areas to think about in terms of challenges:
  
  Data: How much data exists? How representative is it of what we’re
  trying to model? Are there issues in how it is collected which could
  impact the model? Is it likely to contain any missing values? Adoptance
  from users/customers Will it be easy to get people to use the AI in
  their business?
  
  Governance Is the data accessible and are you allowed to use it? Who is
  responsible once the AI model is in use? How will make the final
  decisions?
  
  Impact of solution What do we know about the need for this type of
  solution
	- is it nice to have or need to have? Can we find out if we
	  don’t know? Feasibility What will be the biggest challenge in
	  implementing an AI solution to solve this problem? Can the issue /
	  problem we’re solving actually be measured / forecasted?
	  
	  Ethics
	  
	  Regulations
	  
	  Cybersecurity
	  
	  "Our goal is to empower venue owners to provide an advanced platform
	  that allows world class exhibition creators to tailor unique experiences
	  for each visitor. This enables the crafting of rich, interconnected
	  stories for groups of people, all while ensuring unforgettable, safe
	  experiences for individuals and families.
- ### Accessible metaverse for pre-viz
- Pre-visualization (or "pre-viz") is a process in which a rough
  simulation of a visual effect or scene is created prior to its actual
  production. In the context of LED wall virtual production, pre-viz
  refers to the creation of a 3D representation of a virtual environment,
  including the placement of cameras, actors, and other elements, that is
  then used to plan and test the visual effects and lighting for a
  live-action scene that will eventually be shot in front of an LED wall.
- The pre-viz process allows filmmakers and visual effects artists to
  experiment with different camera angles, lighting, and visual effects
  before committing to a final version. This helps to save time and
  resources during actual production by reducing the need for multiple
  takes or re-shoots. Additionally, it allows the filmmakers to see how
  the final product will look before committing to it, which can help to
  avoid costly mistakes or changes down the line.
- The LED wall virtual production process typically involves using a
  combination of 3D animation software, motion capture technology, and
  real-time rendering to create a virtual environment that accurately
  reflects the physical environment in which the scene will be shot. The
  pre-viz process is then used to plan and test the various visual
  effects, lighting, and camera angles that will be used in the final
  production.
- Our collaborative software stack is potentially ideally suited to some
  of this pre-viz work, especially when combined with the power of machine
  learning, and live linked into Unreal so that changes by stakeholders
  enter the pre-production pipeline in a seamless way.
- ### Novel VP render pipeline
	- Putting the ML image generation on the end of a real-time tracked camera render pipeline might remove the need for detail in set building. To describe how this might work, the set designer, DP, director, etc will be able to ideate in a headset based metaverse of the set design, dropping very basic chairs, windows, light sources whatever. There is -no need- then to create a scene in detail. If the interframe consistency (img2img) can deliver then the output on the VP screen can simply inherit the artistic style from the text prompts, and render production quality from the basic building blocks. Everyone in the set (or just DP/director) could then switch in headset to the final output and ideate (verbally) to create the look and feel (lens, bokeh, light, artistic style etc). This isn’t ready yet as the frames need to generate much faster (100x), but it’s very likely coming in months not years. This “next level pre-vis” is being trailed in the Vircadia collaborative environment described in this book, and can be seen illustrated in Figure <a href="#fig:vircadiasd" data-reference-type="ref" data-reference="fig:vircadiasd">10.1</a>.
	  
	  <span class="image">Top panel is a screen grab from Vircadia and the bottom panel is a quick pass through img2img from Stable Diffusion.</span>
	- This can be done now through the use of camera robots. A scene can be built in basic outline, the camera tracks can be encoded into the robot, and the scene can be rapidly post rendered by Stability with high inter frame consistency.
	- With the help of AI projects such as [LION](https://nv-tlabs.github.io/LION/) it may be possible to pass simple geometry and instructions to ML systems which can create complex textured geometry back into the scene.
	- <span class="image">Robot VP</span>
- ### Money in metaverses
	- ##### Global collaboration and remuneration
		- In the book “Ghosts of my life”[@fisher2014ghosts] Fisher asserts that
		  there has been a slowing, even a ‘cancellation’ of creative progress in
		  developed societies, their art, and their media. His contention is that
		  neoliberalism itself is to blame. He says  
		  it“It is the contention of this book that 21st-century culture is marked
		  by the same anachronism and inertia which afflicted Sapphire and Steel
		  in their final adventure. But this status has been buried, interred
		  behind a superficial frenzy of ‘newness’, of perpetual movement. The
		  ‘jumbling up of time’, the montaging of earlier eras, has ceased to be
		  worthy of comment; it is now so prevalent that it is no longer even
		  noticed.”
		- It is the feeling of the authors of this book that the creative and
		  inspirational efforts of the whole world may be needed to heal these
		  deep wounds. It is possible that by connecting creatives with very
		  different global perspectives, directly into ‘Western’ production
		  pipelines, that we will be able to see the shape of this potential.
- #### ML actors and blockchain based bots
	- Stablity AI is an open source imitative to bring ML/AL capabilities to the world. This is a hugely exciting emergent area and much more will be developed here.
- #### AI economic actors in mixed reality
	- AI actors can now be trusted visually.[@nightingale2022ai] We have some thinking on this which links the external web to our proposed metaverse. There is work in the community working on economically empowered bots which leverage Nostr and RGB to perform functions within our metaverse, and outside in the WWW, as well as interacting economically through trusted cryptography with other bots, anywhere, and human participants, anywhere. This is incredibly powerful and is assured by the Bitcoin security model. Imagine being able to interact with a bot flower seller representing all the real world florists it had found. In the metaverse you could handle the flowers and take advice and guidance from the bot agent, then it would be able to take your money to buy you flowers to send to a real world address, and later find you to tell you when it’s delivered. These possibilities are endless. The AI chat element, the AI translation of images on websites to 3D assets in the Metaverse are difficult but possible challenges, but the secure movement of money from the local context in the metaverse to the real world is within reach using these bots, and they are completely autonomous and distributed.
- ### Our socialisation best practice
	- ##### Identity
		- We will base our identity and object management on Nostr public/private
		  key pairs. The public key of these enable lightning based exchange of
		  value globally. we plan to operate Nostr in multiple modes. Linking
		  flossverse “rooms” will be a Nostr bot to bot system within the private
		  relay mesh. This can also synchronise large amounts of data by
		  leveraging torrents [negotiated by Nostr](https://iris.to/#/settings).
		  Human to human text chat across and within instances is two ’types’ kind
		  of private nostr tag within the private relays mesh. External
		  connectivity to web and nostr apps is just the public relay tags
		  outbound. We don’t need to store data external to the flossverse system,
		  though access is obviously possible through the same torrent network.
	- ##### Webs of trust
		- Webs of trust will be built within worlds using economically costly (but
		  private) social rating systems, between any actor, human or AI. It
		  should be too costly to attack an individual aggressively. This implies
		  an increased weighting for scores issued in short time periods. Poorly
		  behaving AI’s will eventually be excluded through lack of funds.
	- ##### Integration of ’good’ actor AI entities
		- Gratitude practice should be encouraged between AI actors to foster
		  trust and wellbeing in human observers. “It’s nice to be nice” should be
		  incentivised between all parties”. This could include tipping and trust
		  nudging through the social rating system. Great AI behaviour would
		  result in economically powerful entities.
	- #### Emulation of important social cues
		- [Classroom layout](https://www.cleverclassroomsdesign.co.uk/general-5)
	- ##### Behaviour incentives, arbitration, and penalties
		- Collapses of trust and abuse will trigger flags from ML based oversight,
		  which will create situational records and payloads of involved parties
		  to unlock with their nostr private keys. ML red flagged actors will be
		  finacially penalised but have access to human arbitration using their
		  copy of the data blob. Nothing will be stored except by the end users.
	- #### Federations of webs of trust and economics
		- Nostr is developing fast enough to provide global bridges between
		  metaverse instances.
- ### Security evaluation
	- As part of developing our stack we will penetration test the deployment as detailed using [Hexway](https://hexway.io/)
- ### notes for later
	- Notes on build-out The world database in the shared rooms in the metaverse is the global object master, educational materials, videos, audio content and branded objects are fungible tokens authentically proved by rgb client side validation between parties, only validated ones will be persisted in shared rooms like conferences and classes according to the room schema. That allows educators to monetise their content. That can work on lightning. NFT objects between parties like content crafted by participants (coursework, homework) are not on lightning and will attract main chain fees but are rarer. User authentication and communication will be through nostr.
	  
	  <span class="image">image</span>
	  
	  ![image](../assets/707beb139883d3b15e01fd447eb2ceb747861560.png)
-
- # NVIDIA Omniverse design
	- **Phase 1: Foundational Infrastructure**
	- **Bitcoin Base Layer (NixOS):**
		- Set up a secure and reliable Bitcoin full node on NixOS.
		- Implement robust backup and recovery procedures.
		- Consider running a Lightning Network node for faster and cheaper transactions.
	- **Identity and Value Management:**
		- Integrate Nostr protocol for decentralized identity and messaging.
		- Develop or utilize existing libraries for Nostr event creation, signing, and relaying.
		- Implement BIP85 hierarchical deterministic wallets for secure key management.
	- **Digital Assets (RGB):**
		- Choose or design appropriate RGB schemas for the types of digital assets you want to support.
		- Develop or utilize tools for issuing and managing RGB assets.
		- Integrate RGB wallets with the overall wallet management system.
		  
		  **Phase 2: Interaction Module (Omniverse):**
	- **Omniverse Environment Setup:**
		- Deploy an Omniverse Nucleus server to manage collaborative scenes and 3D assets.
		- Design and create the initial 3D environment(s) using USD (Universal Scene Description).
		- Consider incorporating elements from your existing visualizations and research.
	- **Agent Integration:**
		- Develop avatar systems for both human and AI agents within Omniverse.
		- Implement controls and interactions for agents within the 3D environment.
		- Explore the use of Omniverse Kit SDK for advanced features and customizations.
	- **Digital Asset Integration:**
		- Develop methods to represent and interact with RGB digital assets within Omniverse scenes.
		- Implement ownership and transfer functionalities based on the underlying Bitcoin/RGB infrastructure.
		- Explore visual representations of ownership and asset metadata within the 3D environment.
		  
		  **Phase 3: AI and Governance:**
	- **AI Agent Development:**
		- Choose or design AI models for different agent archetypes (e.g., governance agents, task agents, social agents).
		- Implement the D&D-inspired personality system and the wealth decay function.
		- Develop AI behaviors and decision-making processes aligned with the scene schema.
	- **Scene Schema and Governance:**
		- Define the rules and constraints for different scene types within a flexible schema framework.
		- Implement the SupraAgent (governance LLM) with its monitoring and evidence collection capabilities.
		- Develop mechanisms for encrypted evidence payloads and communication with relevant parties.
	- **GenAI Integration:**
		- Explore the use of generative AI models (e.g., ChatGPT, Stable Diffusion) for content creation, world-building, and immersive storytelling.
		- Develop interfaces for users and AI agents to interact with GenAI tools within the metaverse.
		  
		  **Phase 4: User Interface and Experience:**
	- **Nostr-based Chat Interface:**
		- Develop a chat interface using Nostr as the communication protocol.
		- Integrate the chat interface within the Omniverse environment.
		- Enable secure and private communication between agents.
	- **Wallet Integration:**
		- Provide users with access to their digital wallets within the metaverse.
		- Enable users to manage their assets, view transaction history, and interact with the virtual economy.
	- **Accessibility and Multimodality:**
		- Explore ways to make the metaverse experience accessible to users with disabilities.
		- Support multiple interaction modalities (e.g., VR, AR, desktop, mobile).
		  
		  **Additional Considerations:**
	- **Security:** Implement robust security measures at all levels, including encryption, access control, and regular security audits.
	- **Privacy:** Ensure user privacy by minimizing data collection and providing transparent privacy settings.
	- **Scalability:** Design the system to be scalable to accommodate a growing number of users and increasing complexity.
	- **Community Building:** Foster a strong community around your metaverse project through open communication, collaboration, and user engagement initiatives.
- # According to Gemini
	- **Phase 1: Foundational Infrastructure**
		- **Bitcoin Base Layer (NixOS):**
		- **Set up a secure and reliable Bitcoin full node on NixOS.**
		- **Implement robust backup and recovery procedures.**
		- **Consider running a Lightning Network node for faster and cheaper transactions.**
		- **Identity and Value Management:**
		- **Integrate Nostr protocol for decentralized identity and messaging.**
		- **Develop or utilize existing libraries for Nostr event creation, signing, and relaying.**
		- **Implement BIP85 hierarchical deterministic wallets for secure key management.**
		- **Digital Assets (RGB):**
		- **Choose or design appropriate RGB schemas for the types of digital assets you want to support.**
		- **Develop or utilize tools for issuing and managing RGB assets.**
		- **Integrate RGB wallets with the overall wallet management system.**
		   
		   **Phase 2: Interaction Module (Omniverse):**
		- **Omniverse Environment Setup:**
		- **Deploy an Omniverse Nucleus server to manage collaborative scenes and 3D assets.**
		- **Design and create the initial 3D environment(s) using USD (Universal Scene Description).**
		- **Consider incorporating elements from your existing visualizations and research.**
		- **Agent Integration:**
		- **Develop avatar systems for both human and AI agents within Omniverse.**
		- **Implement controls and interactions for agents within the 3D environment.**
		- **Explore the use of Omniverse Kit SDK for advanced features and customizations.**
		- **Digital Asset Integration:**
		- **Develop methods to represent and interact with RGB digital assets within Omniverse scenes.**
		- **Implement ownership and transfer functionalities based on the underlying Bitcoin/RGB infrastructure.**
		- **Explore visual representations of ownership and asset metadata within the 3D environment.**
		   
		   **Phase 3: AI and Governance:**
		- **AI Agent Development:**
		- **Choose or design AI models for different agent archetypes (e.g., governance agents, task agents, social agents).**
		- **Implement the D&D-inspired personality system and the wealth decay function.**
		- **Develop AI behaviors and decision-making processes aligned with the scene schema.**
		- **Scene Schema and Governance:**
		- **Define the rules and constraints for different scene types within a flexible schema framework.**
		- **Implement the SupraAgent (governance LLM) with its monitoring and evidence collection capabilities.**
		- **Develop mechanisms for encrypted evidence payloads and communication with relevant parties.**
		- **GenAI Integration:**
		- **Explore the use of generative AI models (e.g., ChatGPT, Stable Diffusion) for content creation, world-building, and immersive storytelling.**
		- **Develop interfaces for users and AI agents to interact with GenAI tools within the metaverse.**
		   
		   **Phase 4: User Interface and Experience:**
		- **Nostr-based Chat Interface:**
		- **Develop a chat interface using Nostr as the communication protocol.**
		- **Integrate the chat interface within the Omniverse environment.**
		- **Enable secure and private communication between agents.**
		- **Wallet Integration:**
		- **Provide users with access to their digital wallets within the metaverse.**
		- **Enable users to manage their assets, view transaction history, and interact with the virtual economy.**
		- **Accessibility and Multimodality:**
		- **Explore ways to make the metaverse experience accessible to users with disabilities.**
		- **Support multiple interaction modalities (e.g., VR, AR, desktop, mobile).**
		   
		   **Additional Considerations:**
		- **Security:** Implement robust security measures at all levels, including encryption, access control, and regular security audits.
		- **Privacy:** Ensure user privacy by minimizing data collection and providing transparent privacy settings.
		- **Scalability:** Design the system to be scalable to accommodate a growing number of users and increasing complexity.
		- **Community Building:** Foster a strong community around your metaverse project through open communication, collaboration, and user engagement initiatives.
		  
		  **This development plan provides a roadmap for implementing your metaverse vision, step by step. By focusing on the core principles of your research and leveraging innovative technologies like Bitcoin, RGB, Nostr, and Omniverse, you are building a foundation for a truly unique and transformative metaverse experience.**