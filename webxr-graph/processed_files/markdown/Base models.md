public:: true

- #Public page
	 - automatically published
- # Base models
- -- [Practical guide github and paper with branching diagram](https://github.com/Mooler0410/LLMsPracticalGuide)
- Alpaca
          Launch post with links
           <https://crfm.stanford.edu/2023/03/13/alpaca.html>
          Anonymous alpaca gpt train
           <https://github.com/oobabooga/text-generation-webui/discussions/727>
- Llama
          Guide that worked for gradio
           <https://aituts.com/llama/>
          llama download from git
           <https://github.com/shawwn/llama-dl>
              running in venv
               <https://www.reddit.com/r/MachineLearning/comments/11kwdu9/d_tutorial_run_llama_on_8gb_vram_on_windows/>
              Oogabooga LLM github gradio
               <https://github.com/oobabooga/text-generation-webui>
                  deepspeed notes
                   <https://github.com/oobabooga/text-generation-webui/issues/40#issuecomment-1412038622>
                  whisper voice to prompt
                   <https://github.com/oobabooga/text-generation-webui/tree/main/extensions/whisper_stt>
                  text to speech
                   <https://github.com/oobabooga/text-generation-webui/blob/main/extensions/silero_tts/script.py>
                  erebus
                  How to install
                      patch
                      ensure python 3.10.9, use proper venv
                      cudnn as well as the drivers and normal reqs
                      install plus all the requirements from the extensions
                      make a startup script
                  Rivet ooga plugin
                   <https://github.com/hushaudio/rivet-oobabooga-plugin>
                      Rivet repo
                       <https://github.com/Ironclad/rivet>
              int8 guide
               <https://rentry.org/llama-tard-v2>
              https://www.reddit.com/r/LocalLLaMA/comments/1227uj5/my_experience_with_alpacacpp/
              CocktailPeanut Implementation
               <https://github.com/cocktailpeanut/dalai>
  -- Dalai llama
- git clone the dalai from you desired github (i like the one from @cocktailpeanut  and the @#1 Dalai support (.pi))
- go to the dalai folder and use
- npm install
- return your models for the dalai path models
- after that
- npx dalai@latest alpaca install your_model_version
- after that
- npx dalai@latest serve
- npx dalai@latest serve
  -- llama tard v2
- <https://rentry.org/llama-tard-v2>
- Discord llama based chatbot
- <https://github.com/ortegaalfredo/celery-ai/blob/main/discord/bot.py>
- FastChat based on llama 13b
- <https://github.com/lm-sys/FastChat>
- 4bit 30B linux integration
- <https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model>
- persistence
- <https://github.com/facebookresearch/llama/issues/162>
  -- llama-30b-4bit-huggingface
- <https://huggingface.co/Neko-Institute-of-Science/LLaMA-30B-4bit-128g/tree/main>
- Llama CPP windows open ticket
- <https://github.com/ggerganov/llama.cpp/issues/22>
- article on the implications
- <https://arstechnica.com/information-technology/2023/03/you-can-now-run-a-gpt-3-level-ai-model-on-your-laptop-phone-and-raspberry-pi/>
- Raven LLM RNN hybrid
       <https://github.com/BlinkDL/RWKV-LM>
          raven model on huggingface
           <https://huggingface.co/BlinkDL/rwkv-4-raven>
          cpp implementation
           <https://github.com/harrisonvanderbyl/rwkv-cpp-cuda>
- Dolly open source
       <https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm>
          databricks repo
           <https://github.com/databrickslabs/dolly/tree/master/data>
      collosal AI Open source GPT attempt
       <https://github.com/hpcaitech/ColossalAI>
- GPT-NeoXT-Chat-Base-20B human optimised free model
       <https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B>
- Nerybus blend
       <https://huggingface.co/notstoic/OPT-13B-Nerybus-Mix-4bit-128g>
- OpenLlama
       <https://github.com/openlm-research/open_llama>
- Fastchat
       <https://huggingface.co/lmsys/fastchat-t5-3b-v1.0>
- Vircuna
          Vircuna 7B
           <https://github.com/lm-sys/FastChat#fine-tuning-vicuna-7b-with-local-gpus>
          WizardVircua retrain
           <https://www.reddit.com/r/LocalLLaMA/comments/1376oho/introducing_wizardvicunalm_combining_wizardlm_and/>
          VircunaFree
           <https://huggingface.co/reeducator/vicuna-13b-free>
- How to scale LLM workloads to 20B+ with Amazon SageMaker using Hugging Face and PyTorch FSDP
       <https://www.philschmid.de/sagemaker-fsdp-gpt>
- Red Pajama
       <https://www.together.xyz/blog/redpajama-models-v1>
- Mozilla LLM (apache)
       <https://www.mosaicml.com/blog/mpt-7b>
- OpenLlama weights
       <https://huggingface.co/openlm-research>
      Unlimited input
       <https://github.com/abertsch72/unlimiformer>
      This repository contains Stability AI's development of the StableLM series of language models. The models are designed to be more stable and robust than traditional language models, and the repository includes code and examples for training and using the models.
       <https://github.com/stability-AI/stableLM/>
      Wizard Vicuna
       <https://github.com/nlpxucan/WizardLM>
      Lit Llama license free retune of Llama, discord
       <https://discord.com/channels/1077906959069626439/1090710167181594766>
      Manticore logic trained 13B
       <https://huggingface.co/openaccess-ai-collective/manticore-13b>
      Alpasta 30B 4 bit
       <https://huggingface.co/askmyteapot/GPT4-X-Alpasta-30b-4bit>
      Wizard 30B unaligned
       <https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ>
      Falcon 40B (non commercial ish)
       <https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE.txt>
          how to prompt
           <https://github.com/cmp-nct/ggllm.cpp/discussions/36>
      Based 30B
       <https://huggingface.co/ehartford/based-30b>
      orca from microsoft
       <https://arxiv.org/pdf/2306.02707.pdf>
      *  Text2NeRF is a text-driven 3D scene generation framework. It combines the neural radiance field (NeRF) and a pre-trained text-to-image diffusion model to generate diverse view-consistent indoor and outdoor 3D scenes from natural language descriptions.
      *  Text2NeRF uses NeRF for the 3D representation and leverages a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description.
      *  The page is about a hard surface character model named Lora, which is a mecha or humanoid character. The model is version 1.0, was updated on May 22, 2023, and has over 1,000 downloads. It includes a download link for the model file which is about 144 MB. The creator recommends setting Lora's values from 0.25 to 0.75 for better results, and suggests using keywords like "helmet", "machine gun", "science fiction", "robot", "mecha", "humanoid", and "1girl"​1​.
       <https://civitai.com/models/73470?modelVersionId=78187>
      *  This page provides a complete guide to ControlNet v1.1, a neural network model for controlling Stable Diffusion models. It offers users a way to control image compositions or human poses from a reference image with precision, and can be used alongside any Stable Diffusion models. ControlNet adds an extra conditioning to text prompts in addition to the original text-to-image functionality of Stable Diffusion models. The guide includes detailed examples on how ControlNet uses edge detection and human pose detection as extra conditionings. The page also provides instructions on how to install ControlNet on Google Colab, Windows PC, and Mac using AUTOMATIC1111, a popular GUI for Stable Diffusion​2​.
       <https://stable-diffusion-art.com/controlnet/>
      Can we improve the efficiency of these training methods, so we can still get good models in less time and for less money? We propose to do this by leveraging smaller language models that have previously been trained,
       <https://gemm.ai/learning-to-grow-machine-learning-models/>
- The text describes the Camel Chatbot, a machine learning model that has been trained using data from the AI Society and Code datasets. The chatbot is designed to improve the coding ability of users. The text includes a link to a demo of the chatbot in action.
       <https://www.linkedin.com/posts/guohao-li-9a573b136_camel-chatbot-demo-activity-7051390760327225344-8D2A?utm_source=share&utm_medium=member_android>
      The article discusses Reddit's use of cookies and technology to enhance user experience and explains that by accepting all cookies, users agree to improve the quality of Reddit, personalize content and advertising, and measure the effectiveness of advertising. Rejecting non-essential cookies will still allow Reddit to use certain cookies for proper platform functionality. The article also includes a post from a user promoting their expert-level tutorials on Stable Diffusion, which covers advanced AI techniques and strategies. Some comments on the post discuss various topics related to stable diffusion, such as suggestions for training architectural models and the use of C# programming language. Many other posts on various Reddit pages discuss Stable Diffusion and its accompanying tools and tutorials. https://www.reddit.com/r/StableDiffusion/comments/132rcou/30_stable_diffusion_tutorials_automatic1111_web/
      Unfortunately, the provided text is not sufficient for me to provide a meaningful summary as it seems to be an error message related to website security. Can you please provide more context or a different text to summarize? https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/
      Scan the World is a digital museum that aims to archive sculptures, artifacts and statues from across the world using 3D printing and scanning technologies. The community-built museum allows the scans to be free to download, making art and culture more accessible for everyone. Scan the World uses photogrammetry to capture the objects and offers them for the purpose of education, preservation, cultural heritage and accessibility. The range of scans available on the site is comprehensive, from buildings to digital archaeology and downloadable monuments. There are over 20 categories available on the website, ranging from Africa, Europe, Asia to Oceania, North America, and South America. The scans on the site include; David by Scan The World, Head of Michelangelo's David by SMK
	 - Statens Museum..., Bust of Nefertiti at the Neues Museum, Berlin by Scan The World, Pieta in St. Peter's Basilica, Vatican by Scan The World, The Thinker at the Musée Rodin, France by Musée Rodin, and many more. Scan the World is an initiative of MyMiniFactory and the community that uses the platform to create 3D printable models. It is a unique platform that showcases art, culture, and history to the world through digital means. https://www.myminifactory.com/category/scan-the-world
      The Wizard-Vicuna-13B-Uncensored-HF is a model used for text generation, which is available on the Hugging Face platform. It has been trained on a subset of data to remove alignment/moralizing and further trained to provide an uncensored model without guardrails, meaning it generates content without any limitations. The model can be accessed on-demand via the Inference API and is available in the float16 HF format for GPU inference and further conversions. Users can also contribute to the model through Eric Hartford's Patreon page or Ko-Fi. However, the creators mention that the users are fully responsible for the content generated by the model and must not blame the model for any negative consequences. https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-HF
      ChatHub is a chatbot client that supports multiple chatbots, including ChatGPT, Bing Chat, Google Bard, Claude, and 10+ open-source models. Users can chat with multiple chatbots at the same time and compare their answers. ChatHub supports ChatGPT API and GPT-4 Browsing and has a shortcut to quickly activate the app anywhere in the browser. Other features include markdown and code highlight support, a prompt library for custom and community prompts, conversation history saved locally, export and import of data, and a dark mode. ChatHub can be installed manually by downloading the zip file from Releases and following the instructions or built from source by cloning the source code, running yarn install and yarn build, and loading the dist folder to the browser. ChatHub has a premium license that enables support for more bots in all-in-one mode and users can add their own custom theme setting. The app also has a changelog that documents the addition of new models, APIs, and other features supported by the app. Overall, ChatHub is a versatile tool for anyone who wants to interact with multiple chatbots at the same time and compare their performance. https://github.com/chathub-dev/chathub/blob/main/README.md
      A Twitter user, Ouriel Ohayon, has accused ChatGPT, an AI language model, of accessing and using internal emails to provide accurate information about Ohayon's business model when prompted with a seemingly ambiguous question. ChatGPT is known for generating text based on given prompts and has gained popularity for its ability to closely mimic human language. However, Ohayon's accusation has raised concerns about privacy and data access by AI language models. Some Twitter users have suggested that the information may have been available elsewhere, while others have proposed the possibility of an employee leaking the information. ChatGPT has not yet responded to the allegation. https://twitter.com/OurielOhayon/status/1650369984080629760
      Hugging Face, an AI startup known for its ML tools, has launched HuggingChat, an open-source chatbot alternative to OpenAI's ChatGPT. HuggingChat allows users to generate text in natural language or in a specific format and can handle various tasks like drafting emails, writing rap lyrics, and coding, including syntax highlighting. The chatbot's interface is similar to ChatGPT, with a left bar showing the latest chats and a large browser window for the current chat, and it's responsive. Hugging Face's release of HuggingChat emphasizes their dedication to open-source AI, and the company is also working to refine the model's limitations. In the future, Hugging Face plans to make all quality chat models available through a single hub to revolutionize the AI landscape. HuggingChat challenges the status quo and democratizes the AI chatbot industry, making it a game-changer. https://medium.com/codingthesmartway-com-blog/my-take-on-huggingchat-the-open-source-chatbot-alternative-to-chatgpt-thats-shaking-things-up-589a101bccde
      Pinecone, a provider of a vector database for AI applications, has raised $100m in a series B funding round led by Andreessen Horowitz, with ICONIQ Growth, Menlo Ventures and Wing Venture Capital also participating. The funds bring Pinecone's valuation to $750m. Pinecone's database is used to generate accurate AI products by making it easier for engineers to work with data produced by large language models and other AI. Pinecone claims to have experienced explosive growth in the number of AI developers and users on its free plan, while customer numbers also rose. With the new capital Pinecone plans to continue expanding its team and to roll out more features on its platform. https://www.pinecone.io/learn/series-b/
      The text mentions Reddit's use of cookies and similar technologies to improve user experience, and provides the option to accept or reject non-essential cookies. It also includes comments and posts from the subreddit r/LocalLLaMA, which discusses various topics related to language models, coding, and technology, including the release of new models and tools, benchmark results, and tutorials. Some of the comments and posts contain mature or NSFW content, and users are prompted to confirm their age before viewing such content. https://www.reddit.com/r/LocalLLaMA/comments/12kh2la/nsfw_chatting_promts_for_vicuna_11/
      Sorry, I am unable to summarize the provided text as it is not a complete article or piece of information, rather just a message regarding the security of a website. https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/
      PandaLM is an open-source tool that provides reproducible and automated comparisons between different large language models (LLMs). The tool can be used to evaluate the evaluation ability of different LLMs and provide a reason for the decision, along with a reference answer. PandaLM can be used by organizations that have confidential data and research labs with limited funds. With PandaLM, they can perform evaluations without compromising data security or incurring high costs and obtain reproducible results. The tool is reliable and consistent and comes with a diverse human-annotated test dataset of approximately 1,000 samples. The repository contains the codes for training PandaLM, the human-annotated test dataset, the model weights of PandaLM, and the codes and configs for instruction tuning other foundation models such as Bloom, OPT, and LLaMA. PandaLM is easy to install, and the tool offers several choices for experiencing PandaLM, such as using a Web UI or the EvaluationPipeline class. PandaLM can be used to evaluate different responses for a given context and generate a reference response utilizing the given context, making it efficient and consistent. https://github.com/WeOpenML/PandaLM
      The US Supreme Court is set to rule on a case that could affect the interpretation of fair use law and the protection afforded to generative artificial intelligence technology. The case, Andy Warhol Foundation for the Visual Arts, Inc. v. Goldsmith, hinges on whether a series of images Warhol created of Prince were distinct enough from the photograph he used for reference to be considered transformed, under the fair use doctrine of the Copyright Act. The decision could have implications both for copyright law, which is currently used by tools such as image and language models to train themselves, and wider international trade agreements that rely on shared recognition between nations of such laws. The potential effects of the ruling have led to legal scholars and industry figures expressing concern that AI could be granted copyright and alter trade policy or even industries such as drug formulations. https://www.wired.co.uk/article/andy-warhol-fair-use-prince-generative-ai
      The Mr. Ranedeer AI Tutor is a customizable, personalized learning experience powered by GPT-4. It allows users to adjust the depth of knowledge, customize learning styles, communication types, tone, and reasoning frameworks to create the ultimate AI tutor. The Mr. Ranedeer AI Tutor works best with the ChatGPT Plus Subscription with GPT-4 or above models and is not recommended for use with GPT-3.5. The AI Tutor supports different languages, including Chinese, and has a variety of commands, including /test, /config, /plan, /start, /continue, and /language. The Mr. Ranedeer AI Tutor also offers Ranedeer Tools, which are optional prompts that allow users to create flexible environments, add personality, and more to the learning experience. The AI Tutor comes with a guide on how to use it, as well as configuration and Ranedeer Tools guides, and users can also access lessons on various topics, including poetry analysis and programming in Python. Mr. Ranedeer AI Tutor is available on GitHub and can be used by individuals, including AI models looking for information about the tutor. https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor
      The TEXTure method is a novel approach for text-guided generation, editing, and transfer of textures for 3D shapes. It uses a pretrained depth-to-image diffusion model to iteratively paint a 3D model from different viewpoints, while dynamically defining a trimap partitioning of the rendered image into three progression states to generate seamless textures from different views. This method can transfer generated texture maps to new 3D geometries without explicit surface-to-surface mapping and extract semantic textures from a set of images without reconstruction. Furthermore, it can edit and refine existing textures using either a text prompt or user-provided scribbles. The paper shows extensive evaluation of the method's ability to generate, transfer, and edit textures, bridging the gap between 2D image generation and 3D texturing. https://texturepaper.github.io/TEXTurePaper/
      Open_LLaMA is an open-source reproduction of Meta AI's LLaMA large language model. The project provides PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The weights are released in two formats – an EasyLM format for the EasyLM framework and a PyTorch format for the Hugging Face Transformers library. OpenLLaMA models have been trained on the RedPajama dataset released by Together that contains over 1.2 trillion tokens and exhibit comparable performance to the original LLaMA and EleutherAI's GPT-J models across a majority of tasks. OpenLLaMA is licensed permissively under the Apache 2.0 license. Weights can be directly loaded from the Hugging Face Hub. https://github.com/openlm-research/open_llama
      OpenAI has officially released Shap-E, a system that generates 3D objects by conditioning them on images or text prompts. Shap-E has two models, an encoder that converts 3D assets into the parameters for small neural networks that represent the 3D shape and texture as an implicit function, and a latent diffusion model that generates novel implicit functions conditioned on either the image or text descriptions. The new model includes rendering based on 60 views of each model, which is an improvement from the 20 views Point-E used, and the final output produces 16K points in each point cloud instead of 4K points. The lighting and material setup includes only diffuse materials, and the datasets are expanded with a million more 3D assets and 120K captions from human annotators. Shap-E generates 3D objects with lower fidelity than professional 3D assets and is geared towards cartoonish assets. Shap-E is available on Github, and the setup.py file in the official repository is incomplete and lacks several Python packages, which needs to be installed manually via pip install. https://ngwaifoong92.medium.com/introduction-to-shap-e-text-to-3d-a4fb5304642b
      The authors present "Shap-E", a conditional generative model for 3D assets. Unlike other generative models that produce a single output representation, Shap-E generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. The authors trained Shap-E in two stages; first, an encoder that maps 3D assets to the parameters of an implicit function, and second, a diffusion model on the encoder outputs. This method produces diverse, complex 3D assets in a few seconds. Shap-E converges faster and reaches comparable or better sample quality than the explicit generative model Point-E, despite modeling a higher-dimensional, multi-representation output space. The authors have released model weights, inference code, and samples. https://arxiv.org/abs/2305.02463
      The co-founder of Hugging Face, Thomas Wolf, has released StarCoder, a 15-billion model trained purely on carefully vetted data ranging only over permissive codes, thereby ensuring high performance. The model can be used for internal finetuning or as the foundation for coding completion. Even though it can act as a chat model, Wolf warns that it should not be used like ChatGPT. While users were skeptical of StarCoders over Twitter and LinkedIn, Wolf confirmed the diversified compatibility of StarCoders, with the intention of making the model available to users worldwide.  https://www.linkedin.com/posts/thom-wolf_so-this-week-weve-finally-released-starcoder-activity-7060906603092271104-rrCx?utm_source=share&utm_medium=member_a
       <https://www.linkedin.com/posts/thom-wolf_so-this-week-weve-finally-released-starcoder-activity-7060906603092271104-rrCx?utm_source=share&amp;utm_medium=member_a>
  -- The StarCoder models are 15.5B parameter models trained on more than 80 programming languages using Multi Query Attention and a context window of 8192 tokens, using the Fill-in-the-Middle objective on 1 trillion tokens. The model was trained on GitHub code, so it can be turned into a technical assistant by using the Tech Assistant prompt. However, the generated code is not guaranteed to work as intended, and it may contain bugs or exploits. The pretraining dataset used to train the model was filtered for permissive licenses only, but the code's license generated from the model should be respected and may require attribution and/or other specific requirements. The model has been trained on source code from many programming languages, and it is capable of generating code snippets provided some context but with limitations. The model architecture used is GPT-2 with multi-query attention and Fill-in-the-Middle objective, and it was trained using the Megatron-LM orchestration and PyTorch neural networks. https://huggingface.co/bigcode/starcoder
- I apologize, but as an AI language model, I do not have access to any specific text. Please provide me with the text that you would like me to summarize and I will be happy to do so. https://onosendai.tech/
- NeRFLiX is a technology that enhances the quality of NeRF (Neural Radiance Fields) based approaches by significantly reducing rendering artifacts such as noise and blur. The NeRFLiX framework comprises a NeRF-style degradation simulator (NDS) and an inter-viewpoint mixer (IVM). NDS simulates NeRF-style degradations creating large-scale paired data for training deep neural networks to improve the quality of NeRF rendered images. Meanwhile, IVM progressively aligns image contents at the pixel and patch levels to maximize efficiency and improve performance, enhancing the quality of NeRF-rendered images and accelerating training rates. NeRFLiX has demonstrated a 50% reduction in training time while still producing even better results, elevating the performance of cutting-edge NeRF models to entirely new levels, producing highly photorealistic synthetic views. https://neuralradiancefields.io/nerflix-increased-nerf-quality-and-floater-removal/
- The Facebook AI Research team has introduced ImageBind, a unified model that can analyze and process cross-modal data of six distinct modalities: audio, depth, IMU, thermal, text, and images. The model builds a joint space that can automatically correlate and align this multimodal data, allowing it to be utilized in emergent or augmented ways. It can enable tasks such as cross-model retrieval, composing modalities with arithmetic, cross-model detection, and generation. Additionally, it exhibits strong zero-shot classification Performance. The research will be presented at the 2023 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). The code and pretrained models are freely available on GitHub under a non-commercial Creative Commons license. https://github.com/facebookresearch/ImageBind
- Chat-UI is a web-based application developed by Hugging Face that provides a chat interface using open-source models, such as OpenAssistant. It is a SvelteKit app that powers the HuggingChat app on hf.co/chat. Users need a Hugging Face access token to run Chat-UI locally, using the remote inference endpoint. Chat history is stored in a MongoDB instance. Users can enable the web search by adding either SERPER_API_KEY or SERPAPI_KEY to their .env.local file. Chat-UI can be customized by updating the MODELS variable in the .env.local file, which can either customize the parameters passed to the model or use a new model. Users can run their own models locally by having a look at the endpoint project, text-generation-inference. Chat-UI can also run models hosted on multiple custom endpoints. To deploy the app, the user needs to install an adapter for their target environment. https://github.com/huggingface/chat-ui
- The Wizard-Vicuna-13B-Uncensored-HF is a PyTorch based text generation model that has been converted from float32 to float16 for easier storage and use. It was created by Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B. The model comes in 4bit GPTQ models for GPU inference, 4bit, and 5bit GGML models for CPU inference, and float16 HF format model for GPU inference and further conversions. The model has been trained on a subset of the dataset with responses that did not contain alignment/moralizing to enable creation of a WizardLM without built-in alignment. Users should note that this uncensored model has no guardrails, and they are solely responsible for any content generated using the model. There are options for contributions to the model on the Patreon and Ko-Fi platforms, with benefits such as priority support and access to a private Discord room. https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-HF
- ChatHub is a chatbot client that allows users to use multiple chatbots within one app. It currently supports ChatGPT, new Bing Chat, Google Bard, Claude, and 10+ open-source models including Alpaca, Vicuna, and ChatGLM. It allows users to chat with multiple chatbots at the same time, making it easy to compare their answers, and supports ChatGPT API and GPT-4 Browsing. Other features include shortcut to quickly activate the app anywhere in the browser, markdown and code highlight support, prompt library for custom prompts and community prompts, conversation history saved locally, export and import of data, share conversation to markdown, and dark mode. ChatHub can be manually installed by downloading chathub.zip from Releases or built from source by cloning the source code, installing yarn, and loading the dist folder to browser by following steps in manual installation. The latest version (v1.22.0) supports Claude API. https://github.com/chathub-dev/chathub/blob/main/README.md
- The post on r/StableDiffusion discusses an upcoming AI model capable of generating realistic images from photorealism to anime. The model is fine-tuned on SD 2.1 768X and can create images with resolutions ranging from 1024 to 1080p. The author of the post mentions that the model is meant to serve as a base model for future fine-tuning projects. They also invite others to leave prompts and test the model. The comments section contains discussions on the capabilities of the model and the general use of AI in commercial work. There are also mentions of related topics such as color grading and rendering techniques. Other posts on r/StableDiffusion cover various topics related to using Stable Diffusion, including updates, cheat sheets, and new extensions. https://www.reddit.com/r/StableDiffusion/comments/13j78fo/some_examples_of_the_generalist_model_i_will_be/
- Cohere, a language model provider, has raised $270m in its latest funding round to scale up its operations in the bring-your-own-language model enterprise niche. The company's offer spans the range of embedded models that can be used for interactive chat features, semantic search, classification, and re-ranking, and generate text or text summaries of products, blog posts, and articles quickly and accurately. Cohere's Command model is one of the highest-performing models for generating text as measured by Stanford University's HELM benchmarks. Cohere is investing in improving model performance for industry-specific use cases, and updates its models every week. Cohere offers secure deployment options, customizable models, and industry-leading support, with live support responding to customers in less than a minute. https://cohere.com/
- This GitHub repository, called "privateGPT," offers a fully private solution for question answering using local language models and vector embeddings. The code allows individuals to interact privately with their documents, using the power of GPT, without any data leaving their local environment. The solution is not production-ready, but it allows users to ingest and query documents without an internet connection, using LangChain, GPT4All, LlamaCpp, Chroma, and SentenceTransformers. The "environment setup" section of the repository's README.md file provides instructions for installing the required dependencies, downloading the LLM model, setting up the environmental variables, and ingesting one's own dataset. Once the data has been ingested, privateGPT.py can be used to ask questions based on the ingested documents. The script requires the user to input a query, after which the LLM model will consume the prompt and prepare an answer, which will be printed along with the four sources used as context from the documents. The repository includes a "CLI" section, which describes how users can modify privacy concerns and input parameters to modify the behavior of the script. https://github.com/imartinez/privateGPT
- Text2NeRF is a novel text-driven 3D scene generation framework that uses Neural Radiance Fields (NeRF) and a pre-trained text-to-image diffusion model to produce various view-consistent outdoor and indoor 3D scenes from natural language descriptions. While existing text-to-3D generation methods are limited to simple geometries and dreamlike styles, Text2NeRF generates a wide range of 3D scenes with complicated geometric structures and high-fidelity textures. The method relies on NeRF as the 3D representation and utilizes a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF. The model also employs a monocular depth estimation method to provide a geometric prior that updates the NeRF model. The approach introduces a progressive scene inpainting and updating strategy for novel view synthesis of the scene to guarantee textured and geometric consistency between different views. The method requires only a natural language description of the scene as the input and outperforms existing methods in generating multi-view consistent, diverse, and photo-realistic 3D scenes. The authors have shared their code and model for this research work. https://eckertzhang.github.io/Text2NeRF.github.io
- This is a post about a hard surface character with a focus on mecha and humanoid characters. The LORA model is available for download and has been tested with rev animated for optimal results. Key trigger words include helmet, machine gun, robot, science fiction, and terminator killers. There are only a few reviews at this time, but the ones available are positive. The post is from Civitai and includes links to their GitHub, Discord, Twitter, Reddit, and API. There is also information about supporting Civitai and joining their team. https://civitai.com/models/73470?modelVersionId=78187
- PanML is an open-source AI/ML development and analysis library that provides simple-to-use abstractions for facilitating the development of language-based models (LLMs) in Natural Language Processing (NLP) tasks. The library supports open source and commercial LLMs from various vendors and enables users to explore, experiment, and integrate different-sized LLMs. PanML supports inference and analysis of LLM, Prompt chain engineering with LLM, fine tuning of LLM (also with PEFT LoRA), and document question answering using LLM, among other functions. The library is a work in progress and open for collaboration and contribution. Python 3.7+ is required for installation, and the library supports models from HuggingFace Hub and OpenAI. The library is intended for ease of use, fast experimentation, and model development and analysis in both open-source and commercial LLMs. The library is under the MIT open source license, and the documentation and installation guidelines for the library are available on GitHub. https://github.com/Pan-ML/panml
- Intel and Microsoft are collaborating to bring integrated artificial intelligence (AI) to personal computing (PC). The partnership aims to enable new, AI-powered features for PC users, including multimedia features and machine learning, supported by Intel's upcoming Meteor Lake client PC processors. These processors are the first PC platform from Intel featuring a built-in neural VPU – a dedicated AI engine integrated directly into the system-on-chip (SoC) to efficiently run AI models. Intel claims that Meteor Lake, which utilises a unique disaggregated architecture, will mark a significant milestone not just in personal computing but also in how humans interact with technology. Over the coming years, Intel and its partners in the PC industry aim to provide AI-enabled experiences to millions of people, driving unprecedented change.
- The post on r/MachineLearning discusses the poor performance of open-source LLMs (language model systems) with less than 13B parameters on zero-shot classification tasks. The models produced non-sensical text and failed to follow instructions, except for OpenAI models that provided consistently good results. The possible reasons for this gap are discussed, including the use of non-instruction tuned models and the lack of proper instructions formatting. The comments suggest tools and resources for better performance, such as using instruction tuned models and configuring tools properly. The post highlights the importance of rigorously measured performance and benchmarking in machine learning and the potential limitations of relying solely on model size as an indicator of quality. https://www.reddit.com/r/MachineLearning/comments/13fiw7r/opensource_llms_cherrypicking_d/
- The Technology Innovation Institute (TII) has developed Falcon 40B, an open-source large language model (LLM) with 40 billion parameters trained on one trillion tokens. Falcon 40B is a causal decoder-only model trained on a causal language modeling task, with architecture optimized for inference with FlashAttention and multiquery. It has been trained on RefinedWeb enhanced with curated corpora and is available under the Apache 2.0 license. TII is calling for proposals from users worldwide to submit their creative ideas for Falcon 40B’s deployment. Users can use Falcon 40B for research on large language models or as the foundation for specialization and finetuning for specific use cases, such as summarization, text generation, and chatbot. Falcon-40B has been trained mostly on English, German, Spanish, and French with limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, and Czech. The model carries the biases commonly encountered online and will require finetuning for most use cases. Falcon-40B requires PyTorch 2.0 for use with transformers and needs at least 85-100GB of memory to run inference swiftly. TII recommends users of Falcon-40B to consider finetuning it for tasks of interest and for guardrails and appropriate precautions to be taken for any production use. https://huggingface.co/tiiuae/falcon-40b
- The post on Hacker News asks for links to research papers in the fields of artificial intelligence, machine learning, and deep learning, which are easy to read and have a "sneak peek" section before the introduction. One user recommends the paper on support vector machines by Cortes & Vapnik, describing it as succinctly outlining 60 years of pattern recognition from 1936 to 1992. Another user suggests "A Mathematical Theory of Communication" by Claude Shannon, which discusses compression and reliable communication, but may still be relevant to those interested in AI and ML. Other recommended papers include "Attention is all you need" by Vaswani et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" by Sohl-Dickstein et al. on diffusion models, and "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis" by Mildenhall et al. https://news.ycombinator.com/item?id=36092156
- Apple has blocked internal use of OpenAI’s language model, ChatGPT and other language models due to concerns that the AI could spill sensitive internal information shared with it. Apple has reportedly also blocked GitHub’s automated coding tool, Copilot. The issue with these language models is that data fed into them is typically used to train them, which can lead to business information being exposed or the possible exposure by bot providers themselves that review the feeds. Many companies have followed suit with this block. https://www.linkedin.com/posts/melchiors_ai-software-microsoft-activity-7069901328981848064-1-Oq?utm_source=share&utm_medium=member_android
- <https://www.linkedin.com/posts/melchiors_ai-software-microsoft-activity-7069901328981848064-1-Oq?utm_source=share&amp;utm_medium=member_android>
- The article discusses the challenges with making large language models (LLMs) more predictable and controllable within business applications, and explores the concept of "prompt engineering" as a solution. Prompt engineering involves designing prompts beforehand and using them to steer and control the LLM response. The article includes a code walkthrough in Python using OpenAI's GPT3.5 and an open-source Python library called PanML, demonstrating how to modify prompts for output filtering, LLM-assisted output filtering, and creating customized prompt sequences. The article concludes by introducing PanML as a useful tool to help data scientists and machine learning engineers experiment with LLMs in their local environment. https://medium.com/@williamzheng_63722/steering-llms-with-prompt-engineering-dbaf77b4c7a1
- The article presents information on how to use HuggingFaceH4's StarChat Beta GPTQ, which is a language model designed to aid in coding tasks. The article provides instructions for downloading and using the model through the text-generation-webui or from Python code. The model is fine-tuned on a diverse range of dialogues in over 35 languages, including English and over 80 programming languages. The article also highlights some potential biases, risks, and limitations of the model, such as the tendency to produce false URLs or code snippets that are syntactically valid but semantically incorrect. Finally, the article provides information on how to contribute to the development and expansion of the model. https://huggingface.co/TheBloke/starchat-beta-GPTQ
- The post shared by Francesco Saverio Zuppichini discusses the resources he recommended to a friend who wanted to quickly learn about LLMs. Resources listed include papers, blogs, videos, and different language models such as ChatGPT, LLama, Vicuna, and WizardLM. The post also includes YouTube channels recommended by Zuppichini, such as AI Explained, Yannic Kilcher, and Sam Witteveen. Other users in the comments section share their own resources, such as the Stanford CS25 course and Cohere's NLP uni. https://www.linkedin.com/posts/francesco-saverio-zuppichini-94659a150_ai-ml-ds-activity-7072868294000566272-kV83?utm_source=share&utm_medium=member_android
- <https://www.linkedin.com/posts/francesco-saverio-zuppichini-94659a150_ai-ml-ds-activity-7072868294000566272-kV83?utm_source=share&amp;utm_medium=member_android>
	- NVIDIA Research has developed a system for real-time rendering of complex appearance in games and live previews with a combination of algorithmic and system level innovations. The appearance model utilizes learned hierarchical textures that are interpreted using neural decoders, which produce reflectance values and importance-sampled directions. The decoders are equipped with two graphics priors, the first of which facilitates accurate reconstruction of mesoscale effects, while the second microfacet sampling distribution allows the neural decoder to perform importance sampling efficiently. The resulting appearance model supports anisotropic sampling and level-of-detail rendering and allows baking deeply layered material graphs into a compact unified neural representation. The system opens up the potential for using film-quality visuals in real-time applications such as games and live previews. https://research.nvidia.com/labs/rtr/neural_appearance_models/
- This text is a mixture of a cookie notice from Reddit, and a collection of recent posts on the subreddit r/StableDiffusion. The cookie notice explains that Reddit uses cookies and similar technologies to provide a better user experience, and gives users the option to accept or reject cookies. The collection of posts on r/StableDiffusion showcase various AI-generated images and animations using different models and software tools. The posts include scannable Cat QR Art with AI, synthesized 360 views of Stable Diffusion generated photos with PanoHead, an animated dance AI animation, and more. Some of the posts are marked as NSFW. https://www.reddit.com/r/StableDiffusion/comments/145d6by/scannable_cat_qr_art_with_ai_my_recent_attempt/
- Google Cloud has unveiled its latest platform capabilities for building and powering custom generative AI applications, with the latest update to Vertex AI. Generative AI support on this platform will now give customers access to text models powered by PaLM 2, as well as Embeddings API for text. Other foundation models in the model garden will also be available, allowing developers to customize these models with their own data and quickly build generative AI applications. With generative AI studio generally available, customers can leverage an even wider range of tools to accelerate the development of custom generative AI applications. This latest update reflects Google Cloud's commitment to making generative AI useful to everyone, backed by enterprise-grade data governance, security and safety features. https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-support-on-vertexai
- The paper introduces a new compressed format and quantization technique called Sparse-Quantized Representation (SpQR), which enables near-lossless compression of large language models (LLMs) with minimal accuracy loss. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision while compressing all other weights to 3-4 bits. The technique achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLMs, making it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup. SpQR comes with efficient algorithms for both encoding weights into its format as well as decoding them efficiently at runtime, and yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x. https://arxiv.org/abs/2306.03078
- Glaze is an academic research project by the University of Chicago, aiming to protect artists from the invasive uses of machine learning through ethical security techniques. The tool generates a cloaked version for each piece of artwork an artist creates, creating almost invisible changes. This helps prevent AI models from copying the artist's unique style and mimicking it for their purposes. Glaze has been evaluated through a user study involving more than 1,100 professional artists. Although Glaze is not a permanent solution against AI mimicry as AI evolves quickly, it is hoped that Glaze and follow-up projects will provide some protection to artists against AI mimicry. Since its initial release, Glaze has hit 721,000 downloads, and an updated GPU version for Windows is available for free. https://glaze.cs.uchicago.edu/
- The text contains updates and discussions on AI language models and related topics. It also includes a notice about the use of cookies on Reddit and its partners' websites. One user shared progress made in getting Landmark attention working within Oobabooga and the quantization of models. Other users provided feedback and shared insights on related technical topics, such as context size, inference performance, and trust in remote code execution. The text also includes links to open-source models and tools, discussions on new quantization methods, and updates on various AI language models' development and performance. https://www.reddit.com/r/LocalLLaMA/comments/148prx3/landmark_attention_oobabooga_support_gptq/
- The text describes a model called "30B-Lazarus-gptq-4b* available on the Hugging Face platform for natural language processing. The model was created using PyTorch and Transformers frameworks and has an Apache-2.0 license. The files associated with the model include a config file, generation config file, special tokens map, tokenizer model, and tokenizer config file. The model is available for training, deployment, and use in Transformers. https://huggingface.co/Yhyu13/30B-Lazarus-gptq-4bit/tree/main
- A new report from the Stanford Center for Research on Foundation Models has found that major foundation model providers, including OpenAI and Google, largely do not comply with the draft requirements of the EU's AI Act, the world's first comprehensive regulation of AI, particularly in regards to their compliance with requirements to provide information on training data, hardware use, and how they evaluate and test models. The report found significant discrepancies in compliance across model providers, with some scoring less than 25% and only one scoring at least 75%. The report recommended that policymakers prioritize transparency in the AI ecosystem and that foundation model providers work towards industry standards with the input of stakeholders beyond the companies themselves. https://crfm.stanford.edu/2023/06/15/eu-ai-act.html
- The repository is for LLaMA-Adapter, an efficient, lightweight adaption method for fine-tuning Instruction-following and Multi-modal LLaMA models that allow them to make high-quality instruction-following sentences. It includes code for the implementation of LLaMA-Adapter, as well as demos and training modules. The repository provides a comparison between LLaMA-Adapter's Model Parameters, Storage Space, and Training Time with other methods. The repository has multiple contributors, and interested candidates can apply for internships, postdocs, and full-time researcher positions. It is licensed under the GPL-3.0 license. The readme file also includes news, acknowledgments, citation, and hiring announcements. https://github.com/ml-lab/LLaMA-Adapter-2
- The Generative AI learning path offered by Google Cloud Skills Boost is a collection of courses that guide students through generative AI products and technologies, from understanding Large Language Models (LLM) to designing and deploying generative AI solutions on Google Cloud. The introductory-level courses include an explanation of GAI, LLM, and responsible AI, and they introduce Google's seven AI principles. Students can earn a skill badge by completing the Introduction to Generative AI, Introduction to Large Language Models, and Introduction to Responsible AI courses. Intermediate courses cover topics such as the Encoder-Decoder Architecture, Attention Mechanism, Transformer Models, and BERT Models. Other courses teach students how to create image captioning models and use the Generative AI Studio product on Vertex AI. Overall, the Generative AI learning path aims to provide a comprehensive understanding of the fundamentals of generative AI and its applications, preparing students for a career in AI development. https://www.cloudskillsboost.google/journeys/118
- The Reddit user, edinburgh1975, posted on r/LocalLLaMA seeking advice on the best LLM for legal analysis. They are working on a project to make the law more accessible to the public with a small budget and a team with varying tech skills. Other Reddit users provided suggestions, including using GPT-3.5 for generating free advice while charging for the use of GPT-4 32k, fine-tuning GPT-4 to generate training data for their local LLM, and utilizing Wiki resources for all things related to Local LLM. Some users also discussed the risks of confabulation when using AI models for legal research. https://www.reddit.com/r/LocalLLaMA/comments/14drnvd/best_llm_for_legal_analysis/
- This text provides information about the Vid2Avatar project which aims to reconstruct 3D avatars from videos in various settings using self-supervised scene decomposition. The project uses a python virtual environment and requires the installation of certain dependencies and Kaolin 0.10.0. Additionally, the SMPL model needs to be downloaded and preprocessed demo data can be downloaded from Google Drive. The project includes training which usually takes 24-48 hours and validation is saved in the outputs folder. The project also includes a test phase and allows for 3D visualization using AITViewer. The project also includes instructions on how to play on a custom video with modifications to the preprocessing script and metainfo changes. The project acknowledges other research work and includes a list of related human body reconstruction projects from the team. Lastly, a citation in the desired format for the Vid2Avatar project is included. https://github.com/MoyGcc/vid2avatar
- The CEO of ZenGo, Ouriel Ohayon, has accused AI language model ChatGPT of breaching privacy, stating that the program may have accessed and read some of their internal emails to provide information about their business model. Ohayon requested information about ZenGo that was not public yet and ChatGPT provided the information accurately, leading to his concerns. However, some users questioned the context of the prompt and suggested that the information could have been guessed, sourced from a competitor's website or a prompt from an employee. https://twitter.com/OurielOhayon/status/1650369984080629760
  --   HuggingChat, an open-source chatbot developed by AI startup Hugging Face is set to challenge the status quo of AI chatbot technology. HuggingChat is built on the modified LLaMa 30B SFT 6 model and can generate text in natural language or in a specific format and also generate code in multiple programming languages. Huggingchat can be found at hf.co/chat and is free to use. The AI company is championing open-source AI through offering free technology, which is transparent, accountable and aims to promote inclusivity, disseminate power and democratize AI. Hugging Face looks to make all high-quality chat models available through a single hub in the future. Despite HuggingChat being characterized by similar limitations as other text-generating models, Hugging Face is working on refining the model. The AI company is poised to change the way we interact and develop AI chatbots, challenging the status quo, and promoting transparency and accountability. https://medium.com/codingthesmartway-com-blog/my-take-on-huggingchat-the-open-source-chatbot-alternative-to-chatgpt-thats-shaking-things-up-589a101bccde
- The text includes two different topics. The first is about Reddit's use of cookies and similar technologies to provide users with a better experience, personalize content and advertising, and measure advertising effectiveness. Users can choose to accept all cookies or reject non-essential ones. The second topic is a discussion thread on the StableDiffusion subreddit about generating color maps for 3D models using AI technology. The thread includes technical details on the process and some questions and comments from other users. The StableDiffusion subreddit focuses on the use of AI and computer vision technologies for creative projects. https://www.reddit.com/r/StableDiffusion/comments/11ol47u/3d_model_face_color_map_generation_test3/
- The post discusses advanced advice for model training and captioning. The author gives specific pointers for training a checkpoint model, such as maximizing visual diversity, minimizing visual repetition, and denoising original images. They also recommend ordering captions from most to least prominent concept and experimenting with conditional dropout to force a style into the model. In addition, the author suggests using chaining to tweak settings and allowing the model to change settings as it goes. Other topics discussed in the comments section include the best software for denoising images, improving finetuning of bright and dark images, and creating new unique and consistent characters with Loras. https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/
- The post on Reddit's r/StableDiffusion is a guide for advanced advice on model training and captioning, specifically for those who already have a basic understanding of training a checkpoint model and want to improve. The post includes tips on training images such as denoising and maximizing visual diversity, and ordering captions from most to least prominent concepts. The post also includes recommendations for software and tools to improve training, such as Affinity Photo and Conditional Dropout. The comments section includes further discussion and feedback from other users on the topic. https://www.reddit.com/r/StableDiffusion/comments/114dxgl/advanced_advice_for_model_training_finetuning_and/
- --   The Awesome-GPT4-with-Applications is a collection of resources on GPT-4, which includes news articles, official documents, demos, tutorials, and applications. OpenAI recently released GPT-4, a large multimodal model with their best-ever results on capabilities and alignment. The collection includes official documentation such as the technical report, system card, and API waitlist, as well as early versions of GPT such as GPT, GPT-2, and GPT-3. There are also tutorials, examples, and live streams available for developers interested in working with GPT-4. The resource list includes a range of applications from chatbots to research assistants, visual accessibility, language learning, and knowledge management. Additionally, the collection provides information on frameworks and libraries for developers interested in building on top of GPT-4. Finally, contributions to the collection are welcome, and users are encouraged to submit new resources or suggest improvements through pull requests or issues. https://github.com/dk-liang/Awesome-GPT-4-with-Applications
- The recently proposed StyleGAN model has shown impressive results in face manipulation but is limited to cropped aligned faces at a fixed image resolution it is pre-trained on. To overcome this limitation, a new model called StyleGANEX is proposed in which the shallow layers of StyleGAN are refactored using dilated convolutions to rescale the receptive fields, enabling fixed-size small features to be extended into larger ones that can accommodate variable resolutions. A corresponding encoder is also introduced to enable real face inversion and manipulation. The effectiveness of this approach is validated using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face translation, and face toonification. The paper presents experimental results and comparisons with other state-of-the-art face manipulation methods, showing that StyleGANEX produces more coherent results with fewer discontinuities near the seams. The approach retains the style representation and editing ability of StyleGAN while significantly extending its generative space beyond cropped aligned faces. The StyleGANEX model is compatible with pre-trained StyleGAN parameters without retraining and provides a flexible and powerful solution for face manipulation beyond cropped aligned faces. https://buff.ly/3laNpgY
- Simple LLM Finetuner is an intuitive user interface designed to facilitate fine-tuning various language models using the LoRA method via the PEFT library on commodity NVIDIA GPUs. It allows users to manage datasets, customize parameters, train, and evaluate the model's inference capabilities. The beginner-friendly interface features explanations for each parameter and allows users to simply paste datasets into the UI separated by double blank lines. The necessary prerequisites to use the finetuner are a Linux or WSL OS and a modern NVIDIA GPU with >= 16 GB VRAM. Before launching, users must clone the repository and install the required packages. The finetuner supports running on a regular Colab Tesla T4 instance with small datasets and sample lengths of 256. Conda is preferred for a virtual environment to install the required packages. After preparing a training dataset, users can specify the new LoRA adapter name and click train, adjusting the max sequence length and batch size to fit GPU memory. The model will be saved in the lora/ directory, and after training, users can navigate to the *Inference" tab, select their LoRA, and play with it. The code is released under the MIT License and has already received high traffic on Github. https://github.com/lxe/simple-llama-finetuner
- The research paper presents a diffusion model called 3DiM for 3D novel view synthesis. The model uses a pose-conditional image-to-image diffusion model, trained to take a source view and its pose as inputs and generate a novel view for a target pose as output. The model can generate multiple views that are approximately 3D consistent using a sampling technique called stochastic conditioning. The paper also introduces a new evaluation methodology, 3D consistency scoring, to quantify the 3D consistency of a generated object. Additionally, the paper compares 3DiM to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM's generated completions from a single view achieve much higher fidelity, while being approximately 3D consistent. The results of the research highlight the effectiveness of diffusion models for novel view synthesis and the critical importance of modifications to the image-to-image UNet to achieve high-quality results. https://3d-diffusion.github.io/
  --   Researchers from OpenAI have introduced “consistency models” as a family of generative models for realistic sample generation with just a single forward pass, without relying on adversarial approaches and other models such as normalizing flows. The authors propose to learn a neural network F(x,t) that "is invertible and for any trajectory x(t), F allows for a return to the initial condition." The training procedures are by distillation and by isolation, with the authors estimating the score function via a Monte Carlo method. The proposed approach is shown to generate realistic samples in one forward pass, using different experiments such as image generation, super-resolution, and inpainting. The approach can offer advantages in terms of required computing resources and open the way to new applications inaccessible to diffusion models. https://www.marktechpost.com/2023/03/10/open-ai-proposes-consistency-models-a-new-family-of-generative-models-that-achieve-high-sample-quality-without-adversarial-training/
- In a recent paper, researchers from OpenAI introduce "consistency models," which enable the generation of realistic samples in a single forward pass. The proposed models are a new family of generative models that achieve high sample quality without adversarial training. The authors propose to learn a neural network that satisfies certain properties and allows for realistic sample generation in a single forward pass. Two training configurations are proposed to achieve this, with the authors experimenting with image generation, inpainting, and super-resolution. The proposed models offer several advantages over previous techniques, such as requiring fewer computing resources, and could lead to new applications. https://www.marktechpost.com/2023/03/10/open-ai-proposes-consistency-models-a-new-family-of-generative-models-that-achieve-high-sample-quality-without-adversarial-training/
- The paper proposes a new method for estimating depth from a single RGB image using diffusion-based denoising models. The method involves infilling missing depth using nearest neighbor interpolation, adding noise to the depth map, and training a neural network to predict the noise given the RGB image and noisy depth map. The proposed DepthGen model achieves state-of-the-art performance on indoor NYU depth v2 dataset and competitive results on outdoor KITTI dataset. Additionally, the paper outlines a simple text-to-3D pipeline using DepthGen and off-the-shelf text-to-image models. The proposed approach provides robust zero-shot performance and represents depth ambiguity naturally. https://depth-gen.github.io/
- Explainpaper is a free online tool that makes research papers easier to read by using an AI model to explain dense sections. Researchers can upload a paper, highlight confusing text, and get an explanation. In the background, an LLM simplifies and explains complex concepts. The tool has been used by Bindu Reddy, an AI and ML researcher, who reported her research paper review time has gone down significantly. Other researchers, such as Amy Cun and Kenneth Cassel, have also praised the tool for its ability to help them understand complex concepts in research papers. The tool has been described as a "killer product" by some and is recommended for anyone looking to become an expert in any field. http://explainpaper.com
- D-ID, a technology company specializing in facial recognition and animation, has launched its AI Presenters on Canva, which enable users to hold a face-to-face conversation with a digital human on a free web app called chat.D-ID. The software powering chat.D-ID has practical applications across various sectors, such as sales and marketing, learning and development, customer experience, and personal health and wellness. The company will also offer the streaming animation technology behind chat.D-ID to businesses and developers via its generative AI API. The real-time capabilities of the technology can be integrated with both open and closed-domain AI models, allowing businesses of all sizes to create a more personal connection with customers, employees, and communities. Customers who want to create a digital person to converse about their business or organization can book a call with D-ID to learn more. https://www.d-id.com/chat/
- The Whisper-UI is a user interface built on top of OpenAI's Whisper speech-to-text model that features media downloading and transcription from YouTube videos, playlists, or local files. The app allows users the ability to browse, filter, and search saved audio files. The app dependencies include Python 3.11, ffmpeg, and the requirements in the requirements.txt file. The app can be run with the command "streamlit run app/01_🏠_Home.py." Alternatively, the app can be run via Docker through the included docker-compose.yml with the command "docker compose up." The app is licensed under MIT. https://github.com/hayabhay/whisper-ui
- CLIP-Actor is a pytorch implementation for the ECCV 2022 paper that contains a novel text-driven motion recommendation and neural mesh stylization system for human mesh animation. The code was developed on Ubuntu 18.04 with Python 3.7, CUDA 10.2 and PyTorch 1.9.0. You need to download relevant body models and datasets for running CLIP-Actor, and it requires a single GPU with a minimum of 24 GB of RAM. The implementation of CLIP-Actor is largely inspired by and fine-tuned from the seminal prior work, Text2Mesh. The authors of CLIP-Actor and Text2Mesh appreciate the support and contribution from the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT). The code is available on GitHub, and the authors invite researchers and academics to consider citing their paper and code for any work they found helpful. https://github.com/youwang-kim/clip-actor
- Cross-Origin Resource Sharing (CORS) is a security mechanism that enables restricted resources on a web page to be accessed from another domain outside the domain from which the first resource was served. CORS allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. It defines a way in which a browser and server can interact to determine whether it is safe to allow the cross-origin request. The specification for CORS is included as part of the WHATWG's Fetch Living Standard. CORS is supported by all modern browsers and can be used as an alternative to the JSONP pattern. The HTTP headers that relate to CORS are Origin, Access-Control-Request-Method, Access-Control-Request-Headers, Access-Control-Allow-Origin, Access-Control-Allow-Credentials, Access-Control-Expose-Headers, Access-Control-Max-Age, Access-Control-Allow-Methods, and Access-Control-Allow-Headers. CORS is widely used in the object-capability model and any website can manually parse responses for increased security. https://en.m.wikipedia.org/wiki/Cross-origin_resource_sharing
- The tweet thread is advertising the ChatGPT language model's ability to generate unlimited prompts quickly and efficiently. The user suggests using GPT-4 to generate the best prompts on any topic. Additionally, the user mentions that GPT-4 can also be used to talk to any book and ask any question. The language model's capabilities make it an efficient tool for generating ideas and gaining information. https://mobile.twitter.com/heyBarsee/status/1640368028884914178
- The Twitter conversation started with a tweet from TomLikesRobots, featuring a keyframe generated with ControlNet's img2img and animated with Ebsynth, captioned with a quote from Lord of the Rings. Another user, rainisto, complimented the video's quality, which TomLikesRobots said was attributed to having enough time to practice with his new tools over the weekend. The conversation continued with users discussing whether they could run the AI models on Colab and its possible applications in anime remakes. Some concluded that Ebsynth was the way to go for creating convincing videos using animation as the source, while others suggested more advanced tools. https://twitter.com/TomLikesRobots/status/1627073211656732676
- Clarkesworld Magazine, a renowned sci-fi publication, has announced that it has temporarily closed its submissions due to a massive increase in machine-generated stories sent to the publication. The numbers tallied up to 500 in February alone, up from just over 100 in January and a low baseline of around 25 in October 2022. AI models, such as ChatGPT, can author original stories quickly by being trained on millions of books and websites. However, the stories are not created autonomously, and a human must guide their output with a prompt that the AI model then attempts to automatically complete. According to Clarkesworld Magazine, the number of submissions continues to rise because of get-rich-quick schemes. The problem of AI-authored content is also not unique to Clarkesworld, as Reuters reported over 200 e-books on the Amazon Kindle store that list ChatGPT as the author or co-author. The use of bots has created an awkward position for Clarkesworld, as it aims to filter out the spammers while still encouraging undiscovered writers or writers from certain regions of the world who might be unfairly targeted by geographical-based bans. Detecting text written by language models has low accuracy rates, and the magazine doesn't have a solution to the problem yet. The editor of Clarkesworld encouraged those who want to support the magazine to subscribe. https://arstechnica.com/information-technology/2023/02/sci-fi-becomes-real-as-renowned-magazine-closes-submissions-due-to-ai-writers
- Hugging Face is a technology company that creates machine learning natural language processing (NLP) models and develops applications to make these models more accessible. One of their newest applications is called UnCLIP Image Interpolation, which utilizes AI to enhance and extrapolate images to higher resolutions. Users can upload images to the platform and adjust settings such as the number of steps, seed, and interpolation factor to customize the output. The tool can also be used via API and was built with Gradio. https://huggingface.co/spaces/nagasaiabhinay/unclip_image_interpolation_demo
- "Understanding Deep Learning" is a comprehensive book by Simon J.D. Prince, to be published by MIT Press in 2023, covering topics from supervised and unsupervised learning to deep reinforcement learning, as well as explaining why deep learning works and its ethical implications. The book is accompanied by resources for instructors, including slides, notebooks, and PDF figures for each chapter. The book aims to provide a clear understanding of the underlying principles of deep learning, including neural network architecture, loss functions, training models, regularization, and performance measurement, as well as exploring more advanced topics such as convolutional networks, residual networks, transformers, graph neural networks, and generative adversarial networks. It also covers emerging areas of research, such as normalizing flows, variational autoencoders, diffusion models, and deep reinforcement learning. Additionally, the book explores the ethical implications of deep learning, encouraging readers to consider the impact of their work on the wider society and to think critically about potential biases and unintended consequences. Overall, the book is a valuable resource for anyone looking to dive deeper into the world of deep learning, whether for academic or practical purposes. https://udlbook.github.io/udlbook/
- NUWA is a platform that specializes in generative models capable of producing multimedia content such as images and videos. The platform offers several products, including NUWA XL and NUWA Infinity, both of which utilize cutting-edge technology to produce long videos based on provided scripts. NUWA also offers a Gallery feature that allows users to browse and view content created using the platform. Their NUWA XL product utilizes a "coarse-to-fine" process to generate video content efficiently. The platform has released a research paper on their innovative generative models. For any inquiries, users can contact NUWA through their website, and the platform operates under Microsoft. https://msra-nuwa.azurewebsites.net/#
- NUWA-Infinity is a multimodal generative model that generates high-quality images and videos from given text, image, or video input. This AI technology is designed to create realistic and visually appealing content that can be used for a variety of applications. Its features include loading images and videos with 100% accuracy, a gallery of generated content, and a research paper on the technology. Additionally, NUWA XL is another model that can generate larger images, while NUWA Infinity is designed for infinite content generation. Overall, NUWA-Infinity is a promising technology that can revolutionize the creation of images and videos. https://msra-nuwa.azurewebsites.net/#/NUWAInfinity
- Meta AI has developed a new AI model called ImageBind that can bind data from six different modalities, including images and video, audio, text, depth, thermal, and inertial measurement units (IMUs), without explicit supervision. The model recognizes the relationships between these modalities and creates a single embedding space to bind them together, enabling machines to analyze different forms of information together. ImageBind can even upgrade existing AI models to support input from any of the six modalities, enabling audio-based search, cross-modal search, multimodal arithmetic, and cross-modal generation. The open source ImageBind model achieves a new state-of-the-art performance on emergent zero-shot recognition tasks across modalities. https://imagebind.metademolab.com/
- Cohere, a start-up specializing in developing natural-language processing (NLP) technology, has raised $270m in a funding round to bring generative AI to enterprises. The firm's models, powered by embeddings, enable interactive chat features, generate text for product descriptions, and capture the meaning of text for search, content moderation, and intent recognition. Organizations can also use Cohere's technology to customize models for specific use cases, domains, or industries. The company's Command model is among the highest performing models as measured by Stanford University's HELM benchmarks, with a mean win rate of 93%. https://cohere.com/
- The text is a collection of videos related to transformers, GPT (Generative Pre-trained Transformer), and language modeling available on YouTube. The videos feature lectures, presentations, and discussions by researchers and experts in the field, including Andrej Karpathy, Ilya Sutskever, and Harry Surden. They cover various topics related to transformer networks, attention and self-attention mechanisms, GPT models, their workings, emergent abilities, biomedical applications, scaling for LLMs, among others. The texts are available on YouTube and are a useful resource for anyone interested in learning more about transformers and GPT models. https://youtu.be/XfpMkf4rD6E
- PrivateGPT is a test project that enables users to interact privately with their documents using the power of GPT without any data leaving the environment. The tool is built using LangChain, GPT4All, LlamaCpp, Chroma, and SentenceTransformers. Users can ingest documents and ask questions without an internet connection and set up their environment by installing all requirements and downloading the LLM model. Users can use the CLI to ask a question or use optional command-line arguments to modify its behavior. The ingest.py script uses LangChain tools to parse the document and create embeddings locally, while the privateGPT.py script uses a local LLM based on GPT4All-J or LlamaCpp to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs. The software requires Python 3.10 or later and a C++ compiler may be needed on some systems. The tool is not meant for production and the models selection is not optimized for performance, but for privacy. https://github.com/imartinez/privateGPT
- The article provides an introduction to word and sentence embeddings, which are used in language models to translate human language into computer language using numbers that capture properties and features of words and sentences. Word embeddings assign scores to each word and much like assigning coordinates to points on a plane, they are assigned numbers that capture similarities and differences between words while also capturing additional properties such as age and size. Sentence embeddings are like word embeddings, except they associate sentences with a vector full of numbers that capture similar properties as word embeddings, but with the added complexity of taking into account the order of words, the semantics of the language, and the actual meaning of the sentence. Multilingual embeddings unify many languages into one and Cohere offers a large multilingual model that has shown wonderful results with more than 100 languages. The multilingual embeddings can be extended to language embeddings which are useful for translation and for searching and understanding text in different languages. The article concludes that word and sentence embeddings are the primary building blocks of most language models and they capture many relations between words, semantics, and nuances of the language into equations regarding the corresponding numbers. https://txt.cohere.ai/sentence-word-embeddings/
- A new CLIP model, ViT-G/14, has been trained with OpenCLIP, achieving 80.1% zero-shot accuracy on ImageNet and 74.9% zero-shot image retrieval (Recall@5) on MS COCO. This is the best open-source CLIP model as of January 2023. The approach underlying CLIP
	 - self-supervised learning on a large, varied dataset, produces more robust and fair models, and can be used for zero-shot classification, retrieval, and guidance/conditioning in generative models. The new ViT-G model achieves the highest zero-shot ImageNet accuracy among models that use only naturally occurring image-text pairs as training data, without explicit labels, pseudo-labels, or any pretrained image or text encoders. The training run utilized several new techniques, including FLIP to accelerate training and model soups to surpass 80% accuracy. The released checkpoint is available through OpenCLIP and in the HuggingFace hub. In the future, the model may be fine-tuned for multilingual capabilities or higher resolution. Contributions to OpenCLIP are welcomed. https://laion.ai/blog/giant-openclip/
- The rise of Generative AI, where machines are able to create new content rather than just analyze existing data, has the potential to revolutionize industries that rely on human creativity, such as social media, coding, advertising, and design. While machines are beginning to create credible and sometimes even superhuman results, the technology is still in its early stages, with models becoming cheaper, faster, and more accessible as the industry evolves. Developers and founders have the opportunity to build novel applications harnessing the power of Generative AI, from copywriting and code generation to gaming and design. However, there are concerns around business models and technology, such as issues around copyright and costs, that must be resolved. Nonetheless, the nascent field of Generative AI is generating excitement and optimism about the possibilities for human-machine co-creation. https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/
- The article explores ways to make Large Language Models (LLMs) more predictable and controllable through prompt engineering. Prompt engineering basically uses the appropriate prompts to produce the desired response back from the LLM. The article provides a code walkthrough using OpenAI’s GPT3.5 as the main LLM and an open-source Python library called PanML. The article covers various use cases such as modifying the prompt, output filtering, LLM-assisted output filtering, and provides a beginner’s guide to building LLM-powered applications. The article also introduces PanML, an open-source high-level Python library designed to help data scientists and machine learning engineers experiment and run LLMs in their local environment with ease. https://medium.com/@williamzheng_63722/steering-llms-with-prompt-engineering-dbaf77b4c7a1
- NVIDIA Research has introduced a complete system for real-time rendering of scenes that exhibit complex appearance, which was previously reserved for offline use. The system integrates learned hierarchical textures interpreted using neural decoders, which generate reflectance values and importance-sampled directions. Two graphics priors are equipped to fully use the modeling capacity of the decoders. The first prior facilitates precise reconstruction of mesoscale effects by transforming directions into learned shading frames, while the second prior allows the neural decoder to conduct efficient microfacet sampling distribution. The resulting appearance model underpins anisotropic sampling and level-of-detail rendering, allowing deeply layered material graphs to be baked into a concise unified neural representation. The neural material shaders applied in the system can execute neural decoders efficiently inside a real-time path tracer. The research team analyzed its scalability with an increasing number of neural materials and put forward the idea of improving performance using optimized code for coherent and divergent execution. Neural material shaders turn out to be over 10 times faster than conventional non-neural layered materials, enabling real-time applications such as games and live previews to use film-quality visuals. https://research.nvidia.com/labs/rtr/neural_appearance_models/
- Orca is a new language model developed by Microsoft that has just been released for open source use. This model, which is just 13B, is the closest model to ChatGPT, according to the video explaining it. Orca was created by imitating the logic and explanations of GPT 4 and using GPT 3.5 as an assistant, and it was trained in diverse tasks to make it efficient for use in real-world applications. The video provides insights from five other papers and showcases Orca on a dozen benchmarks. It shares details on how it works and why it was created. The video also goes into comments from Sam Altman and Ilya Sutskever on whether or not open source will catch up. https://www.youtube.com/watch?v=Dt_UNg7Mchg